
ARTICLE IN PRESS
Information Processing and Management xxx (2009) xxx–xxx

Contents lists available at ScienceDirect

Information Processing and Management
journal homepage: www.elsevier.com/locate/infoproman

Conceptual language models for domain-speciﬁc retrieval q
Edgar Meij a,*, Dolf Trieschnigg b, Maarten de Rijke a, Wessel Kraaij c
a b c

ISLA, University of Amsterdam, Science Park 107, 1098 XG, Amsterdam, The Netherlands HMI group, Faculty of EEMCS, University of Twente, P.O. Box 217, 7500 AE Enschede, The Netherlands IFL, Radboud University Nijmegen and TNO ICT, P.O. Box 9010, 6500 GL Nijmegen, The Netherlands

a r t i c l e

i n f o

a b s t r a c t
Over the years, various meta-languages have been used to manually enrich documents with conceptual knowledge of some kind. Examples include keyword assignment to citations or, more recently, tags to websites. In this paper we propose generative concept models as an extension to query modeling within the language modeling framework, which leverages these conceptual annotations to improve retrieval. By means of relevance feedback the original query is translated into a conceptual representation, which is subsequently used to update the query model. Extensive experimental work on ﬁve test collections in two domains shows that our approach gives signiﬁcant improvements in terms of recall, initial precision and mean average precision with respect to a baseline without relevance feedback. On one test collection, it is also able to outperform a text-based pseudo-relevance feedback approach based on relevance models. On the other test collections it performs similarly to relevance models. Overall, conceptual language models have the added advantage of offering query and browsing suggestions in the form of conceptual annotations. In addition, the internal structure of the meta-language can be exploited to add related terms. Our contributions are threefold. First, an extensive study is conducted on how to effectively translate a textual query into a conceptual representation. Second, we propose a method for updating a textual query model using the concepts in conceptual representation. Finally, we provide an extensive analysis of when and how this conceptual feedback improves retrieval. Ó 2009 Elsevier Ltd. All rights reserved.

Article history: Received 31 October 2008 Received in revised form 1 July 2009 Accepted 3 September 2009 Available online xxxx Keywords: Information retrieval Meta-language Language modeling Query modeling

1. Introduction Explicit and often manually curated knowledge is frequently being added to documents for a variety of reasons, e.g., to increase their ﬁndability or to aid navigation of the collection to which they belong. Such knowledge is typically expressed in a meta-language and can be either formal (e.g., in the form of a thesaurus or ontology) or more informal (e.g., in the form of user-generated tags). Annotations of this kind may be found in a broad range of domains and a variety of document types. News articles, for example, can be annotated with concepts from the NewsCodes taxonomies provided by the IPTC. Another example is the annotation of bibliographic records with indexing terms from a controlled vocabulary. In the biomedical domain citations in the MEDLINE database are manually indexed with concepts from the Medical Subject Headings (MeSH) thesaurus. We will refer to this broad range of meta-languages as concept languages and to their vocabulary terms as concepts. Tables 1 and 2 show two examples of document–concept annotations from the two test collections we describe later.
q

This work is a revised and substantially expanded version of Meij, Trieschnigg, de Rijke, and Kraaij (2008) and Meij and de Rijke (2008). * Corresponding author. Tel.: +31 205257565; fax: +31 205257490. E-mail address: edgar.meij@uva.nl (E. Meij).

0306-4573/$ - see front matter Ó 2009 Elsevier Ltd. All rights reserved. doi:10.1016/j.ipm.2009.09.005

Please cite this article in press as: Meij, E., et al. Conceptual language models for domain-speciﬁc retrieval. Information Processing and Management (2009), doi:10.1016/j.ipm.2009.09.005

ARTICLE IN PRESS
2 E. Meij et al. / Information Processing and Management xxx (2009) xxx–xxx Table 1 Example of a CSA document from the CLEF domain speciﬁc test collection, annotated with SA concepts. Document text [CSASA-1-EN-9600048] Immigration and Economic Dependence in the US: approaches to presenting logistic regression results Logistic regression models are found increasingly in the social science literature, but the coefﬁcients can be difﬁcult to interpret for novice users. Strategies are discussed that can enhance the substantive interpretation of logistic regression results. . . Concept annotations United States of America Immigrants Citizens Beneﬁts Social security Regression analysis

Table 2 Example of a MEDLINE document (title and part of abstract) annotated with MeSH concepts. Document text [PMID: 10077651] Mechanism of increased iron absorption in murine model of hereditary hemochromatosis: increased duodenal expression of the iron transporter DMT1 Hereditary hemochromatosis (HH) is a common autosomal recessive disorder characterized by tissue iron deposition secondary to excessive dietary iron absorption. We recently reported that HFE, the protein defective in HH, was physically associated with the transferrin receptor (TfR) in duodenal crypt cells and proposed that mutations in HFE attenuate the uptake of transferrin-bound iron from plasma by duodenal crypt cells, leading to up-regulation of transporters for dietary iron. . . Concept annotations Animals Carrier proteins Cation transport proteins Duodenum Hemochromatosis Iron Iron-binding proteins Mice Mutation

The introduction of concept languages was initially driven by a need to facilitate search and navigation of the collection (Roberts, 1984; Joyce & Needham, 1958). Concepts were deﬁned to unambiguously and precisely represent the content of documents. Today, most of these early retrieval systems have been replaced by full-text search systems which have been shown to be at least as effective (Cleverdon, Mills, & Keen, 1966). Since full-text search systems do not require a manually curated concept language, they are far less labour-intensive. Despite the effectiveness of full-text search, full-text indexing terms (which typically comprise of all the terms used in the documents in a given collection) can be more ambiguous or less expressive than concepts. Not surprisingly then, information retrieval (IR) researchers continue to study ways of incorporating information from concept languages to address problems in textual query representations. For example, a textual query may be mapped to one or more concepts in a thesaurus and expanded with their synonymous terms (Voorhees, 1994). Results of such approaches, however, have been mixed at best. In this paper we show that a concept language can be effectively used to improve full-text retrieval. In a two step process that extends on relevance feedback and uses a conceptual representation as a pivot language we improve the query model representing the information need of the user. In the ﬁrst step, the textual information need is translated into a conceptual representation. In a process we call conceptual query modeling, feedback documents from an initial retrieval run are used for obtaining a conceptual query model. This model represents the user’s information need at a different, higher conceptual level than the original query. The intuition behind this step is that this conceptual representation gives an unambiguous representation of the information need. In contrast to traditional textual relevance feedback, where the query reﬁnement is biased towards terms occurring in the initial query, this intermediate conceptual representation is less dependent on the original query words. On its own, this explicit conceptual representation can be used to aid retrieval, for example by suggesting relevant concepts to the user (Meij & de Rijke, 2007), or by matching it to a conceptual representation of the documents (Trieschnigg et al., 2009). In the second step, however, we translate the conceptual query model back into a contribution to the textual query model. We hypothesize that, since the textual representation of documents is more detailed than its conceptual representation,1 retrieving information with a textual query representation translated from a conceptual form, results in better retrieval performance than strictly matching with only concepts. Essential to these two translation steps is the estimation of a query model, both for terms and for concepts. The textual query should be captured by a small set of speciﬁc concepts and the conceptual query model should be translated to speciﬁc textual terms. To achieve this, we employ an expectation maximization algorithm inspired by parsimonious language models (Hiemstra, Robertson, & Zaragoza, 2004). The paper is organized around a number of research questions that aim to investigate the effectiveness of our proposed conceptual language models and place it in the context of state-of-the-art full-text retrieval systems. These questions are deﬁned as follows:

1

A document is typically represented by far more terms than concepts.

Please cite this article in press as: Meij, E., et al. Conceptual language models for domain-speciﬁc retrieval. Information Processing and Management (2009), doi:10.1016/j.ipm.2009.09.005

ARTICLE IN PRESS
E. Meij et al. / Information Processing and Management xxx (2009) xxx–xxx 3

1. To estimate a conceptual query model we propose a method that looks at the top-ranked documents in an initially retrieved set (Section 4.1). In order to assess the effectiveness of this step, we compare the results of using these concepts with a standard language modeling approach. Moreover, since this method relies on pseudo-relevant documents from an initial retrieval run, we also compare the results of our conceptual query models to another, established pseudo-relevance feedback algorithm based on relevance models. We ask: What is the relative retrieval effectiveness of this method with respect to the standard language modeling and conventional pseudo-relevance feedback approach? 2. For the estimation of both the conceptual query model and generative concept model we apply an iterative EM algorithm which emphasizes more informative terms. We ask: What is the impact of applying this algorithm compared to conventional estimates in terms of retrieval effectiveness? 3. The proposed method based on conceptual language models is dependent on a number of parameters. We ask: What is the sensitivity of the method to its parameter settings? How robust are the results across different collections and test sets? 4. By deﬁnition, curated knowledge is domain speciﬁc. So we ask the question: How portable is our conceptual language model? What are the results of the model across multiple test collections? Can we say anything about which evaluation measures are helped most using our model? Is it mainly a recall or precision-enhancing device? We make the following contributions in this article:  We propose a method for determining the concepts that are most likely to be associated with a given query, which allows effective conceptual (blind) relevance feedback. Moreover, this explicit conceptual query representation may be used as a means of suggesting query-related concepts to the user.  We propose generative concept models, that are used to generate terms for concepts related to the query. Besides this particular application, they may also be employed to determine semantic relatedness.  Finally, we provide an empirical comparison of our proposed method to existing relevance feedback models. The remainder of this paper is organized as follows: We discuss related work in Section 2. We then describe our retrieval framework and our conceptual language models are introduced next. We describe our experimental setup in Section 5 and report on the outcomes of our experimental evaluation and discuss our ﬁndings in Section 6. We end with a concluding section. 2. Related work Work related to our proposed conceptual language models may be found in overlapping areas, viz. query expansion, conceptual retrieval, and cluster-based retrieval. These will be discussed in this section. Query expansion aims at bridging the vocabulary gap between queries and documents by adding and reweighing terms in the original query (Voorhees, 1994). Query expansion approaches can be local or global (Xu & Croft, 1996). Local query expansion methods try to take into account the context of a query; one might, for example, consider a user’s history or proﬁle, in order to automatically enrich queries (Korfhage, 1984). Much later, similar notions were adopted in a language modeling setting (Bai et al., 2008). Finkelstein et al. (2002) propose to use the local context of query terms as they appear in documents to locate additional query terms. Relevance feedback is a form of local query expansion that relies on the analysis of documents from an initial retrieval run. The retrieved documents serve as examples to select additional query terms (Rocchio, 1971). Pseudo-relevance feedback methods assume the top-ranked documents to be relevant, but explicit or implicit relevance judgements from users may also be used (Anick, 2003; Keskustalo, Järvelin, & Pirkola, 2008; Vakkari, Jones, Macfarlane, & Sormunen, 2004; Xu & Croft, 1996). The recent interest of the semantic web community regarding models and methods related to ontologies have also sparked a renewed interest in using ontological information for relevance feedback (Bhogal, Macfarlane, & Smith, 2007; Rocha, Schwabe, & Aragao, 2004). In a language modeling setting, local query expansion has been applied to estimate query language models (Lafferty & Zhai, 2003; Tao & Zhai, 2006) or relevance models (Lavrenko & Croft, 2001); we elaborate on the latter in Section 3. Our method is related to these approaches in that it also looks at the results of an initial retrieval run. Instead of looking at the terms in these documents, however, we consider the concepts associated with the documents. Global query expansion uses global collection statistics or ‘‘external” knowledge sources such as concept languages to enhance the query. For example, concepts and lexical-syntactic relations as deﬁned in a thesaurus have been used for query expansion, with varying degrees of effectiveness (Bai, Song, Bruza, Nie, & Cao, 2005; Gao, Nie, & Bai, 2005; Meij & de Rijke, 2007; Roberts et al., 1984; Voorhees, 1994). Our method can be viewed as a combination of a local and global expansion method; a local expansion method is used to obtain a conceptual representation of a query, whereas a global method is used to translate the conceptual representation to a textual query contribution. Using a conceptual representation obtained from pseudo-relevance feedback has been investigated by different researchers in the biomedical domain. Srinivasan (1996) proposes adding concepts directly to an initial query and reports the largest improvement in retrieval effectiveness when another round of blind relevance feedback on vocabulary terms is applied afterwards. This method is similar to ours, although there are distinct differences in her approach and evaluation. For one,
Please cite this article in press as: Meij, E., et al. Conceptual language models for domain-speciﬁc retrieval. Information Processing and Management (2009), doi:10.1016/j.ipm.2009.09.005

ARTICLE IN PRESS
4 E. Meij et al. / Information Processing and Management xxx (2009) xxx–xxx

Srinivasan (1996) creates a separate ‘‘concept index” in which tokenized concept labels are used as terms. In this way, searching using a concept labeled ‘‘Stomach cancer” also matches the related, but clearly different concept ‘‘Breast can er” because they share the word ‘‘cancer”. In our opinion, this obfuscates the added value of using clearly deﬁned concepts; searching with a textual representation containing the word ‘‘cancer” will already result in matching related concepts. Therefore, we decide to use unique concept identiﬁers in our conceptual representation. Srinivasan (1996) concludes that concepts are beneﬁcial for retrieval, but remarks that the OHSUMED collection used for evaluation was quite small. Our research uses the larger TREC Genomics test collections and, additionally, investigates the use of document level annotations in another domain using the CLEF domain speciﬁc test collections. Finally, we remark that our proposed model is an extension of the language modeling retrieval framework, whereas Srinivasan (1996) extends a vector space retrieval model. Camous, Blott, and Smeaton (2006) also use the annotations of the top-5 retrieved documents to obtain a conceptual query representation, but incorporate them in a different fashion. The authors use them to create a new ranked list of documents, which is subsequently combined with the initially retrieved documents. In contrast, we explicitly update the original query model. All of the methods based on concept languages need a way of mapping between the concepts and their textual representation. Where the described approaches look for exact occurrences of the concepts in the text, we use the vocabulary terms associated with concepts to make this connection, as detailed in Section 4. Taking a step back from query expansion, many different ways of directly improving text-based retrieval by incorporating concepts or a concept language have been proposed. For example, the entries from a concept language may be used to deﬁne the indexing terms employed by the retrieval system. In the absence of a concept language, similar information might be derived from statistical methods (Joyce et al., 1958; Salton, 1971; Sparck-Jones & Jackson, 1970). For instance, a co-occurrence analysis of the entire collection might be applied to estimate dependencies between vocabulary terms (Bai et al., 2005; Chung, 2004). Alternatively, term dependencies may be determined on a query-dependent subset of the collection, such as a set of initially retrieved documents (Metzler & Croft, 2005; Mitra, Singhal, & Buckley, 1998; Xu & Croft, 1996). These dependencies may then be employed to locate terms related to the initial query. One of the ﬁrst attempts at automatically relating concepts with text was introduced in the 1980s. Giger (1988) incorporated a mapping between concepts from a thesaurus and words as they appear in the collection. The main motivation was to move beyond text-based retrieval and bridge the semantic gap between the user and the information retrieval system, a motivation closely related to ours. His algorithm ﬁrst deﬁnes atomic concepts, which are string-based concept to term mappings. Then, documents are placed in disjoint groups based on so-called elementary logical conjuncts, which are deﬁned through the atomic concepts. At retrieval time, the query is parsed and the sets of documents with the lowest distance to the requested concepts are returned. His ideas relate to recent work done by Zhou, Hu, Zhang, Lin, and Song (2006) and Zhou, Hu, and Zhang (2007), who use so-called topic signatures to index and retrieve documents. These signatures are comprised of recognizing named entities within each document and query; when named entities are not available, term pairs are used. Their named entity recognition step is automated and might not be completely accurate; we suspect that the errors in this concept detection process do not strongly affect retrieval performance because pairs of concepts (topic signatures) are used for retrieval. In our method, we rely on manually curated concept annotations, making the topic signatures superﬂuous. Trieschnigg, Kraaij, and Schuemie (2007) also use named entity recognition to obtain a conceptual representation of queries and documents. They conclude that searching only with an automatically obtained conceptual representation seriously degrades retrieval when searching for short documents (citations). Interestingly, the same approach performs on par with text-only search when larger documents (full-text articles) are retrieved. Instead of using named entity recognition, Gabrilovich and Markovitch (2007) employ document-level annotations, in the form of Wikipedia categories. They represent the categories as term vectors, where the individual term weights are determined using TF.IDF scores from the documents that are labeled with the concept at hand. In this way, the strength between vocabulary terms and concepts can be quantiﬁed, which can subsequently be used to generate vectors of concepts for a piece of text—either a document or query. This approach is similar to the topic modeling approach described by Wei (2007), which uses Open Directory Project (ODP) concepts in conjunction with generative language models. Instead of using concept–document associations, however, she uses an ad hoc approach based on the descriptions of the concepts in the concept language (in this case, ODP categories). Our conceptual language models are related to these approaches in that they also bridge between concepts and terms. We, however, use an iterative EM algorithm in tandem with a statistical translation model to establish the association between terms and concepts. Interestingly, all of these approaches open up the door to providing conceptual relevance feedback to users. Instead of suggesting vocabulary terms that are related to the query, we can now suggest related concepts that can, for example, be used for navigational purposes (Keskustalo et al., 2008; Meij & de Rijke, 2007; Silveira & Ribeiro-Neto, 2004; Vakkari et al., 2004). Trajkova and Gauch (2004) describe another possible application; their system keeps track of a user’s history by classifying visited web pages onto the concepts from the ODP. Further examples of mapping queries to conceptual representations can be found in the area of web query classiﬁcation. Broder et al. (2007) use a pseudo-relevance feedback technique to classify rare queries into a commercial taxonomy of web queries, with the goal to improve web advertisements. A classiﬁer is used to classify the highest ranked results, and these classiﬁcations are subsequently used to classify the query by means of voting. We use a similar method to obtain the conceptual representation of our query described in Section 4.1, with the important difference that all our documents have been manually classiﬁed. Mishne et al. (2006) classify queries into taxonomies using category-based web services. Shen, Sun, Yang, and Chen (2006) improve web query classiﬁcation by mapping the query to concepts in an intermediate taxonomy which in turn are linked to concepts in the target taxonomy. In our work, we use a single concept taxonomy which is used
Please cite this article in press as: Meij, E., et al. Conceptual language models for domain-speciﬁc retrieval. Information Processing and Management (2009), doi:10.1016/j.ipm.2009.09.005

ARTICLE IN PRESS
E. Meij et al. / Information Processing and Management xxx (2009) xxx–xxx 5

as a pivot language to improve the textual query model. Chen, Xue, and Yu (2008) use a taxonomy to suggest keywords. After mapping the seed keywords to a concept hierarchy, content phrases related to the found concepts are suggested. In our approach the concepts are used to update the query model, i.e., to update the probabilities of terms based on the found concepts rather than the addition of related discrete terms or phrases. Concepts can be recognized at different levels of granularity, either at the term level, by recognizing concepts in the text, or at the document level, by using document-level annotations or categories. While the former can be described as a form of concept-based indexing (Lancaster, 1982), the latter is more related to text classiﬁcation. Indeed, the mapping of vocabulary terms to concepts as described above is in fact a text (or concept) classiﬁcation algorithm (Sparck-Jones & Needham, 1968). Work done on cluster-based retrieval can be viewed as a variation on the same theme; in our case the clusters are deﬁned by the concepts that are associated with the documents in the collection. Kurland et al. (2004), for example, determine overlapping clusters of documents in a collection, which are considered facets of the collection. They use a language modeling framework in which their aspect-x algorithm smoothes documents based on the information from the clusters and the strength of the connection between each document and cluster. Liu and Croft (2004) evaluate both the direct retrieval of clusters and cluster-based smoothing. Their CBDM model is a mixture between a document model, a collection model, and the cluster each document belongs to, which is able to signiﬁcantly outperform a standard query-likelihood baseline. Instead of smoothing documents, Minker, Wilson, and Zimmerman (1972) use cluster-based information for query expansion. The authors evaluate their algorithm on several small test collections, without achieving any improvements over the unexpanded queries. More recently, Lee, Croft, and Allan (2008) have shown that detecting clusters in a set of (pseudo-)relevant documents is helpful for identifying dominant documents for a query and, thus, for subsequent query expansion, a ﬁnding which was corroborated on different test collections by Kurland (2008). These approaches all exploit the notion that ‘‘associations between documents convey information about the relevance of documents to requests” (Jardine & van Rijsbergen, 1971). Indeed, if we have evidence that a given concept is relevant for a particular query, it is natural to assume that all documents labeled with this concept have a higher prior probability of being relevant to the query. This is the main motivating idea for our current work. 3. The KL-divergence retrieval framework The success of generative language models in statistical machine translation and automatic speech recognition inspired several IR researchers to re-cast IR in a generative probabilistic framework, by representing documents as generative probabilistic models. Such models can be used to compute the probability of observing a sequence of terms, by computing the product of the probabilities of observing the individual terms. The ﬁrst published application of generative models for IR was based on the multiple Bernoulli distribution (Ponte & Croft, 1998), but the simpler multinomial unigram model became the mainstream model (Hiemstra, 1998; Miller, Leek, & Schwartz, 2000). Recent work has addressed some of the shortcomings of the multinomial model for modeling text and considers the Dirichlet compound multinomial distribution instead (Xu & Akella, 2008). This distribution provides a better model of the ‘burstiness’ of language and the authors show signiﬁcant improvements over the standard multinomial model. Whether it is a better candidate for representing text in our current context remains a subject for future work. In the multinomial unigram model, each document D is represented as a multinomial probability distribution P ðt jhD Þ over all the terms t in the vocabulary. At retrieval time, each document is ranked according to the likelihood of having generated the query, i.e., the probability that the query terms ðt 2 Q Þ are sampled independently and identically from the document language model (Hiemstra, 1998):

Score ðQ ; DÞ / PðQ jDÞ ¼

Y
t 2Q

Pðt jhD Þnðt;Q Þ ;

ð 1Þ

where nðt ; Q Þ denotes the count of term t in query Q. This model was generalized soon after, by realizing that an information need can also be modeled by a language model. In this way, a more general and ﬂexible retrieval model can be obtained by using a comparison of two language models as the basis for ranking. Several authors proposed the use of the Kullback–Leibler (KL)-divergence for ranking, since it is a well established measure for the comparison of probability distributions with some intuitive properties—it always has a non-negative value and equal distributions receive a zero divergence value (Lafferty et al., 2001; Ng, 2001; Xu & Croft, 1999). Using KL-divergence, documents are scored by measuring the divergence between a query model hQ and each document model hD . Since we want to assign a high score for high similarity and a low score for low similarity, the KL-divergence is negated for ranking purposes. More formally, the score for each query–document pair using the KL-divergence retrieval model is:

Score ðQ ; DÞ ¼ ÀKLðhQ khD Þ ¼ À

X
t 2V

P ðt jhQ Þ log

X X P ðt jhQ Þ Pðt jhQ Þ log Pðt jhD Þ þ PðtjhQ Þ log Pðt jhQ Þ; ¼À P ðt jhD Þ t 2V t 2V

ð 2Þ

where V denotes the set of all terms used in all documents in the collection. KL-divergence is also known as the relative entropy, which is deﬁned as the cross-entropy of the observed distribution (in this case the query) as if it was generated by a reference distribution (in this case the document) minus the entropy of the observed distribution. KL-divergence can also be measured in the reverse direction (also known as document likelihood), but this leads to poorer results for ad-hoc
Please cite this article in press as: Meij, E., et al. Conceptual language models for domain-speciﬁc retrieval. Information Processing and Management (2009), doi:10.1016/j.ipm.2009.09.005

ARTICLE IN PRESS
6 E. Meij et al. / Information Processing and Management xxx (2009) xxx–xxx

P search tasks (Lavrenko, 2004). The entropy of the query, t2V PðtjhQ Þ log PðtjhQ Þ, is a query speciﬁc constant and can thus be ignored for ranking purposes. In fact, one could argue that ranking on just the cross-entropy term provides a more concise ranking formula and is a suitable distance measure for comparing probability distributions in its own right (Kraaij, 2004). When the query model is generated using the empirical, maximum-likelihood estimate (MLE) on the original query, i.e.,

nðt; Q Þ Pðt j~ hQ Þ ¼ P ðt jQ Þ ¼ ; jQ j

ð3Þ

where jQ j indicates the length of the query, it can be shown that documents are ranked in the same order as using the query likelihood model from Eq. (1) (Zhai, 2002). In fact, a query is just a verbal expression of an underlying information need. The query model is therefore an estimate of the model for the underlying information need, sometimes called a relevance model (Lavrenko & Croft, 2001). This initial estimate can be improved by adding and reweighing terms, using external resources or relevance feedback techniques as described in Section 2. Next, we describe our baseline query modeling (Section 3.1) and document modeling (Section 3.2) approaches. In Section 4 we deﬁne our conceptual language models on top of these baseline approaches. 3.1. Query models Relevance models (Lavrenko & Croft, 2001) are one of the baselines we employ. Here, it is assumed that for every information need there exists an underlying relevance model and that the query and relevant documents are random samples from this model. The query model, parametrized by hQ , may be viewed as an approximation of this model. However, in a typical retrieval setting improving the estimation of hQ is problematic because we have no or only limited training data. The authors present two methods for estimating relevance models without training data by constructing models from the queries and a set of pseudo-relevant documents, using different independence assumptions. They determine the probability of observing t after having observed Q as:

Pðt ; q1 ; . . . ; qk Þ Pðt ; q1 ; . . . ; qk Þ ¼P ; Pðt j^ hQ Þ % P ðt jq1 ; . . . ; qk Þ ¼ 0 Pðq1 ; . . . ; qk Þ t 0 P ðt ; q1 ; . . . ; qk Þ

ð4Þ

where q1 ; . . . ; qk are the individual query terms. Under their method 2, the query terms are independent of each other, but keep their dependence on t:

Pðt ; q1 ; . . . ; qk Þ ¼ PðtÞ

k X Y i¼1 D2DQ

Pðqi jhD ÞP ðhD jt Þ;

ð5Þ

where DQ is a set of pseudo-relevant documents and

PðhD jt Þ ¼

P ðt jhD ÞPðDÞ : P ðt Þ

ð6Þ

Then, in order to obtain a query model that is a better estimate of the information need, the initial query Pðtj~ hQ Þ may be interpolated with the expanded part Pðtj^ hQ Þ (Balog, Weerkamp, 
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 8 - Practical: Searching your own PDF library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our Information Retrieval system of the previous lesson is simple and also quite efficient, but it lacks many features of a full-blown search engine. One particular downside of the system is that it stores the index in RAM memory. This means that either we have to keep it there or we have to rebuild the index each time we would like to search through a particular collection. We could try to improve our system, but there are some excellent Python packages for Information Retrieval and search engines, we could use. In this section we will explore one of them, called [Whoosh](http://whoosh.readthedocs.org/en/latest/), a search engine and retrieval system written in pure Python. \n",
    "\n",
    "![whoosh](files/images/whoosh_logo.png)\n",
    "\n",
    "Ever since science-journal giant Elsevier bought the once so promising bibliography management software [Mendeley](http://www.mendeley.com/), I have looked for alternative ways to manage my research PDF collection. For me, one of the most important features of a PDF management tool is to be able to do full text search in the PDFs for the content I am interested in. Uptill today I have not found a tool that fulfills all my needs, so we are going to build one ourselves. We will develop a full-blown search using Whoosh. We'll build a web interface on top of [Flask](http://flask.pocoo.org/) to query our search engine in a user-friendly way. Just to tease you a bit: this is what our search engine will look like:\n",
    "\n",
    "![pydf](files/images/pydf.png)\n",
    "\n",
    "This lesson is the first in a series of more practical lessons, in which we will build actual applications ready for use by end-users. You won't be learning many new programming techniques in this lesson, but we will introduce you to a large number of modules and packages that are available either in the standard library of Python or as third-party packages. The most important take-home message is that if you think about implementing a piece of software, the first thing you should do, is check whether someone else hasn't done it before you. Chances are good that someone has, and she or he probably has done a better job. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Index` is the most central object in the Whoosh' search engine. It allows us to store our data in a secure and sustainable way and makes it possible to search through our data. Every index requires an index schema which defines the available fields in the index. Fields represent pieces of information for each document in the collection. Examples of fields are: the author of a text, its publication date or the text body itself, and so on and so forth. For our PDF Index we will use a schema consisting of the following fields:\n",
    "\n",
    "1. id (a unique document ID);\n",
    "2. path (the filepath to the document on your computer);\n",
    "3. source (the filepath to the original source on your computer);\n",
    "4. author (the author or authors of the text);\n",
    "5. title (the title of a text);\n",
    "6. text (the actual text of the pdf).\n",
    "\n",
    "All these fields will be indexed for each pdf file in our collection and each field will be searchable. To create our schema in Whoosh, we first import the `Schema` object from `whoosh.fields`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from whoosh.fields import Schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each field we need to specify to Whoosh what kind of field it is. Whoosh defines fields like `KEYWORD` for keyword fields, `ID` for unique identifiable fields (e.g. filepaths), `DATETIME` for fields specifying information about dates, `TEXT` for textual objects, and some others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quiz!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have a look at the different field types provided by Whoosh, [here](http://whoosh.readthedocs.org/en/latest/api/fields.html#pre-made-field-types). Which types would be appropriate for the fields defined above?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Double click this cell and write down your answer*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each field type can be passed some additional arguments, such as whether the field should be stored, whether it should be sortable and scorable, etc. For reasons that will be clear later on, we want all our ID fields plus the author and the title field to be stored in the index. We import the appropriate fields and define our schema as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from whoosh.fields import ID, KEYWORD, TEXT\n",
    "\n",
    "pdf_schema = Schema(id = ID(unique=True, stored=True), \n",
    "                    path = ID(stored=True), \n",
    "                    source = ID(stored=True),\n",
    "                    author = TEXT(stored=True), \n",
    "                    title = TEXT(stored=True),\n",
    "                    text = TEXT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have defined a schema, we can create the index using the function `create_in`. We'll create the index in the directory `pydf`. The web application will reside in the same folder. Therefore, it is convenient to first direct your notebook to that directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/markov/Documents/UoA/GT/GT2016/PythonCourse/pydf\n"
     ]
    }
   ],
   "source": [
    "cd pydf/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from whoosh.index import create_in\n",
    "\n",
    "if not os.path.exists(\"pdf-index\"):\n",
    "    os.mkdir(\"pdf-index\")\n",
    "    index = create_in(\"pdf-index\", pdf_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After creation, the index can be openend using the function `open_dir`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from whoosh.index import open_dir\n",
    "\n",
    "index = open_dir(\"pdf-index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have an index, we can add some documents to it. The `IndexWriter` object let's you do just that. The method `writer()` of the class `Index` returns an instantiation of the `IndexWriter`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "writer = index.writer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the `add_document` method of `IndexWriter` to add documents to the index. `add_document` accepts keyword arguments corresponding to the fields we specified in our schema. We add our first document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "writer.add_document(id = 'blei2003', \n",
    "                    path = 'data/blei2003.txt',\n",
    "                    source = 'static/pdfs/blei2003.pdf',\n",
    "                    author = 'David Blei, Andrew Ng, Michael Jordan',\n",
    "                    title = 'Latent Dirichlet Allocation',\n",
    "                    text = open('data/blei2003.txt', encoding='utf-8').read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And some more:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "writer.add_document(id = 'goodwyn2013', \n",
    "                    path = 'data/goodwyn2013.txt',\n",
    "                    source = 'static/pdfs/goodwyn2013.pdf',\n",
    "                    author = 'Erik Goodwyn',\n",
    "                    title = 'Recurrent motifs as resonant attractor states in the narrative ﬁeld: a testable model of archetype',\n",
    "                    text = open('data/goodwyn2013.txt', encoding='utf-8').read())\n",
    "\n",
    "writer.add_document(id = 'meij2009', \n",
    "                    path = 'data/meij2009.txt',\n",
    "                    source = 'static/pdfs/meij2009.pdf',\n",
    "                    author = 'Edgar Meij, Dolf Trieschnigg, Maarten de Rijke, Wessel Kraaij',\n",
    "                    title = 'Conceptual language models for domain-speciﬁc retrieval',\n",
    "                    text = open('data/meij2009.txt', encoding='utf-8').read())\n",
    "\n",
    "writer.add_document(id = 'muellner2011', \n",
    "                    path = 'data/muellner2011.txt',\n",
    "                    source = 'static/pdfs/muellner2011.pdf',\n",
    "                    author = 'David Muellner',\n",
    "                    title = 'Modern hierarchical, agglomerative clustering algorithms',\n",
    "                    text = open('data/muellner2011.txt', encoding='utf-8').read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling commit() on the `IndexWriter` saves the changes to the index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "writer.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Searching and Querying"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The index contains four documents. How can we search the index for particular documents? Similar to the method `writer` of the `Index` object, the method `searcher` returns a `Searcher` object which allows us to search the index. The `Searcher` object opens a connection to the index, similar to the way Python opens regular files. To prevent the system from running out of file handles, it is best practice to instantiate the searcher within a `with` statement:\n",
    "\n",
    "    with index.searcher() as searcher:\n",
    "        do something\n",
    "        \n",
    "We'll do so later when we automatically index a complete collection of pdfs, but for now it is slightly more convenient to instantiate a searcher as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "searcher = index.searcher()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An instantiation of the class `Searcher` has a `search` method that takes as argument a `Query` object. There are two ways to construct `Query` objects: manually or via a query parser. To construct a query that searches for the terms *topic* and *probabilistic* in the text field, we could write something like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from whoosh.query import Term, And\n",
    "\n",
    "query = And([Term(\"text\", \"model\"), Term(\"text\", \"topic\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can feed this to the `search` method to obtain a `Result` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of hits: 2\n",
      "Best hit: <Hit {'title': 'Latent Dirichlet Allocation', 'path': 'data/blei2003.txt', 'source': 'static/pdfs/blei2003.pdf', 'author': 'David Blei, Andrew Ng, Michael Jordan', 'id': 'blei2003'}>\n"
     ]
    }
   ],
   "source": [
    "results = searcher.search(query)\n",
    "print('Number of hits:', len(results))\n",
    "print('Best hit:', results[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that all our four documents contain both the term *model* and *topic* but the paper _about_ topic models is considered the most relevant one for our query. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quiz!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a)** Construct the same query, but this time using an `Or` operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Or([Term('text', 'topic'), Term('text', 'model')])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from whoosh.query import Or \n",
    "\n",
    "# insert your code here\n",
    "query = Or([Term(\"text\", \"topic\"), Term(\"text\", \"model\")])\n",
    "query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b)** Construct a query using the `And` operator that searches for the terms *index* and *topic* in documents of which Dolf Trieschnigg is one of the co-authors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "And([Term('author', 'Trieschnigg'), Term('text', 'index'), Term('text', 'topic')])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# insert your code here\n",
    "query = And([Term(\"author\", \"Trieschnigg\"), Term(\"text\", \"index\"), Term(\"text\", \"topic\")])\n",
    "query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These query constructs are very explicit and clean. It is, however, much more convenient to use Whoosh' `QueryParser` object to automatically parse strings into `Query` objects. We construct a query parser as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from whoosh.qparser import QueryParser\n",
    "\n",
    "parser = QueryParser(\"text\", index.schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we have to specify the field in which we want to search and pass the schema of our index. The `QueryParser` is quite an intelligent object. It allows users to group terms using the string `AND` or `OR` and eliminate terms with the string `NOT`. It also allows to manually specify other fields in which to search for specific terms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "And([Term('text', 'probability'), Term('text', 'model'), Term('text', 'prior')])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser.parse(\"probability model prior\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "And([Or([Term('text', 'cluster'), Term('text', 'grouping')]), Or([Term('text', 'model'), Term('text', 'schema')])])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser.parse(\"(cluster OR grouping) AND (model OR schema)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "And([Term('text', 'topic'), Term('text', 'index'), Term('author', 'dolf'), Term('author', 'trieschnigg')])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser.parse(\"topic index author:'Dolf Trieschnigg'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prefix('text', 'clust')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser.parse(\"clust*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last query is a wildcard query that attempts to match all terms starting with *clust*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quiz!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment a little with the `QueryParser` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# insert your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing our PDFs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have some basic understanding of Whoosh' most important data structures and functions, it is time to put together a number of Python scripts that will construct a Whoosh index on the basis of your own PDF library.\n",
    "\n",
    "Open your favorite text editor and open a new Python file called `indexer.py`. Save that in the directory `python-course/pydf`. Add the schema we defined above to this file as well as the corresponding import statements. Before we will start, it is good to make a list of all the components we need to index our collection. In order to pass our documents to the `add_document` method we will need the following components:\n",
    "\n",
    "1. A function that transforms a PDF into text, since Whoosh requires plain text;\n",
    "2. A way to extract meta information from the PDF files (e.g. the author and the title);\n",
    "3. The directory or directories containing PDF files we would like to index;\n",
    "4. A way to remember which files are already in the index.\n",
    "\n",
    "Let's start with the first item on our bullet list. Once you have installed Xpdf, a program named `pdftotext` will be available. This program converts PDF files into .txt files. Fire up a terminal or commandline prompt and test whether the command `pdftotext` is available. The [subprocess](https://docs.python.org/3.4/library/subprocess.html) module provides different ways to execute external programs using Python. For example, on a Unix machine, the following lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "subprocess.call(['ls', '-l'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "will silently call the program `ls` with the list argument `-l` and return 0 to Python, meaning that the call has been completed. In a similar way we can call `pdftotext` to convert a PDF file into a text file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subprocess.call(['pdftotext', 'pdfs/blei2003.pdf', 'data/blei2003.txt'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If all went well, Python should return 0 to your notebook. Whoosh requires that all text is encoded in UTF-8. We can pass the argument `-enc UTF-8` to `pdftotext` to make sure our text files are in the right encoding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subprocess.call(['pdftotext', '-enc', 'UTF-8', 'pdfs/blei2003.pdf', 'data/blei2003.txt'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quiz!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function called `pdftotext` in Python that takes as argument the filename of a PDF file. It should convert the PDF into plain text and store the result in the directory `pydf/data`. The .txt file should have the same filename as the PDF file only with a different extension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Journal of Machine Learning Research 3 (2003) 993-1022\n",
      "\n",
      "Submitted 2/02; Published 1/03\n",
      "\n",
      "Latent Dirichlet Allocation\n",
      "David M. Blei\n",
      "\n",
      "BLEI @ CS . BERKELEY. EDU\n",
      "\n",
      "Computer Science Division\n",
      "University of California\n",
      "Berkeley, CA 94720, USA\n",
      "\n",
      "Andrew Y. Ng\n",
      "\n",
      "ANG @ CS . STANFORD . EDU\n",
      "\n",
      "Computer Science Department\n",
      "Stanford University\n",
      "Stanford, CA 94305, USA\n",
      "\n",
      "Michael I. Jordan\n",
      "\n",
      "JORDAN @ CS . BERKELEY. EDU\n",
      "\n",
      "Computer Science Division and Department of Statistics\n",
      "University of California\n",
      "Berkeley, CA 94720, USA\n",
      "\n",
      "Editor: John Lafferty\n",
      "\n",
      "Abstract\n",
      "We describe latent Dirichlet allocation (LDA), a generative probabilistic model for collections of\n",
      "discrete data such as text corpora. LDA is a three-level hierarchical Bayesian model, in which each\n",
      "item of a collection is modeled as a finite mixture over an underlying set of topics. Each topic is, in\n",
      "turn, modeled as an infinite mixture over an underlying set of topic probabilities. In the context of\n",
      "text modeling, the topic probabilities provide an explicit rep\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def pdftotext(pdf):\n",
    "    # insert your code here\n",
    "    basename, _ = os.path.splitext(os.path.basename(pdf))\n",
    "    subprocess.call(['pdftotext', '-enc', 'UTF-8',\n",
    "                     pdf, os.path.join('data', basename + '.txt')])\n",
    "\n",
    "# if your answer is correct this should print the first 1000 bytes of the text file\n",
    "pdftotext(\"pdfs/blei2003.pdf\")\n",
    "with open(os.path.join('data', 'blei2003.txt')) as infile:\n",
    "    print(infile.read(1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pdftotext` has the option `-htmlmeta` to extract some of the meta data stored in a pdf file. If we run the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subprocess.call(['pdftotext', '-htmlmeta', '-enc', 'UTF-8', \n",
    "                 'pdfs/muellner2011.pdf', 'data/muellner2011.html'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the output is a HTML file that looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Transitional//EN\" \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\"><html xmlns=\"http://www.w3.org/1999/xhtml\">\n",
      "<head>\n",
      "<title>Modern hierarchical, agglomerative clustering algorithms</title>\n",
      "<meta name=\"Subject\" content=\"\"/>\n",
      "<meta name=\"Keywords\" content=\"\"/>\n",
      "<meta name=\"Author\" content=\"Daniel Müllner\"/>\n",
      "<meta name=\"Creator\" content=\"LaTeX with hyperref package\"/>\n",
      "<meta name=\"Producer\" content=\"pdfTeX-1.40.10\"/>\n",
      "<meta name=\"CreationDate\" cont\n"
     ]
    }
   ],
   "source": [
    "print(open('data/muellner2011.html').read(500))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will write a function called `parse_html` to extract the meta information and the text from these files. It takes as argument the filepath of the HTML file and returns a dictionary formatted as follows:\n",
    "\n",
    "    d = {'author': AUTHOR, 'title': TITLE, 'text': TEXT}\n",
    "    \n",
    "We have used BeautifulSoup in the previous lesson to read and parse web pages. It will be of service here as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'author': 'Daniel Müllner',\n",
       " 'text': '\\nModern hierarchical, agglomerative\\nclustering algorithms\\n\\narXiv:1109.2378v1 [stat.ML] 12 Sep 2011\\n\\nDaniel Müllner\\n\\nThis paper presents algorithms for hierarchical, agglomerative clustering which\\nperform most efficiently in the general-purpose setup that is given in modern\\nstandard software. Requirements are: (1) the input data is given by pairwise dissimilarities between data points, but extensions to vector data are also discussed\\n(2) the output is a “stepwise dendrogram”, a data structure which is shared by\\nall implementations in current standard software. We present algorithms (old and\\nnew) which perform clustering in this setting efficiently, both in an asymptotic\\nworst-case analysis and from a practical point of view. The main contributions of\\nthis paper are: (1) We present a new algorithm which is suitable for any distance\\nupdate scheme and performs significantly better than the existing algorithms. (2)\\nWe prove the correctness of two algorithms by Rohlf and Murtagh, which is necessary in each case for different reasons. (3) We give well-founded recommendations\\nfor the best current algorithms for the various agglomerative clustering schemes.\\nKeywords: clustering, hierarchical, agglomerative, partition, linkage\\n\\n1 Introduction\\nHierarchical, agglomerative clustering is an important and well-established technique in unsupervised machine learning. Agglomerative clustering schemes start from the partition of\\nthe data set into singleton nodes and merge step by step the current pair of mutually closest\\nnodes into a new node until there is one final node left, which comprises the entire data set.\\nVarious clustering schemes share this procedure as a common definition, but differ in the way\\nin which the measure of inter-cluster dissimilarity is updated after each step. The seven most\\ncommon methods are termed single, complete, average (UPGMA), weighted (WPGMA, McQuitty), Ward, centroid (UPGMC) and median (WPGMC) linkage (see Everitt et al., 2011,\\nTable 4.1). They are implemented in standard numerical and statistical software such as R\\n(R Development Core Team, 2011), MATLAB (The MathWorks, Inc., 2011), Mathematica\\n(Wolfram Research, Inc., 2010), SciPy (Jones et al., 2001).\\nThe stepwise, procedural definition of these clustering methods directly gives a valid but\\ninefficient clustering algorithm. Starting with Gower’s and Ross’s observation (Gower and\\nRoss, 1969) that single linkage clustering is related to the minimum spanning tree of a graph\\nin 1969, several authors have contributed algorithms to reduce the computational complexity\\nof agglomerative clustering, in particular Sibson (1973), Rohlf (1973), Anderberg (1973, page\\n135), Murtagh (1984), Day and Edelsbrunner (1984, Table 5).\\n\\n1\\n\\n\\x0cEven when software packages do not use the inefficient primitive algorithm (as SciPy (Eads,\\n2007) and the R (R Development Core Team, 2011) methods hclust and agnes do), the\\nauthor found that implementations largely use suboptimal algorithms rather than improved\\nmethods suggested in theoretical work. This paper is to advance the theory further up to a\\npoint where the algorithms can be readily used in the standard setting, and in this way bridge\\nthe gap between the theoretical advances that have been made and the existing software\\nimplementations, which are widely used in science and industry.\\nThe main contributions of this paper are:\\n• We present a new algorithm which is suitable for any distance update scheme and performs significantly better than the existing algorithms for the “centroid” and “median”\\nclustering schemes.\\n• We prove the correctness of two algorithms, a single linkage algorithm by Rohlf (1973)\\nand Murtagh’s nearest-neighbor-chain algorithm (Murtagh, 1985, page 86). These\\nproofs were still missing, and we detail why the two proofs are necessary, each for\\ndifferent reasons.\\n• These three algorithms (together with an alternative by Sibson, 1973) are the best\\ncurrently available ones, each for its own subset of agglomerative clustering schemes.\\nWe justify this carefully, discussing potential alternatives.\\nThe specific class of clustering algorithms which is dealt with in this paper has been characterized by the acronym SAHN (sequential, agglomerative, hierarchic, nonoverlapping methods) by Sneath and Sokal (1973, § 5.4, 5.5). The procedural definition (which is given in\\nFigure 1 below) is not the only possibility for a SAHN method, but this method together with\\nthe seven common distance update schemes listed above is most widely used. The scope of\\nthis paper is contained further by practical considerations: We consider methods here which\\ncomply to the input and output requirements of the general-purpose clustering functions in\\nmodern standard software:\\n• The input to the algorithm is the list of N2 pairwise dissimilarities between N points.\\n(We mention extensions to vector data in Section 6.)\\n• The output is a so called stepwise dendrogram (see Section 2.2), in contrast to laxly\\nspecified output structure or weaker notions of (non-stepwise) dendrograms in earlier\\nliterature.\\nThe first item has always been a distinctive characteristic to previous authors since the input\\nformat broadly divides into the stored matrix approach (Anderberg, 1973, § 6.2) and the stored\\ndata approach (Anderberg, 1973, § 6.3). In contrast, the second condition has not been given\\nattention yet, but we will see that it affects the validity of algorithms.\\nWe do not aim to present and compare all available clustering algorithms but build upon\\nthe existing knowledge and present only the algorithms which we found the best for the given\\npurpose. For reviews and overviews we refer to Rohlf (1982), Murtagh (1983, 1985), Gordon\\n(1987, §3.1), Jain and Dubes (1988, § 3.2), Day (1996, § 4.2), Hansen and Jaumard (1997).\\nThose facts about alternative algorithms which are necessary to complete the discussion and\\nwhich are not covered in existing reviews are collected in Section 5.\\nThe paper is structured as follows:\\nSection 2 contains the definitions for input and output data structures as well as specifications of the distance update formulas and the “primitive” clustering algorithm.\\n\\n2\\n\\n\\x0cSection 3 is the main section of this paper. We present and discuss three algorithms: our\\nown “generic algorithm”, Murtagh’s nearest-neighbor-chain algorithm and Rohlf’s algorithm\\nbased on the minimum spanning tree of a graph. We prove the correctness of these algorithms.\\nSection 4 discusses the complexity of the algorithms, both as theoretical, asymptotic complexity in Section 4.1 and by use-case performance experiments in Section 4.2. We conclude\\nthis section by recommendations on which algorithm is the best one for each distance update\\nscheme, based on the preceding analysis.\\nSection 5 discusses alternative algorithms, and Section 6 gives a short outlook on extending\\nthe context of this paper to vector data instead of dissimilarity input. The paper ends with\\na brief conclusion in Section 7.\\nThe algorithms in this paper have been implemented in C++ by the author and are available\\nwith interfaces to the statistical software R and the programming language Python (van\\nRossum et al.). This implementation is presented elsewhere (Müllner, 2011).\\n\\n2 Data structures and the algorithmic definition of\\nSAHN clustering methods\\nIn this section, we recall the common algorithmic (procedural) definition of the SAHN clustering methods which demarcate the scope of this paper. Before we do so, we concretize the\\nsetting further by specifying the input and output data structures for the clustering methods.\\nEspecially the output data structure has not been specifically considered in earlier works, but\\nnowadays there is a de facto standard given by the shared conventions in the most widely\\nused software. Hence, we adopt the setting from practice and specialize our theoretical consideration to the modern standard of the stepwise dendrogram. Later, Section 5 contains an\\nexample of how the choice of the output data structure affects the result which algorithms\\nare suitable and/or most efficient.\\n\\n2.1 Input data structure\\nThe input to the hierarchical clustering algorithms in this paper is always a finite set together\\nwith a dissimilarity index (see Hansen and Jaumard, 1997, § 2.1).\\nDefinition. A dissimilarity index on a set S is a map d : S × S → [0, ∞) which is reflexive\\nand symmetric, i.e. we have d(x, x) = 0 and d(x, y) = d(y, x) for all x, y ∈ S.\\nA metric on S is certainly a dissimilarity index. In the scope of this paper, we call the\\nvalues of d distances in a synonymous manner to dissimilarities, even though they are not\\nrequired to fulfill the triangle inequalities, and dissimilarities between different elements may\\nbe zero.\\nIf the set S has N elements, a dissimilarity index is given by the N2 pairwise dissimilarities.\\nHence, the input size to the clustering algorithms is Θ(N 2 ). Once the primitive clustering\\nalgorithm is specified in Section 2.4, it is easy to see that the hierarchical clustering schemes\\nare sensitive to each input value. More precisely, for every input size N and for every index\\npair i = j, there are two dissimilarities which differ only at position (i, j) and which produce\\ndifferent output. Hence, all input values must be processed by a clustering algorithm, and\\ntherefore the run-time is bounded below by Ω(N 2 ).\\nThis bound applies to the general setting when the input is a dissimilarity index. In a\\ndifferent setting, the input could also be given as N points in a normed vector space of\\ndimension D (the “stored data approach”, Anderberg, 1973, §6.3). This results in an input\\nsize of Θ(N D), so that the lower bound does not apply for clustering of vector data. See\\n\\n3\\n\\n\\x0cSection 6 for a discussion to which extent the algorithms in this paper can be used in the\\n“stored data approach”.\\n\\n2.2 Output data structures\\nThe output of a hierarchical clustering procedure is traditionally a dendrogram. The term\\n“dendrogram” has been used with three different meanings: a mathematical object, a data\\nstructure and a graphical representation of the former two. In the course of this section, we\\ndefine a data structure and call it stepwise dendrogram. A graphical representation may be\\ndrawn from the data in one of several existing fashions. The graphical representation might\\nlose information (e.g. when two merges happen at the same dissimilarity value), and at the\\nsame time contain extra information which is not contained in the data itself (like a linear\\norder of the leaves).\\nIn the older literature, e.g. Sibson (1973), a dendrogram (this time, as a mathematical\\nobject) is rigorously defined as a piecewise constant, right-continuous map D : [0, ∞) → P(S),\\nwhere P(S) denotes the partitions of S, such that\\n• D(s) is always coarser than or equal to D(t) for s > t,\\n• D(s) eventually becomes the one-set partition {S} for large s.\\nA dendrogram in this sense with the additional condition that D(0) is the singleton partition\\nis in one-to-one correspondence to an ultrametric on S (Johnson, 1967, § I). The ultrametric\\ndistance between x and y is given by µ(x, y) = min{s ≥ 0 | x ∼ y in D(s)}. Conversely,\\nthe partition at level s ≥ 0 in the dendrogram is given by the equivalence relation x ∼ y ⇔\\nµ(x, y) ≤ s. Sibson’s “pointer representation” and “packed representation” (Sibson, 1973, § 4)\\nare examples of data structures which allow the compact representation of a dendrogram or\\nultrametric.\\nIn the current software, however, the output of a hierarchical clustering procedure is a\\ndifferent data structure which conveys potentially more information. We call this a stepwise\\ndendrogram.\\nDefinition. Given a finite set S0 with cardinality N = |S0 |, a stepwise dendrogram is a list\\nof N − 1 triples (ai , bi , δi ) (i = 0, . . . , N − 2) such that δi ∈ [0, ∞) and ai , bi ∈ Si , where Si+1\\nis recursively defined as (Si \\\\ {ai , bi }) ∪ ni and ni ∈\\n/ S \\\\ {ai , bi } is a label for a new node.\\nThis has the following interpretation: The set S0 are the initial data points. In each step,\\nni is the new node which is formed by joining the nodes ai and bi at the distance δi . The\\norder of the nodes within each pair (ai , bi ) does not matter. The procedure contains N − 1\\nsteps, so that the final state is a single node which contains all N initial nodes.\\n(The mathematical object behind this data structure is a sequence of N + 1 distinct, nested\\npartitions from the singleton partition to the one-set partition, together with a nonnegative\\nreal number for each partition. We do not need this abstract point of view here, though.)\\nThe identity of the new labels ni is not part of the data structure; instead it is assumed that\\nthey are generated according to some rule which is part of the specific data format convention.\\nIn view of this, it is customary to label the initial data points and the new nodes by integers.\\nFor example, the following schemes are used in software:\\n• R convention: S0 := (−1, . . . , −N ), new nodes: (1, . . . , N − 1)\\n• SciPy convention: S0 := (0, . . . , N − 1), new nodes: (N, . . . , 2N − 2)\\n• MATLAB convention: S0 := (1, . . . , N ), new nodes: (N + 1, . . . , 2N − 1)\\n\\n4\\n\\n\\x0cWe regard a stepwise dendrogram both as an ((N − 1) × 3)-matrix or as a list of triples,\\nwhichever is more convenient in a given situation.\\nIf the sequence (δi ) in Section 2.2 is non-decreasing, one says that the stepwise dendrogram\\ndoes not contain inversions, otherwise it does.\\nIn contrast to the first notion of a dendrogram above, a stepwise dendrogram can take\\ninversions into account, which an ordinary dendrogram cannot. Moreover, if more than two\\nnodes are joined at the same distance, the order of the merging steps does matter in a stepwise\\ndendrogram. Consider e.g. the following data sets with three points:\\n(A)\\n2.0\\n\\nx1\\n\\n•\\n\\n(B)\\n\\nx0\\n•\\n\\n2.0\\n\\n2.0\\n\\n•\\nx2\\n\\n3.0\\n\\nx2\\n\\n•\\n\\nx1\\n•\\n\\n(C)\\n2.0\\n\\n2.0\\n\\n•\\nx0\\n\\n3.0\\n\\nx0\\n\\n•\\n\\nx2\\n•\\n\\n2.0\\n\\n•\\nx1\\n\\n3.0\\n\\nThe array\\n0, 1, 2.0\\n2, 3, 2.0\\nis a valid output (SciPy conventions) for single linkage clustering on the data sets (A) and\\n(B) but not for (C). Even more, there is no stepwise dendrogram which is valid for all three\\ndata sets simultaneously. On the other hand, the non-stepwise single linkage dendrogram is\\nthe same in all cases:\\nD(s) =\\n\\n{x0 }, {x1 }, {x2 }\\n{x0 , x1 , x2 }\\n\\nif s < 2\\nif s ≥ 2\\n\\n2.0\\n\\npictorially:\\n\\nx0\\n\\nx1\\n\\nx2\\n\\nHence, a stepwise dendrogram conveys slightly more information than a non-stepwise dendrogram in the case of ties (i.e. when more than one merging step occurs at a certain distance).\\nThis must be taken into account when we check the correctness of the algorithms. Although\\nthis complicates the proofs in Sections 3.3 and 3.2 and takes away from the simplicity of the\\nunderlying ideas, it is not a matter of hairsplitting: E.g. Sibson’s SLINK algorithm (Sibson,\\n1973) for single linkage clustering works flawlessly if all distances are distinct but produces\\nthe same output on all data sets (A), (B) and (C). Hence, the output cannot be converted\\ninto a stepwise dendrogram. See Section 5 for further details.\\n\\n2.3 Node labels\\nThe node labels ni in a stepwise dendrogram may be chosen as unique integers according to\\none of the schemes described in the last section. In an implementation, when the dissimilarities\\nare stored in a large array in memory, it is preferable if each node label ni for the joined cluster\\nreuses one of the indices ai , bi of its constituents, so that the dissimilarities can be updated\\nin-place. Since the clusters after each row in the dendrogram form a partition of the initial set\\nS0 , we can identify each cluster not only by its label but also by one of its members. Hence, if\\nthe new node label ni is chosen among ai , bi , this is sufficient to reconstruct the partition at\\nevery stage of the clustering process, and labels can be converted to any other convention in\\na postprocessing step. Generating unique labels from cluster representatives takes only Θ(N )\\ntime and memory with a suitable union-find data structure. See Section 3.2 and Figure 5 for\\ndetails.\\n\\n5\\n\\n\\x0cFigure 1 Algorithmic definition of a hierarchical clustering scheme.\\nprocedure Primitive_clustering(S, d)\\nN ← |S|\\nL ← []\\nsize[x] ← 1 for all x ∈ S\\nfor i ← 0, . . . , N − 2 do\\n(a, b) ← argmin(S×S)\\\\∆ d\\nAppend (a, b, d[a, b]) to L.\\n8:\\nS ← S \\\\ {a, b}\\n9:\\nCreate a new node label n ∈\\n/ S.\\n10:\\nUpdate d with the information\\n1:\\n\\n2:\\n3:\\n4:\\n5:\\n6:\\n7:\\n\\nS: node labels, d: pairwise dissimilarities\\nNumber of input nodes\\nOutput list\\n\\nd[n, x] = d[x, n] = Formula(d[a, x], d[b, x], d[a, b], size[a], size[b], size[x])\\nfor all x ∈ S.\\nsize[n] ← size[a] + size[b]\\nS ← S ∪ {n}\\nend for\\nreturn L\\nthe stepwise dendrogram, an ((N − 1) × 3)-matrix\\n15: end procedure\\n(As usual, ∆ denotes the diagonal in the Cartesian product S × S.)\\n11:\\n12:\\n13:\\n14:\\n\\n2.4 The primitive clustering algorithm\\nThe solution that we expect from a hierarchical clustering algorithm is defined procedurally.\\nAll algorithms in this paper are measured against the primitive algorithm in Figure 1. We state\\nit in a detailed form to point out exactly which information about the clusters is maintained:\\nthe pairwise dissimilarities and the number of elements in each cluster.\\nThe function Formula in line 10 is the distance update formula, which returns the distance\\nfrom a node x to the newly formed node a ∪ b in terms of the dissimilarities between clusters\\na, b and x and their sizes. The table in Figure 2 lists the formulas for the common distance\\nupdate methods.\\nFor five of the seven formulas, the distance between clusters does not depend on the order\\nwhich the clusters were formed by merging. In this case, we also state closed, non-iterative\\nformulas for the cluster dissimilarities in the third row in Figure 2. The distances in the\\n“weighted” and the “median” update scheme depend on the order, so we cannot give noniterative formulas.\\nThe “centroid” and “median” formulas can produce inversions in the stepwise dendrograms;\\nthe other five methods cannot. This can be checked easily: The sequence of dissimilarities at\\nwhich clusters are merged in Figure 1 cannot decrease if the following condition is fulfilled for\\nall disjoint subsets I, J, K ⊂ S0 :\\nd(I, J) ≤ min{d(I, K), d(J, K)}\\n\\n⇒\\n\\nd(I, J) ≤ d(I ∪ J, K)\\n\\nOn the other hand, configurations with inversion in the “centroid” and “median” schemes\\ncan be easily produced, e.g. three points near the vertices of an equilateral triangle in R2 .\\nThe primitive algorithm takes Θ(N 3 ) time since in the i-th iteration of N − 1 in total, all\\nN −1−i\\n∈ Θ(i2 ) pairwise distances between the N − i nodes in S are searched.\\n2\\nNote that the stepwise dendrogram from a clustering problem (S, d) is not always uniquely\\ndefined, since the minimum in line 6 of the algorithm might be attained for several index\\n\\n6\\n\\n\\x0cFigure 2 Agglomerative clustering schemes.\\nName\\n\\nDistance update formula\\nFormula for d(I ∪ J, K)\\n\\nsingle\\n\\nmin(d(I, K), d(J, K))\\n\\ncomplete\\n\\nmax(d(I, K), d(J, K))\\n\\naverage\\n\\nnI d(I, K) + nJ d(J, K)\\nnI + nJ\\n\\nweighted\\nWard\\n\\nCluster dissimilarity\\nbetween clusters A and B\\nmin\\n\\na∈A,b∈B\\n\\nd[a, b]\\n\\nmax d[a, b]\\n\\na∈A,b∈B\\n\\n1\\n|A||B|\\n\\nd[a, b]\\na∈A b∈B\\n\\nd(I, K) + d(J, K)\\n2\\n(nI + nK )d(I, K) + (nJ + nK )d(J, K) − nK d(I, J)\\nnI + nJ + nK\\n\\n2|A||B|\\n· cA − cB\\n|A| + |B|\\n\\ncentroid\\n\\nnI d(I, K) + nJ d(J, K) nI nJ d(I, J)\\n−\\nnI + nJ\\n(nI + nJ )2\\n\\ncA − cB\\n\\nmedian\\n\\nd(I, K) d(J, K) d(I, J)\\n+\\n−\\n2\\n2\\n4\\n\\nwA − w B\\n\\n2\\n\\n2\\n\\n2\\n\\nLegend: Let I, J be two clusters joined into a new cluster, and let K be any other cluster.\\nDenote by nI , nJ and nK the sizes of (i.e. number of elements in) clusters I, J, K, respectively.\\nThe update formulas for the “Ward”, “centroid” and “median” methods assume that the input\\npoints are given as vectors in Euclidean space with the Euclidean distance as dissimilarity\\nmeasure. The expression cX denotes the centroid of a cluster X. The point wX is defined\\niteratively and depends on the clustering steps: If the cluster L is formed by joining I and J,\\nwe define wL as the midpoint 21 (wI + wJ ).\\nAll these formulas can be subsumed (for squared Euclidean distances in the three latter cases)\\nunder a single formula\\nd(I ∪ J, K) := αI d(I, K) + αJ d(J, K) + βd(I, J) + γ|d(I, K) − d(J, K)|,\\nwhere the coefficients αI , αJ , β may depend on the number of elements in the clusters I, J\\nand K. For example, αI = αJ = 21 , β = 0, γ = − 12 gives the single linkage formula. All\\nclustering methods which use this formula are combined under the name “flexible” in this\\npaper, as introduced by Lance and Williams (1967).\\nReferences: Lance and Williams (1967), Kaufman and Rousseeuw (1990, §5.5.1)\\n\\n7\\n\\n\\x0cpairs. We consider every possible output of Primitive_clustering under any choices of\\nminima as a valid output.\\n\\n3 Algorithms\\nIn the main section of this paper, we present three algorithms which are the most efficient ones\\nfor the task of SAHN clustering with the stored matrix approach. Two of the algorithms were\\ndescribed previously: The nearest-neighbor chain (“NN-chain”) algorithm by Murtagh (1985,\\npage 86), and an algorithm by Rohlf (1973), which we call “MST-algorithm” here since it is\\nbased on Prim’s algorithm for the minimum spanning tree of a graph. Both algorithms were\\npresented by the respective authors, but for different reasons each one still lacks a correctness\\nproof. Sections 3.2 and 3.3 state the algorithms in a way which is suitable for modern standard\\ninput and output structures and supply the proofs of correctness.\\nThe third algorithm in Section 3.1 is a new development based on Anderberg’s idea to\\nmaintain a list of nearest neighbors for each node Anderberg (1973, pages 135–136). While\\nwe do not show that the worst-case behavior of our algorithm is better than the O(N 3 ) worstcase complexity of Anderberg’s algorithm, the new algorithm is for all inputs at least equally\\nfast, and we show by experiments in Section 4.2 that the new algorithm is considerably faster\\nin practice since it cures Anderberg’s algorithm from its worst-case behavior at random input.\\nAs we saw in the last section, the solution to a hierarchical clustering task does not have a\\nsimple, self-contained specification but is defined as the outcome of the “primitive” clustering\\nalgorithm. The situation is complicated by the fact that the primitive clustering algorithm\\nitself is not completely specified: if a minimum inside the algorithm is attained at more than\\none place, a choice must be made. We do not require that ties are broken in a specific\\nway; instead we consider any output of the primitive algorithm under any choices as a valid\\nsolution. Each of the “advanced” algorithms is considered correct if it always returns one of\\nthe possible outputs of the primitive algorithm.\\n\\n3.1 The generic clustering algorithm\\nThe most generally usable algorithm is described in this section. We call it Generic_\\nlinkage since it can be used with any distance update formula. It is the only algorithm among\\nthe three in this paper which can deal with inversions in the dendrogram. Consequentially,\\nthe “centroid” and “median” methods must use this algorithm.\\nThe algorithm is presented in Figure 3. It is a sophistication of the primitive clustering\\nalgorithm and of Anderberg’s approach (Anderberg, 1973, pages 135–136). Briefly, candidates\\nfor nearest neighbors of clusters are cached in a priority queue to speed up the repeated\\nminimum searches in line 6 of Primitive_clustering.\\nFor the pseudocode in Figure 3, we assume that the set S are integer indices from 0 to\\nN − 1. This is the way in which it may be done in an implementation, and it makes the\\ndescription easier than for an abstract index set S. In particular, we rely on an order of the\\nindex set (see e.g. line 6: the index ranges over all y > x).\\nThere are two levels of sophistication from the primitive clustering algorithm to our generic\\nclustering algorithm. In a first step, one can maintain a list of nearest neighbors for each\\ncluster. For the sake of speed, it is enough to search for the nearest neighbors of a node x\\nonly among the nodes with higher index y > x. Since the dissimilarity index is symmetric, this\\nlist will still contain a pair of closest nodes. The list of nearest neighbors speeds up the global\\nminimum search in the i-th step from N −i−1\\ncomparisons to N − i − 1 comparisons at the\\n2\\nbeginning of each iteration. However, the list of nearest neighbors must be maintained: if the\\n\\n8\\n\\n\\x0cFigure 3 The generic clustering algorithm.\\n1:\\n2:\\n3:\\n4:\\n5:\\n6:\\n7:\\n8:\\n9:\\n10:\\n11:\\n12:\\n13:\\n14:\\n15:\\n16:\\n17:\\n18:\\n19:\\n20:\\n21:\\n22:\\n23:\\n24:\\n25:\\n26:\\n27:\\n28:\\n29:\\n30:\\n31:\\n32:\\n33:\\n34:\\n35:\\n36:\\n37:\\n38:\\n39:\\n40:\\n41:\\n42:\\n43:\\n\\nprocedure Generic_linkage(N, d)\\nN : input size, d: pairwise dissimilarities\\nS ← (0, . . . , N − 1)\\nL ← []\\nOutput list\\nsize[x] ← 1 for all x ∈ S\\nfor x in S \\\\ {N − 1} do\\nGenerate the list of nearest neighbors.\\nn_nghbr[x] ← argminy>x d[x, y]\\nmindist[x] ← d[x, n_nghbr[x]]\\nend for\\nQ ← (priority queue of indices in S \\\\ {N − 1}, keys are in mindist)\\nfor i ← 1, . . . , N − 1 do\\nMain loop.\\na ← (minimal element of Q)\\nb ← n_nghbr[a]\\nδ ← mindist[a]\\nwhile δ = d[a, b] do\\nRecalculation of nearest neighbors, if necessary.\\nn_nghbr[a] ← argminx>a d[a, x]\\nUpdate mindist and Q with (a, d[a, n_nghbr[a]])\\na ← (minimal element of Q)\\nb ← n_nghbr[a]\\nδ ← mindist[a]\\nend while\\nRemove the minimal element a from Q.\\nAppend (a, b, δ) to L.\\nMerge the pairs of nearest nodes.\\nsize[b] ← size[a] + size[b]\\nRe-use b as the index for the new node.\\nS ← S \\\\ {a}\\nfor x in S \\\\ {b} do\\nUpdate the distance matrix.\\nd[x, b] ← d[b, x] ← Formula(d[a, x], d[b, x], d[a, b], size[a], size[b], size[x])\\nend for\\nfor x in S such that x < a do\\nUpdate candidates for nearest neighbors.\\nif n_nghbr[x] = a then\\nDeferred search; no nearest\\nn_nghbr[x] ← b\\nneighbors are searched here.\\nend if\\nend for\\nfor x in S such that x < b do\\nif d[x, b] < mindist[x] then\\nn_nghbr[x] ← b\\nUpdate mindist and Q with (x, d[x, b])\\nPreserve a lower bound.\\nend if\\nend for\\nn_nghbr[b] ← argminx>b d[b, x]\\nUpdate mindist and Q with (b, d[b, n_nghbr[b]])\\nend for\\nreturn L\\nThe stepwise dendrogram, an ((N − 1) × 3)-matrix.\\nend procedure\\n\\n9\\n\\n\\x0cnearest neighbor of a node x is one of the clusters a, b which are joined, then it is sometimes\\nnecessary to search again for the nearest neighbor of x among all nodes y > x. Altogether,\\nthis reduces the best-case complexity of the clustering algorithm from Θ(N 3 ) to Θ(N 2 ), while\\nthe worst case complexity remains O(N 3 ). This is the method that Anderberg suggested in\\n(Anderberg, 1973, pages 135–136).\\nOn a second level, one can try to avoid or delay the nearest neighbor searches as long as\\npossible. Here is what the algorithm Generic_linkage does: It maintains a list n_nghbr\\nof candidates for nearest neighbors, together with a list mindist of lower bounds for the\\ndistance to the true nearest neighbor. If the distance d[x, n_nghbr[x]] is equal to mindist[x],\\nwe know that we have the true nearest neighbor, since we found a realization of the lower\\nbound; otherwise the algorithm must search for the nearest neighbor of x again.\\nTo further speed up the minimum searches, we also make the array mindist into a priority\\nqueue, so that the current minimum can be found quickly. We require a priority queue Q with\\na minimal set of methods as in the list below. This can be implemented conveniently by a\\nbinary heap (see Cormen et al., 2009, Chapter 6). We state the complexity of each operation\\nby the complexity for a binary heap.\\n• Queue(v): Generate a new queue from a vector v of length |v| = N . Return: an object\\nQ. Complexity: O(N ).\\n• Q.Argmin: Return the index to a minimal value of v. Complexity: O(1).\\n• Q.Remove_Min: Remove the minimal element from the queue. Complexity: O(log N ).\\n• Q.Update(i, x): Assign v[i] ← x and update the queue accordingly. Complexity:\\nO(log N ).\\nWe can now describe the Generic_linkage algorithm step by step: Lines 5 to 8 search\\nthe nearest neighbor and the closest distance for each point x among all points y > x. This\\ntakes O(N 2 ) time. In line 9, we generate a priority queue from the list of nearest neighbors\\nand minimal distances.\\nThe main loop is from line 10 to the end of the algorithm. In each step, the list L for\\na stepwise dendrogram is extended by one row, in the same way as the primitive clustering\\nalgorithm does.\\nLines 11 to 20 find a current pair of closest nodes. A candidate for this is the minimal index\\nin the queue (assigned to a), and its candidate for the nearest neighbor b := n_nghbr[a]. If\\nthe lower bound mindist[a] is equal to the actual dissimilarity d[a, b], then we are sure that\\nwe have a pair of closest nodes and their distance. Otherwise, the candidates for the nearest\\nneighbor and the minimal distance are not the true values, and we find the true values for\\nn_nghbr[a] and mindist[a] in line 15 in O(N ) time. We repeat this process and extract the\\nminimum among all lower bounds from the queue until we find a valid minimal entry in the\\nqueue and therefore the actual closest pair of points.\\nThis procedure is the performance bottleneck of the algorithm. The algorithm might be\\nforced to update the nearest neighbor O(N ) times with an effort of O(N ) for each of the\\nO(N ) iterations, so the worst-case performance is bounded by O(N 3 ). In practice, the inner\\nloop from lines 14 to 20 is executed less often, which results in faster performance.\\nLines 22 to 27 are nearly the same as in the primitive clustering algorithm. The only\\ndifference is that we specialize from an arbitrary label for the new node to re-using the index\\nb for the joined node. The index a becomes invalid, and we replace any nearest-neighbor\\nreference to a by a reference to the new cluster b in lines 28 to 32. Note that the array\\nn_nghbr contains only candidates for the nearest neighbors, so we could have written any\\n\\n10\\n\\n\\x0cvalid index here; however, for the single linkage method, it makes sense to choose b: if the\\nnearest neighbor of a node was at index a, it is now at b, which represents the join a ∪ b.\\nThe remaining code in the main loop ensures that the array n_nghbr still contains lower\\nbounds on the distances to the nearest neighbors. If the distance from the new cluster x to\\na cluster b < x is smaller than the old bound for b, we record the new smallest distance and\\nthe new nearest neighbor in lines 34 to 37.\\nLines 39 and 40 finally find the nearest neighbor of the new cluster and record it in the\\narrays n_nghbr and mindist and the queue Q\\nThe main idea behind this approach is that invalidated nearest neighbors are not recomputed immediately. Suppose that the nearest neighbor of a node x is far away from\\nx compared to the global closed pair of nodes. Then it does not matter that we do not know\\nthe nearest neighbor of x, as long as we have a lower bound on the distance to the nearest\\nneighbor. The candidate for the nearest neighbor might remain invalid, and the true distance\\nmight remain unknown for many iterations, until the lower bound for the nearest-neighbor\\ndistance has reached the top of the queue Q. By then, the set of nodes S might be much\\nsmaller since many of them were already merged, and the algorithm might have avoided many\\nunnecessary repeated nearest-neighbor searches for x in the meantime.\\nThis concludes the discussion of our generic clustering algorithm; for the performance see\\nSection 4. Our explanation of how the minimum search is improved also proves the correctness\\nof the algorithm: Indeed, in the same way as the primitive algorithm does, the Generic_\\nlinkage algorithm finds a pair of globally closest nodes in each iteration. Hence the output\\nis always the same as from the primitive algorithm (or more precisely: one of several valid\\npossibilities if the closest pair of nodes is not unique in some iteration).\\n\\n3.2 The nearest-neighbor-chain algorithm\\nIn this section, we present and prove correctness of the nearest-neighbor-chain algorithm\\n(shortly: NN-chain algorithm), which was described by Murtagh (1985, page 86). This algorithm can be used for the “single”, “complete”, “average”, “weighted” and “Ward” methods.\\nThe NN-chain algorithm is presented in Figure 4 as NN-chain-linkage. It consists of the\\ncore algorithm NN-chain-core and two postprocessing steps. Because of the postprocessing,\\nwe call the output of NN-chain-core an unsorted dendrogram. The unsorted dendrogram\\nmust first be sorted row-wise, with the dissimilarities in the third column as the sorting key.\\nIn order to correctly deal with merging steps which happen at the same dissimilarity, it is\\ncrucial that a stable sorting algorithm is employed, i.e. one which preserves the relative order\\nof elements with equal sorting keys. At this point, the first two columns of the output array\\nL contain the label of a member of the respective cluster, but not the unique label of the\\nnode itself. The second postprocessing step is to generate correct node labels from cluster\\nrepresentatives. This can be done in Θ(N ) time with a union-find data structure. Since this\\nis a standard technique, we do not discuss it here but state an algorithm in Figure 5 for the\\nsake of completeness. It generates integer node labels according to the convention in SciPy\\nbut can easily be adapted to follow any convention.\\nWe prove the correctness of the NN-chain algorithm in this paper for two reasons:\\n• We make sure that the algorithm resolves ties correctly, which was not in the scope of\\nearlier literature.\\n• Murtagh claims (Murtagh, 1984, page 111), (Murtagh, 1985, bottom of page 86) that\\nthe NN-chain algorithm works for any distance update scheme which fulfills a certain\\n“reducibility property”\\n\\n11\\n\\n\\x0cd(I, J) ≤ min{d(I, K), d(J, K)}\\n\\n⇒\\n\\nmin{d(I, K), d(J, K)} ≤ d(I ∪ J, K)\\n\\n(1)\\n\\nfor all disjoint nodes I, J, K at any stage of the clustering (Murtagh, 1984, § 3), (Murtagh, 1985, § 3.5). This is false.1 We give a correct proof which also shows the limitations\\nof the algorithm. In Murtagh’s papers (Murtagh, 1984, 1985), it is not taken into account\\nthat the dissimilarity between clusters may depend on the order of clustering steps; on\\nthe other hand, it is explicitly said that the algorithm works for the “weighted” scheme,\\nin which dissimilarities depend on the order of the steps.\\nSince there is no published proof for the NN-chain algorithm but claims which go beyond\\nwhat the algorithm can truly do, it is necessary to establish the correctness by a strict proof:\\nTheorem 1. Fix a distance update formula. For any sequence of merging steps and any four\\ndisjoint clusters I, J, K, L resulting from these steps, require two properties from the distance\\nupdate formula:\\n• It fulfills the reducibility property (1).\\n• The distance d(I ∪ J, K ∪ L) is independent of whether (I, J) are merged first and then\\n(K, L) or the other way round.\\nThen the algorithm NN-chain-linkage produces valid stepwise dendrograms for the given\\nmethod.\\nProposition 2. The “single”, “complete”, “average”, “weighted” and “Ward” distance update\\nformulas fulfill the requirements of Theorem 1.\\nProof of Theorem 1. We prove the theorem by induction in the size of the input set S. The\\ninduction start is trivial since a dendrogram for a one-point set is empty.\\nWe call two nodes a, b ∈ S reciprocal nearest neighbors (“pairwise nearest neighbors” in the\\nterminology of Murtagh, 1985) if the distance d[a, b] is minimal among all distances from a to\\npoints in S, and also minimal among all distances from b:\\nd[a, b] = min d[a, x] = min d[b, x].\\nx∈S\\nx=a\\n\\nx∈S\\nx=b\\n\\nEvery finite set S with at least two elements has at least one pair of reciprocal nearest\\nneighbors, namely a pair which realizes the global minimum distance.\\nThe list chain is in the algorithm constructed in a way such that every element is a nearest\\nneighbor of its predecessor. If chain ends in [. . . , b, a, b], we know that a and b are reciprocal\\nnearest neighbors. The main idea behind the algorithm is that reciprocal nearest neighbors\\na, b always contribute a row (a, b, d[a, b]) to the stepwise dendrogram, even if they are not\\ndiscovered in ascending order of dissimilarities.\\n1 For\\n\\nexample, consider the distance update formula d(I ∪ J, K) := d(I, K) + d(J, K) + d(I, J). This formula\\nfulfills the reducibility condition. Consider the following distance matrix between five points in the first\\ncolumn below. The Primitive_clustering algorithm produces the correct stepwise dendrogram in the\\nmiddle column. However, if the point A is chosen first in line 7 of NN-chain-core, the algorithm outputs\\nthe incorrect dendrogram in the right column.\\nA\\nB\\nC\\nD\\n\\nB\\n3\\n\\nC\\n4\\n5\\n\\nD\\n6\\n7\\n1\\n\\nE\\n15\\n12\\n13\\n14\\n\\n(C,\\n(A,\\n(AB,\\n(ABCD,\\n\\n12\\n\\nD,\\nB,\\nCD,\\nE,\\n\\n1)\\n3)\\n27)\\n85)\\n\\n(C,\\n(A,\\n(CD,\\n(AB,\\n\\nD,\\nB,\\nE,\\nCDE,\\n\\n1)\\n3)\\n28)\\n87)\\n\\n\\x0cFigure 4 The nearest-neighbor clustering algorithm.\\n1:\\n2:\\n3:\\n4:\\n5:\\n6:\\n\\nprocedure NN-chain-linkage(S, d)\\nS: node labels, d: pairwise dissimilarities\\nL ← NN-chain-core(N, d)\\nStably sort L with respect to the third column.\\nL ← Label(L)\\nFind node labels from cluster representatives.\\nreturn L\\nend procedure\\n\\n1:\\n2:\\n3:\\n\\nprocedure NN-chain-core(S, d)\\nS: node labels, d: pairwise dissimilarities\\nS ← (0, . . . , N − 1)\\nchain = [ ]\\nsize[x] ← 1 for all x ∈ S\\nwhile |S| > 1 do\\nif length(chain) ≤ 3 then\\na ← (any element of S)\\nE.g. S[0]\\nchain ← [a]\\nb ← (any element of S \\\\ {a})\\nE.g. S[1]\\nelse\\na ← chain[−4]\\nb ← chain[−3]\\nRemove chain[−1], chain[−2] and chain[−3]\\nCut the tail (x, y, x).\\nend if\\nrepeat\\nc ← argminx=a d[x, a] with preference for b\\na, b ← c, a\\nAppend a to chain\\nuntil length(chain) ≥ 3 and a = chain[−3]\\na, b are reciprocal\\nAppend (a, b, d[a, b]) to L\\nnearest neighbors.\\nRemove a, b from S\\nn ← (new node label)\\nsize[n] ← size[a] + size[b]\\nUpdate d with the information\\n\\n4:\\n5:\\n6:\\n7:\\n8:\\n9:\\n10:\\n11:\\n12:\\n13:\\n14:\\n15:\\n16:\\n17:\\n18:\\n19:\\n20:\\n21:\\n22:\\n23:\\n24:\\n\\nd[n, x] = d[x, n] = Formula(d[a, x], d[b, x], d[a, b], size[a], size[b], size[x])\\nfor all x ∈ S.\\nS ← S ∪ {n}\\nend while\\nreturn L\\nan unsorted dendrogram\\nend procedure\\n(We use the Python index notation: chain[−2] is the second-to-last element in the list chain.)\\n25:\\n26:\\n27:\\n28:\\n\\n13\\n\\n\\x0cFigure 5 A union-find data structure suited for the output conversion.\\n1:\\n2:\\n3:\\n4:\\n5:\\n6:\\n7:\\n8:\\n9:\\n10:\\n\\nprocedure Label(L)\\nL ← []\\nN ← (number of rows in L) + 1\\nNumber of initial nodes.\\nU ← new Union-Find(N )\\nfor (a, b, δ) in L do\\nAppend (U.Efficient-Find(a), U.Efficient-Find(b), δ) to L\\nU.Union(a, b)\\nend for\\nreturn L\\nend procedure\\n\\nclass Union-Find\\nmethod Constructor(N )\\nparent ← new int[2N − 1]\\nparent[0, . . . , 2N − 2] ← None\\n15:\\nnextlabel ← N\\n16:\\nend method\\n\\n11:\\n12:\\n13:\\n14:\\n\\n17:\\n18:\\n19:\\n20:\\n21:\\n22:\\n23:\\n24:\\n25:\\n26:\\n27:\\n28:\\n29:\\n30:\\n31:\\n32:\\n33:\\n34:\\n35:\\n36:\\n37:\\n38:\\n\\nmethod Union(m, n)\\nparent[m] = nextlabel\\nparent[n] = nextlabel\\nnextlabel ← nextlabel + 1\\nend method\\n\\nN is the number of data points.\\n\\nSciPy convention: new labels start at N\\n\\nSciPy convention: number new labels consecutively\\n\\nmethod Find(n)\\nThis works but the search process is not efficient.\\nwhile parent[n] is not None do\\nn ← parent[n]\\nend while\\nreturn n\\nend method\\nmethod Efficient-Find(n)\\np←n\\nwhile parent[n] is not None do\\nn ← parent[n]\\nend while\\nwhile parent[p] = n do\\np, parent[p] ← parent[p], n\\nend while\\nreturn n\\nend method\\nend class\\n\\nThis speeds up repeated calls.\\n\\n14\\n\\n\\x0cLines 15 to 19 in NN-chain-core clearly find reciprocal nearest neighbors (a, b) in S.\\nOne important detail is that the index b is preferred in the argmin search in line 16, if the\\nminimum is attained at several indices and b realizes the minimum. This can be respected in\\nan implementation with no effort, and it ensures that reciprocal nearest neighbors are indeed\\nfound. That is, the list chain never contains a cycle of length > 2, and a chain = [. . . , b, a]\\nwith reciprocal nearest neighbors at the end will always be extended by b, never with an\\nelement c = b which coincidentally has the same distance to a.\\nAfter line 19, the chain ends in (b, a, b). The nodes a and b are then joined, and the internal\\nvariables are updated as usual.\\nWe now show that the remaining iterations produce the same output as if the algorithm had\\nstarted with the set S := (S \\\\ {a, b}) ∪ {n}, where n is the new node label and the distance\\narray d and the size array are updated accordingly.\\nThe only data which could potentially be corrupted is that the list chain could not contain\\nsuccessive nearest neighbors any more, since the new node n could have become the nearest\\nneighbor of a node in the list.\\nAt the beginning of the next iteration, the last elements (b, a, b) are removed from chain.\\nThe list chain then clearly does not contain a or b at any place any more, since any occurrence\\nof a or b in the list would have led to an earlier pair of reciprocal nearest neighbors, before\\n(b, a, b) was appended to the list. Hence, chain contains only nodes which really are in S. Let\\ne, f be two successive entries in chain, i.e. f is a nearest neighbor of e. Then we know\\nd[e, f ] ≤ d[e, a]\\n\\nd[a, b] ≤ d[a, e]\\n\\nd[e, f ] ≤ d[e, b]\\n\\nd[a, b] ≤ d[b, e]\\n\\nTogether with the reducibility property (1) (for I = a, J = b, K = e), this implies d[e, f ] ≤\\nd[e, n]. Hence, f is still the nearest neighbor of e, which proves our assertion.\\nWe can therefore be sure that the remaining iterations of NN-chain-core produce the\\nsame output as if the algorithm would be run freshly on S . By the inductive assumption,\\nthis produces a valid stepwise dendrogram for the set S with N − 1 nodes. Proposition 3\\ncarries out the remainder of the proof, as it shows that the first line (a, b, d[a, b]) of the unsorted\\ndendrogram, when it is sorted into the right place in the dendrogram for the nodes in S , is\\na valid stepwise dendrogram for the original set S with N nodes.\\nProposition 3. Let (S, d) be a set with dissimilarities (|S| > 1). Fix a distance update\\nformula which fulfills the requirements in Theorem 1. Let a, b be two distinct nodes in S\\nwhich are reciprocal nearest neighbors.\\nDefine S as (S \\\\ {a, b}) ∪ {n}, where the label n represents the union a ∪ b. Let d be the updated dissimilarity matrix for S , according to the chosen formula. Let L = ((ai , bi , δi )i=0,...,m )\\nbe a stepwise dendrogram for S . Let j be the index such that all δi < d[a, b] for all i < j\\nand δi ≥ d[a, b] for all i ≥ j. That is, j is the index where the new row (a, b, d[a, b]) should\\nbe inserted to preserve the sorting order, giving d[a, b] priority over potentially equal sorting\\nkeys. Then the array L, which we define as\\n\\uf8ee\\n\\uf8f9\\na0\\nb0\\nδ0\\n\\uf8ef\\n\\uf8fa\\n\\uf8ef ...\\n\\uf8fa\\n\\uf8ef\\n\\uf8fa\\n\\uf8ef aj−1 bj−1 δj−1 \\uf8fa\\n\\uf8ef\\n\\uf8fa\\n\\uf8ef\\n\\uf8fa\\n→ \\uf8ef a\\nb\\nd[a, b] \\uf8fa ←\\n\\uf8ef\\n\\uf8fa\\n\\uf8ef aj\\n\\uf8fa\\nbj\\nδj\\n\\uf8ef\\n\\uf8fa\\n\\uf8ef\\n\\uf8fa\\n\\uf8f0 ...\\n\\uf8fb\\nam\\nbm\\nδm\\n\\n15\\n\\n\\x0cis a stepwise dendrogram for (S, d).\\nProof. Since a and b are reciprocal nearest neighbors at the beginning, the reducibility property (1) guarantees that they stay nearest neighbors after any number of merging steps between\\nother reciprocal nearest neighbors. Hence, the first j steps in a dendrogram for S cannot contain a or b, since these steps all happen at merging dissimilarities smaller than d[a, b]. This is\\nthe point where we must require that the sorting in line 3 of NN-chain-linkage is stable.\\nMoreover, the first j rows of L cannot contain a reference to n: Again by the reducibility\\nproperty, dissimilarities between n and any other node are at least as big as d[a, b]. Therefore,\\nthe first j rows of L are correct for a dendrogram for S.\\nAfter j steps, we know that no inter-cluster distances in S \\\\ {a, b} are smaller than d[a, b].\\nAlso, d[a, b] is minimal among all distances from a and b, so the row (a, b, d[a, b]) is a valid\\nnext row in L.\\nAfter this step, we claim that the situation is the same in both settings: The sets S after\\nj steps and the set S after j + 1 steps, including the last one merging a and b into a new\\ncluster n, are clearly equal as partitions of the original set. It is required to check that also\\nthe dissimilarities are the same in both settings. This is where we need the second condition\\nin Theorem 1:\\nThe row (a, b, d[a, b]) on top of the array L differs from the dendrogram L by j transpositions, where (a, b, d[a, b]) is moved one step downwards. Each transposition happens between\\ntwo pairs (a, b) and (ai , bi ), where all four nodes are distinct, as shown above. The dissimilarity from a distinct fifth node x to the join a ∪ b does not depend on the merging of ai and bi\\nsince there is no way in which dissimilarities to ai and bi enter the distance update formula\\nFormula(d[a, x], d[b, x], d[a, b], size[a], size[b], size[x]). The symmetric statement holds for the\\ndissimilarity d[x, ai ∪ bi ]. The nodes a, b, ai , bi are deleted after the two steps, so dissimilarities\\nlike d[a, ai ∪ bi ] can be neglected. The only dissimilarity between active nodes which could be\\naltered by the transposition is d[a ∪ b, ai ∪ bi ]. It is exactly the second condition in Theorem 1\\nthat this dissimilarity is independent of the order of merging steps. This finishes the proof of\\nTheorem 1.\\nWe still have to prove that the requirements of Theorem 1 are fulfilled by the “single”,\\n“complete”, “average”, “weighted” and “Ward” schemes:\\nProof of Proposition 2. It is easy and straightforward to check from the table in Figure 2\\nthat the distance update schemes in question fulfill the reducibility property. Moreover, the\\ntable also conveys that the dissimilarities between clusters in the “single”, “complete” and\\n“average” schemes do not depend on the order of the merging steps.\\nFor Ward’s scheme, the global dissimilarity expression in the third column in Figure 2\\napplies only if the dissimilarity matrix consists of Euclidean distances between vectors (which\\nis the prevalent setting for Ward’s method). For a general argument, note that the global\\ncluster dissimilarity for Ward’s method can also be expressed by a slightly more complicated\\nexpression:\\nd(A, B) =\\n\\n1\\n|A| + |B|\\n\\nd(a, b)2 −\\n\\n2\\na∈A b∈B\\n\\n|B|\\n|A|\\n\\nd(a, a )2 −\\na∈A a ∈A\\n\\n|A|\\n|B|\\n\\nd(b, b )2\\nb∈B b ∈B\\n\\nThis formula can be proved inductively from the recursive distance update formula for Ward’s\\nmethod, hence it holds independently of whether the data is Euclidean or not. This proves\\nthat the dissimilarities in Ward’s scheme are also independent of the order of merging steps.\\nDissimilarities in the “weighted” scheme, however, do in general depend on the order of\\nmerging steps. However, the dissimilarity between joined nodes I ∪ J and K ∪ L is always the\\n\\n16\\n\\n\\x0cFigure 6 The single linkage algorithm.\\n1:\\n2:\\n3:\\n4:\\n5:\\n6:\\n\\n1:\\n2:\\n3:\\n4:\\n5:\\n6:\\n7:\\n8:\\n9:\\n10:\\n11:\\n12:\\n13:\\n14:\\n15:\\n\\nprocedure MST-linkage(S, d)\\nS: node labels, d: pairwise dissimilarities\\nL ← MST-linkage-core(S, d)\\nStably sort L with respect to the third column.\\nL ← Label(L)\\nFind node labels from cluster representatives.\\nreturn L\\nend procedure\\nprocedure MST-linkage-core(S0 , d)\\nL ← []\\nc ← (any element of S0 )\\nD0 [s] ← ∞ for s ∈ S0 \\\\ {c}\\nfor i in (1, . . . , |S0 | − 1) do\\nSi ← Si−1 \\\\ {c}\\nfor s in Si do\\nDi [s] ← min{Di−1 [s], d[s, c]}\\nend for\\nn ← argmins∈Si Di [s]\\nAppend (c, n, Di [n]) to L\\nc←n\\nend for\\nreturn L\\nend procedure\\n\\nS0 : node labels, d: pairwise dissimilarities\\nc: current node\\n\\nn: new node\\n\\nan unsorted dendrogram\\n\\nmean dissimilarity 41 (d[I, K] + d[I, L] + d[J, K] + d[J, L]), independent of the order of steps,\\nand this is all that is required for Proposition 2.\\n\\n3.3 The single linkage algorithm\\nIn this section, we present and prove correctness of a fast algorithm for single linkage clustering. Gower and Ross (1969) observed that a single linkage dendrogram can be obtained\\nfrom a minimum spanning tree (MST) of the weighted graph which is given by the complete\\ngraph on the singleton set S with the dissimilarities as edge weights. The algorithm here\\nwas originally described by Rohlf (1973) and is based on Prim’s algorithm for the MST (see\\nCormen et al., 2009, § 23.2).\\nThe single linkage algorithm MST-linkage is given in Figure 6. In the same way as the NNchain algorithm, it consists of a core algorithm MST-linkage-core and two postprocessing\\nsteps. The output structure of the core algorithm is again an unsorted list of clustering\\nsteps with node representatives instead of unique labels. As will be proved, exactly the same\\npostprocessing steps can be used as for the NN-chain algorithm.\\nRohlf’s algorithm in its original version is a full Prim’s algorithm and maintains enough\\ndata to generate the MST. He also mentions a possible simplification which does not do\\nenough bookkeeping to generate an MST but enough for single linkage clustering. It is this\\nsimplification that is discussed in this paper. We prove the correctness of this algorithm for\\ntwo reasons:\\n• Since the algorithm MST-linkage-core does not generate enough information to reconstruct a minimum spanning tree, one cannot refer to the short proof of Prim’s algo-\\n\\n17\\n\\n\\x0crithm in any easy way to establish the correctness of MST-linkage.\\n• Like for the NN-chain algorithm in the last section, it is not clear a priori that the\\nalgorithm resolves ties correctly. A third algorithm can serve as a warning here (see\\nSection 5 for more details): There is an other fast algorithm for single linkage clustering, Sibson’s SLINK algorithm (Sibson, 1973). More or less by coincidence, all three\\nalgorithms NN-chain-core, MST-linkage-core and SLINK generate output which\\ncan be processed by exactly the same two steps: sorting followed by Label. In case\\nof the SLINK algorithm this works fine if all dissimilarities are distinct but produces\\nwrong stepwise dendrograms in situations when two merging dissimilarities are equal.\\nThere is nothing wrong with the SLINK algorithm, however. Sibson supplied a proof for\\nthe SLINK algorithm in his paper (Sibson, 1973), but it is written for a (non-stepwise)\\ndendrogram as the output structure, not for a stepwise dendrogram. Hence, the additional information which is contained in a stepwise dendrogram in the case of ties is not\\nprovided by all, otherwise correct algorithms.\\nThis should be taken as a warning that ties demand more from an algorithm and must be\\nexplicitly taken into account when we prove the correctness of the MST-linkage algorithm\\nbelow.\\nTheorem 4. The algorithm MST-linkage yields an output which can also be generated by\\nPrimitive_clustering.\\nWe do not explicitly refer to Prim’s algorithm in the following, and we make the proof selfcontained, since the algorithm does not collect enough information to construct a minimum\\nspanning tree. There are unmistakable similarities, of course, and the author got most of the\\nideas for this proof from Prim’s algorithm (see Cormen et al., 2009, § 23.2).\\nLet us first make two observations about the algorithm MST-linkage.\\n(a) Starting with the full initial set S0 , the algorithm MST-linkage-core chooses a “current node” c in each step and removes it from the current set Si in every iteration. Let\\nSic := S0 \\\\ Si be the complement of the current set Si . Then Di [s] (s ∈ Si ) is the\\ndistance from Sic to s, i.e.\\nDi [s] = minc d[s, t].\\nt∈Si\\n\\n(b) Let L be the output of MST-linkage-core(S, d). The 2i entries in the first two\\ncolumns and the first i rows contain only i + 1 distinct elements of S, since the second\\nentry in one row is the first entry in the next row.\\nWe prove Theorem 4 by the following stronger variant:\\nTheorem 5. Let L be the output of MST-linkage-core(S, d). For all n < |S|, the first n\\nrows of L are an unsorted single linkage dendrogram for the n + 1 points of S in this list (see\\nObservation (b)).\\nProof. We proceed by induction. After the first iteration, the list L contains one triple\\n(a0 , b0 , δ0 ). δ0 = D1 [b0 ] is clearly the dissimilarity d[a0 , b0 ], since the array D1 contains the\\ndissimilarities to a0 after the first iteration (Observation (a)).\\nLet (a0 , b0 , δ0 ), . . . , (an , bn , δn ) be the first n + 1 rows of L. We sort the rows with a stable\\nsorting algorithm as specified in MST-linkage. We leave the postprocessing step Label out\\nof our scope and work with the representatives ai , bi for the rest of the proof.\\n\\n18\\n\\n\\x0cLet s(0), . . . , s(n) be the stably sorted indices (i.e. δs(i) ≤ δs(i+1) for all i and s(i) < s(i + 1)\\nif δs(i) = δs(i+1) ). Let k be the sorted index of the last row n. Altogether, we have a sorted\\nmatrix\\n\\uf8f9\\n\\uf8ee\\nas(0)\\nbs(0)\\nδs(0)\\n\\uf8fa\\n\\uf8ef\\n\\uf8fa\\n\\uf8ef ...\\n\\uf8fa\\n\\uf8ef\\n\\uf8ef as(k−1) bs(k−1) δs(k−1) \\uf8fa\\n\\uf8fa\\n\\uf8ef\\n\\uf8fa\\n\\uf8ef\\n→ \\uf8ef as(k)\\nbs(k)\\nδs(k)\\n\\uf8fa ←\\n\\uf8fa\\n\\uf8ef\\n\\uf8ef as(k+1) bs(k+1) δs(k+1) \\uf8fa\\n\\uf8fa\\n\\uf8ef\\n\\uf8fa\\n\\uf8ef\\n\\uf8fb\\n\\uf8f0 ...\\nas(n)\\nbs(n)\\nδs(n)\\nThe new row is at the index k, i.e. (as(k) , bs(k) , δs(k) ) = (an , bn , δn ). The matrix without\\nthe k-th row is a valid stepwise, single linkage dendrogram for the points a0 , . . . , an , by the\\ninduction hypothesis. (Recall that bi = ai+1 .) Our goal is to show that the matrix with its\\nk-th row inserted yields a valid single linkage dendrogram on the points a0 , . . . , an , bn .\\nFirst step: rows 0 to k − 1. The distance δn is the minimal distance from bn to any\\nof the points a0 , . . . , an . Therefore, the dendrograms for the sets S − := {a0 , . . . , an } and\\nS + := S − ∪ {bn } have the same first k steps, when all the inter-cluster distances are smaller\\nthan or equal to δn . (If the distance δn occurs more than once, i.e. when δs(k−1) = δn , we\\nassume by stable sorting that the node pairs which do not contain bn are chosen first.)\\nTherefore, the first k rows are a possible output of Primitive_clustering in the first k\\nsteps. After this step, we have the same partial clusters in S + as in the smaller data set, plus\\na singleton {bn }.\\nSecond step: row k. The distance δn from bn to some point a0 , . . . , an is clearly the\\nsmallest inter-cluster distance at this point, since all other inter-cluster distances are at least\\nδs(k+1) , which is greater than δs(k) = δn . Since the output row is (an , bn , δn ), it remains to\\ncheck that the distance δn is realized as the distance from bn to a point in the cluster of an ,\\ni.e. that an is in a cluster with distance δn to bn .\\nThe clusters mentioned in the last sentence refer to the partition of S + which is generated\\nby the relations as(0) ∼ bs(0) , . . . , as(k−1) ∼ bs(k−1) . Since we have bi = ai+1 , the partition of\\nS + consists of contiguous chains in the original order of points a0 , a1 , . . . , an , bn .\\nThe diagram below visualizes a possible partition after k steps.\\na0 a1 a2 a3 a4 a5\\n\\n...\\n\\n. . . am\\n\\n. . . an bn\\n\\nIn this particular example, the distances δ0 , δ1 and δ4 are among the first k smallest, while\\nδ2 , δ3 and δ5 come later in the sorted order.\\nLet δn be realized as the distance between bn and am for some m ≤ n. Then the dissimilarities between consecutive points in the sequence am , bm = am+1 , bm+1 = am+2 , . . . ,\\nbn−1 = an must be less than or equal to δn ; otherwise bn and the dissimilarity δn = d[bn , am ]\\nwould have been chosen first over these other dissimilarities in one of the first k steps. Since\\nthe dissimilarities of all pairs (ai , bi ) in this chain are not more than δn , they are contained\\nin the first k sorted triples. Hence, am and an have been joined into a cluster in the first k\\nsteps, and an is a valid representative of the cluster that also contains am .\\nNote that the argument in the last paragraph is the point where we need that the sorting\\nin line 3 of MST-linkage is stable. Otherwise it could not be guaranteed that am and an\\nhave been joined into a cluster before bn is added.\\nThird step: rows k + 1 to n. Here is the situation after row k: We have the same clusters\\nin S + as after k steps in the smaller data set S, except that the last cluster (the one which\\n\\n19\\n\\n\\x0ccontains an ) additionally contains the point bn . In a diagram:\\na0 a1 a2 a3 a4 a5\\n\\n...\\n\\n. . . am\\n\\n. . . an bn\\n\\nThe inter-cluster distances in S + from the cluster with bn to the other clusters might be\\nsmaller than without the point bn in S − . We show, however, that this does not affect the\\nremaining clustering steps:\\nIn each step r > k, we have the following situation for some x ≤ y ≤ s(k). The point bn\\nmight or might not be in the same cluster as bs(r) .\\n. . . ax\\n\\n. . . ay\\n\\n. . . as(r) bs(r) = as(r)+1\\n\\n...\\n\\n. . . bn−1\\n\\nLet the distance δs(r) be realized as the distance from bs(r) to ay . From Observation (a) and\\nline 10 in MST-linkage-core, we know that this distance is minimal among the distances\\nfrom X := {a0 , . . . , as(r) } to all other points in S0 \\\\ X. In particular, the distance from X to\\nbn ∈ S0 \\\\ X is not smaller than δs(r) .\\nThis proves that the addition of bn in step k does not change the single linkage clustering\\nin any later step r > k. This completes the inductive proof of Theorem 5\\n\\n4 Performance\\nIn this section, we compare the performance of the algorithms and give recommendations on\\nwhich algorithm to choose for which clustering method. We compare both the theoretical,\\nasymptotic worst-case performance, and the use-case performance on a range of synthetic\\nrandom data sets.\\n\\n4.1 Asymptotic worst-case performance\\nLet N denote the problem size, which is in this case the number of input data points. The\\ninput size is N2 ∈ Θ(N 2 ).\\nThe asymptotic run-time complexity of MST-linkage-core is obviously Θ(N 2 ), since\\nthere are two nested levels of loops in the algorithm (line 8 and implicitly line 10). The\\nrun-time complexity of the NN-chain-core algorithm is also Θ(N 2 ) (Murtagh, 1985, page\\n86). Postprocessing is the same for both algorithms and is less complex, namely O(N log N )\\nfor sorting and Θ(N ) for Label, so the overall complexity is Θ(N 2 ). This is optimal (in the\\nasymptotic sense): the lower bound is also quadratic since all Θ(N 2 ) input values must be\\nprocessed.\\nThe NN-chain algorithm needs a writable working copy of the input array to store intermediate dissimilarities and otherwise only Θ(N ) additional memory.\\nThe generic algorithm has a best-case time complexity of Θ(N 2 ), but without deeper analysis, the worst-case complexity is O(N 3 ). The bottleneck is line 15 in Generic_linkage:\\nIn O(N ) iterations, this line might be executed up to O(N ) times and does a minimum search\\nover O(N ) elements, which gives a total upper bound of O(N 3 ). This applies for all clustering\\nschemes except single linkage, where the loop starting at line 14 is never executed and thus\\nthe worst-case performance is Θ(N 2 ). The memory requirements for the generic algorithm are\\nsimilar to the NN-chain algorithm: a working copy of the dissimilarity array and additionally\\nonly Θ(N ) temporary memory.\\nIn contrast, the MST algorithm does not write to the input array d. All other temporary\\nvariables are of size O(N ). Hence, MST-linkage requires no working copy of the input\\n\\n20\\n\\n\\x0carray and hence only half as much memory as Generic_linkage and NN-chain-linkage\\nasymptotically.\\nAnderberg’s algorithm (Anderberg, 1973, pages 135–136) has the same asymptotic bounds\\nas our generic algorithm. The performance bottleneck are again the repeated minimum\\nsearches among the updated dissimilarities. Since the generic algorithm defers minimum\\nsearches to a later point in the algorithm (if they need to be performed at all, by then),\\nthere are at least as many minimum searches among at least as many elements in Anderberg’s algorithm as in the generic algorithm. The only point where the generic algorithm\\ncould be slower is the maintenance of the priority queue with nearest neighbor candidates,\\nsince this does not exist in Anderberg’s algorithm. The bottleneck here are potentially up to\\nO(N 2 ) updates of a queue in line 36 of Generic_linkage. In the implementation in the\\nnext section, the queue is realized by a binary heap, so an update takes O(log N ) time. This\\ncould potentially amount to O(N 2 log N ) operations for maintenance of the priority queue.\\nHowever, a reasonable estimate is that the saved minimum searches in most cases save more\\ntime than the maintenance of the queue with O(N ) elements costs, and hence there is a good\\nreason to believe that the generic algorithm is at least as fast as Anderberg’s algorithm.\\nNote that the maintenance effort of the priority queue can be easily reduced to O(N 2 )\\ninstead of O(N 2 log N ) worst case:\\n• A different priority queue structure can be chosen, where the “decrease-key” operation\\ntakes only O(1) time. (Note that the bottleneck operation in line 36 of Generic_linkage never increases the nearest-neighbor distance, only decreases it.) The author did\\nnot test a different structure since a binary heap convinces by its simple implementation.\\n• Changed keys (minimal distances) need not be updated in the priority queue immediately. Instead, the entire queue might be resorted/regenerated at the beginning of\\nevery iteration. This takes N − 1 times O(N ) time with a binary heap. Although this\\nlowers the theoretical complexity for the maintenance of the binary queue, it effectively\\nslowed down the algorithms in practice by a small margin. The reason is, of course, that\\nthe number and complexity of updates of the priority queue did by far not reach their\\ntheoretical upper bound in our test data sets (see below). Altogether, the maintenance\\nof the priority queue, as proposed in Figure 3 seems quite optimal from the practical\\nperspective.\\n\\n4.2 Use-case performance\\nIn addition to the theoretical, asymptotic and worst-case considerations, we also measured\\nthe practical performance of the algorithms. Figure 7 shows the run-time of the algorithms\\nfor a number of synthetic test data sets (for details see below). The solid lines are the average\\nover the data sets. (The graphs labeled “Day-Edelsbrunner” are discussed in Section 5.) The\\nlightly colored bands show the range from minimum to maximum time over all data sets for\\na given number of points.\\nThe following observations can be made:\\n• For single linkage clustering, the MST-algorithm is clearly the fastest one. Together\\nwith the fact that it has only half the memory requirements of the other algorithms (if\\nthe input array is to be preserved), and thus allows the processing of larger data sets,\\nthe MST-algorithm is clearly the best choice for single linkage clustering.\\n• For the clustering schemes without inversions (all except “centroid” and “median”), the\\ngeneric algorithm, the NN-chain algorithm and Anderberg’s algorithm have very similar\\nperformance.\\n\\n21\\n\\n\\x0c103\\n102\\n\\nMethod: “single”\\n\\nCPU time in s\\n\\n101\\n100\\n10−1\\n10−2\\n10−3\\n10−4\\n10−5\\n\\n101\\n\\n102\\n\\n103\\nNumber of points\\n\\n104\\n\\n101\\n\\n102\\n\\n103\\nNumber of points\\n\\n104\\n\\n101\\n\\n102\\n\\n103\\nNumber of points\\n\\n104\\n\\n103\\n102\\n\\nMethod: “average”\\n“complete”, “weighted” and\\n“Ward” look very similar.\\n\\nCPU time in s\\n\\n101\\n100\\n10−1\\n10−2\\n10−3\\n10−4\\n10−5\\n\\n103\\n102\\n\\nMethod: “centroid”\\n“median” looks very similar.\\n\\nCPU time in s\\n\\n101\\n100\\n10−1\\n10−2\\n10−3\\n10−4\\n10−5\\n\\nFigure 7: Performance of several SAHN clustering algorithms. Legend:\\nGeneric algorithm\\n(Figure 3), Anderberg (1973, pages 135–136), NN-chain algorithm (Figure 4),\\nMST-algorithm (Figure 6), Day and Edelsbrunner (1984, Table 5).\\n\\n22\\n\\n\\x0cThe NN-chain algorithm is the only one with guaranteed O(N 2 ) performance here. We\\ncan conclude that the good worst-case performance can be had here without any cutbacks to the use-case performance.\\n• For the “centroid” and “median” methods, we see a very clear disadvantage to Anderberg’s algorithm. Here, the worst case cubic time complexity occurs already in the\\nrandom test data sets. This happens with great regularity, over the full range of input\\nsizes. Our Generic_linkage algorithm, on the other hand, does not suffer from this\\nweakness: Even though the theoretical worst-case bounds are the same, the complexity\\ndoes not raise above the quadratic behavior in our range of test data sets. Hence, we\\nhave grounds to assume that Generic_linkage is much faster in practice.\\n\\n4.3 Conclusions\\nBased on the theoretical considerations and use-case tests, we can therefore recommend algorithms for the various distance update schemes as follows:\\n• “single” linkage clustering: The MST-algorithm is the best, with respect to worst-case\\ncomplexity, use-case performance and memory requirements.\\n• “complete”, “average”, “weighted”, “ward”: The NN-chain algorithm is preferred, since\\nit guarantees O(N 2 ) worst case complexity without any disadvantage to practical performance and memory requirements.\\n• “centroid”, “median”: The generic clustering algorithm is the best choice, since it can\\nhandle inversions in the dendrogram and the performance exhibits quadratic complexity\\nin all observed cases.\\nOf course, the timings in the use-case tests depend on implementation, compiler optimizations, machine architecture and the choice of data sets. Nevertheless, the differences between\\nthe algorithms are very clear here, and the comparison was performed with careful implementations in the identical environment.\\nThe test setup was as follows: All algorithms were implemented in C++ with an interface\\nto Python (van Rossum et al.) and the scientific computing package NumPy (Num) to handle\\nthe input and output of arrays. The test data sets are samples from mixtures of multivariate\\nGaussian distributions with unity covariance\\nmatrix in various dimensions (2, 3, 10, 200) with\\n√\\nvarious numbers of modes (1, 5, [ N ]), ranging from N = 10 upwards until memory was\\nexhausted (N = 20000 except for single linkage). The centers of the Gaussian distributions\\nare also distributed by a Gaussian distribution. Moreover, for the methods for which it makes\\nsense (single, complete, average, weighted: the “combinatorial” methods), we also generated\\n10 test sets per number of input points with a uniform distribution of dissimilarities.\\nThe timings were obtained on a PC with an Intel dual-core CPU T7500 with 2.2 GHz clock\\nspeed and 4GB of RAM and no swap space. The operating system was Ubuntu 11.04 64-bit,\\nPython version: 2.7.1, NumPy version: 1.5.1, compiler: GNU C++ compiler, version 4.5.2.\\nOnly one core of the two available CPU cores was used in all computations.\\n\\n5 Alternative algorithms\\nThe MST algorithm has the key features that it (1) needs no working copy of the Θ(N 2 ) input\\narray and only Θ(N ) working memory, (2) is fast since it reads every input dissimilarity only\\nonce and otherwise deals only with Θ(N ) memory. There is a second algorithm with these\\n\\n23\\n\\n\\x0ccharacteristics, Sibson’s SLINK algorithm (Sibson, 1973). It is based on the insight that a\\nsingle linkage dendrogram for N + 1 points can be computed from the dendrogram of the first\\nN points plus a single row of distances (d[N, 0], . . . , d[N, N − 1]). In this fashion, the SLINK\\nalgorithm even reads the input dissimilarities in a fixed order, which can be an advantage\\nover the MST algorithm if the favorable input order can be realized in an application, or if\\ndissimilarities do not fit into random-access memory and are read from disk.\\nHowever, there is one important difference: even though the output data format looks\\ndeceptively similar to the MST algorithm (the output can be converted to a stepwise dendrogram by exactly the same process: sorting with respect to dissimilarities and a union-find\\nprocedure to generate node labels from cluster representatives), the SLINK algorithm cannot handle ties. This is definite, since e.g. the output in the example situation on page 5 is\\nthe same in all three cases, and hence no postprocessing can recover the different stepwise\\ndendrograms.\\nThere is an easy way out by specifying a secondary order\\nd(i, j) ≺ d(k, l)\\n\\n:⇐⇒\\n\\nd(i, j) < d(k, l)\\nNi + j < Nk + l\\n\\nif this holds,\\nif d(i, j) = d(k, l)\\n\\nto make all dissimilarities artificially distinct. In terms of performance, the extra comparisons\\nput a slight disadvantage on the SLINK algorithm, according to the author’s experiments.\\nHowever, the difference is not much, and the effect on timings may be compensated or even\\nreversed in a different software environment or when the input order of dissimilarities is in\\nfavor of SLINK. Hence, the SLINK algorithm is a perfectly fine tool, as long as care is taken\\nto make all dissimilarities unique.\\nThe same idea of generating a dendrogram inductively is the basis of an algorithm by\\nDefays (1977). This paper is mostly cited as a fast algorithm for complete linkage clustering.\\nHowever, it definitely is not an algorithm for complete linkage clustering, as the complete\\nlinkage method is commonly defined, in this paper and identically elsewhere.\\nAn algorithm which is interesting from the theoretical point of view is given by Day and\\nEdelsbrunner (1984, Table 5). It uses N priority queues for the nearest neighbor of each\\npoint. By doing so, the authors achieve a worst-case time complexity of O(N 2 log N ), which\\nis better than the existing bound O(N 3 ) for the schemes where the NN-chain algorithm\\ncannot be applied. The overhead for maintaining a priority queue for each point, however,\\nslows the algorithm down in practice. The performance measurements in Figure 7 include\\nthe Day-Edelsbrunner algorithm. Day and Edelsbrunner write their algorithm in general\\nterms, for any choice of priority queue structure. We implemented the algorithm for the\\nmeasurements in this paper with binary heaps, since these have a fixed structure and thus\\nrequire the least additional memory. But even so, the priority queues need additional memory\\nof order Θ(N 2 ) for their bookkeeping, which can also be seen in the graphs since they stop\\nat fewer points, within the given memory size of the test. The graphs show that even if the\\nDay-Edelsbrunner algorithm gives the currently best asymptotic worst-case bound for the\\n“centroid” and “median” methods, it is inefficient for practical purposes.\\nKřivánek (1990, § II) suggested to put all N2 dissimilarity values into an (a, b)-tree data\\nstructure. He claims that this enables hierarchical clustering to be implemented in O(N 2 )\\ntime. Křivánek’s conceptually very simple algorithm relies on the fact that m insertions into\\nan (a, b)-tree can be done in O(m) amortized time. This is only true when the positions,\\nwhere the elements should be inserted into the tree, are known. Searching for these positions\\ntakes O(log N ) time per element, however. (See Mehlhorn and Tsakalidis (1990, § 2.1.2) for\\nan accessible discussion of amortized complexity for (2, 4)-trees; Huddleston and Mehlhorn\\n(1982) introduce and discuss (a, b)-trees in general.) Křivánek did not give any details of his\\n\\n24\\n\\n\\x0canalysis, but based on his short remarks, the author cannot see how Křivánek’s algorithm\\nachieves O(N 2 ) worst-case performance for SAHN clustering.\\n\\n6 Extension to vector data\\nIf the input to a SAHN clustering algorithm is not the array of pairwise dissimilarities but\\nN points in a D-dimensional real vector space, the lower bound Ω(N 2 ) on time complexity\\ndoes not hold any more. Since much of the time in an SAHN clustering scheme is spent on\\nnearest-neighbor searches, algorithms and data structures for fast nearest-neighbor searches\\ncan potentially be useful. The situation is not trivial, however, since (1) in the “combinatorial”\\nmethods (e.g. single, complete, average, weighted linkage) the inter-cluster distances are not\\nsimply defined as distances between special points like cluster centers, and (2) even in the\\n“geometric” methods (the Ward, centroid and median schemes), points are removed and\\nnew centers added with the same frequency as pairs of closest points are searched, so a\\ndynamic nearest-neighbor algorithm is needed, which handles the removal and insertion of\\npoints efficiently.\\nMoreover, all known fast nearest-neighbor algorithms lose their advantage over exhaustive\\nsearch with increasing dimensionality. Additionally, algorithms will likely work for one metric\\nin RD but not universally. Since this paper is concerned with the general situation, we do\\nnot go further into the analysis of the “stored data approach” (Anderberg, 1973, § 6.3). We\\nonly list at this point what can be achieved with the algorithms from this paper. This will\\nlikely be the best solution for high-dimensional data or general-purpose algorithms, but there\\nare better solutions for low-dimensional data outside the scope of this paper. The suggestions\\nbelow are at least helpful to process large data sets since memory requirements are of class\\nΘ(N D), but they do not overcome their Ω(N 2 ) lower bound on time complexity.\\n• The MST algorithm for single linkage can compute distances on-the-fly. Since every\\npairwise dissimilarity is read in only once, there is no performance penalty compared to\\nfirst computing the whole dissimilarity matrix and then applying the MST algorithm.\\nQuite the contrary, computing pairwise distances in-process can result in faster execution since much less memory must be reserved and accessed. The MST algorithm is\\nsuitable for any dissimilarity measure which can be computed from vector representations (that is, all scale types are possible, e.g. R-valued measurements, binary sequences\\nand categorical data).\\n• The NN-chain algorithm is suitable for the “Ward” scheme, since inter-cluster distances\\ncan be defined by means of centroids as in Figure 2. The initial inter-point dissimilarities\\nmust be Euclidean distances (which is anyway the only setting in which Ward linkage\\ndescribes a meaningful procedure).\\n• The generic algorithm is suitable for the “Ward”, “centroid” and “median” scheme on\\nEuclidean data. There is a simpler variant of the Generic_linkage algorithm in Section 6, which works even faster in this setting. The principle of the algorithm Generic_\\nlinkage_variant is the same: each array entry mindist[x] maintains a lower bound on\\nall dissimilarities d[x, y] for nodes with label y > x. The Generic_linkage algorithm\\nis designed to work efficiently with a large array of pairwise dissimilarities. For this purpose, the join of two nodes a and b re-uses the label b, which facilitates in-place updating\\nof the dissimilarity array in an implementation. The Generic_linkage_variant algorithm, in contrast, generates a unique new label for each new node, which is smaller\\nthan all existing labels. Since the new label is at the beginning of the (ordered) list of\\n\\n25\\n\\n\\x0cFigure 8 The generic clustering algorithm (variant).\\n1:\\n\\n5:\\n\\nprocedure Generic_linkage_variant(N, d)\\nd is either an array or a function which computes dissimilarities from cluster centers.\\n..\\n.\\n(Lines 2 to 13 are the same as in Generic_linkage.)\\nfor x in S \\\\ {N − 1} do\\n..\\n.\\n\\n14:\\n\\nwhile b ∈\\n/ S do\\n..\\n.\\n\\n21:\\n22:\\n23:\\n24:\\n25:\\n26:\\n\\nend while\\nRemove a and b from Q.\\nAppend (a, b, δ) to L.\\nCreate a new label n ← −i\\nsize[n] ← size[a] + size[b]\\nS ← (S \\\\ {a, b}) ∪ {n}\\n\\n27:\\n28:\\n29:\\n\\nfor x in S \\\\ {n} do\\nExtend the distance information.\\nd[x, n] ← d[n, x] ← Formula(d[a, x], d[b, x], d[a, b], size[a], size[b], size[x])\\nend for\\n\\nRecalculation of nearest neighbors, if necessary.\\n\\nor\\n27:\\n30:\\n31:\\n32:\\n33:\\n34:\\n\\nCompute the cluster center for n as in Figure 2.\\nn_nghbr[n] ← argminx>n d[n, x]\\nInsert (n, d[n, n_nghbr[n]]) into mindist and Q\\nend for\\nreturn L\\nend procedure\\n\\nnodes and not somewhere in the middle, the bookkeeping of nearest neighbor candidates\\nand minimal distances is simpler in Generic_linkage_variant: in particular, the\\ntwo loops in lines 28–38 of Generic_linkage can be disposed of entirely. Moreover,\\nexperiments show that Generic_linkage_variant needs much less recalculations of\\nnearest neighbors in some data sets. However, both algorithms are similar, and which\\none is faster in an implementation seems to depend strongly on the actual data structures\\nand their memory layout.\\n\\nAnother issue which is not in the focus of this paper is that of parallel algorithms. For the\\n“stored matrix approach”, this has a good reason since the balance of memory requirements\\nversus computational complexity does not make it seem worthwhile to attempt parallelization\\nwith current hardware. This changes for vector data, when the available memory is not the\\nlimiting factor and the run-time is pushed up by bigger data sets. In high-dimensional vector\\nspaces, the advanced clustering algorithms in this paper require little time compared to the\\ncomputation of inter-cluster distances. Hence, parallelizing the nearest-neighbor searches with\\ntheir inherent distance computations appears a fruitful and easy way of sharing the workload.\\nThe situation becomes less clear for low-dimensional data, however.\\n\\n26\\n\\n\\x0c7 Conclusion\\nAmong the algorithms for sequential, agglomerative, hierarchic, nonoverlapping (SAHN)\\nclustering on data with a dissimilarity index, three current algorithms are most efficient:\\nRohlf’s algorithm MST-linkage for single linkage clustering, Murtagh’s algorithm NNchain-linkage for the “complete”, “average”, “weighted” and “Ward” schemes, and the\\nauthor’s Generic_linkage algorithm for the “centroid” and “median” schemes and the\\n“flexible” family. The last algorithm can also be used for an arbitrary distance update formula. There is even a simpler variant Generic_linkage_variant, which seems to require\\nless internal calculations, while the original algorithm is optimized for in-place updating of\\na dissimilarity array as input. The Generic_linkage algorithm and its variant are new;\\nthe other two algorithms were described before, but for the first time they are proved to be\\ncorrect.\\n\\nAcknowledgments\\nThis work was funded by the National Science Foundation grant DMS-0905823 and the Air\\nForce Office of Scientific Research grant FA9550-09-1-0643.\\n\\nReferences\\nNumPy: Scientific computing tools for Python. Available at http://numpy.scipy.org/.\\nMichael R. Anderberg. Cluster analysis for applications. Academic Press, New York, 1973.\\nISBN 0120576503.\\nThomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest, and Clifford Stein. Introduction\\nto Algorithms. MIT Press, 3rd edition, 2009.\\nWilliam H. E. Day. Complexity theory: an introduction for practitioners of classification. In\\nClustering and classification, pages 199–233. World Scientific Publishing, River Edge, NJ,\\n1996.\\nWilliam H. E. Day and Herbert Edelsbrunner.\\nEfficient algorithms for agglomerative hierarchical clustering methods. Journal of Classification, 1(1):7–24, 1984. doi:\\n10.1007/BF01890115.\\nDaniel Defays. An efficient algorithm for a complete link method. The Computer Journal, 20\\n(4):364–366, 1977. doi: 10.1093/comjnl/20.4.364.\\nDamian Eads. Hierarchical clustering (scipy.cluster.hierarchy), 2007. Package for SciPy\\nversion 0.9.0. Available at http://www.scipy.org.\\nBrian S. Everitt, Sabine Landau, Morven Leese, and Daniel Stahl. Cluster Analysis. John\\nWiley & Sons, 5th edition, 2011. doi: 10.1002/9780470977811.\\nAllan D. Gordon. A review of hierarchical classification. Journal of the Royal Statistical\\nSociety. Series A (General), 150(2):119–137, 1987. doi: 10.2307/2981629.\\nJohn C. Gower and G. J. S. Ross. Minimum spanning trees and single linkage cluster analysis.\\nJournal of the Royal Statistical Society. Series C (Applied Statistics), 18(1):54–64, 1969. doi:\\n10.2307/2346439.\\n\\n27\\n\\n\\x0cPierre Hansen and Brigitte Jaumard. Cluster analysis and mathematical programming. Mathematical Programming, 79(1–3):191–215, 1997. doi: 10.1007/BF02614317.\\nScott Huddleston and Kurt Mehlhorn. A new data structure for representing sorted lists.\\nActa Informatica, 17(2):157–184, 1982. doi: 10.1007/BF00288968.\\nAnil K. Jain and Richard C. Dubes. Algorithms for Clustering Data. Prentice Hall, Englewood\\nCliffs, NJ, 1988.\\nStephen C. Johnson. Hierarchical clustering schemes. Psychometrika, 32(3):241–254, 1967.\\ndoi: 10.1007/BF02289588.\\nEric Jones, Travis Oliphant, Pearu Peterson, et al. SciPy: Open source scientific tools for\\nPython, 2001. http://www.scipy.org.\\nLeonard Kaufman and Peter J. Rousseeuw. Finding groups in data: An introduction to cluster\\nanalysis. John Wiley & Sons, New York, 1990. doi: 10.1002/9780470316801.\\nMirko Křivánek. Connected admissible hierarchical clustering. KAM series, (90-189), 1990.\\nDepartment of Applied Mathematics, Charles University, Prague (CZ). Available at http:\\n//kam.mff.cuni.cz/~kamserie/serie/clanky/1990/s189.pdf.\\nG. N. Lance and W. T. Williams. A general theory of classificatory sorting strategies. Computer Journal, 9(4):373–380, 1967. doi: 10.1093/comjnl/9.4.373.\\nKurt Mehlhorn and Athanasios Tsakalidis. Data structures. In Handbook of theoretical computer science, Vol. A, pages 301–341. Elsevier, Amsterdam, 1990. Available at\\nhttp://www.mpi-sb.mpg.de/~mehlhorn/ftp/DataStructures.pdf.\\nFionn Murtagh. A survey of recent advances in hierarchical clustering algorithms. Computer\\nJournal, 26(4):354–359, 1983. doi: 10.1093/comjnl/26.4.354.\\nFionn Murtagh. Complexities of hierarchic clustering algorithms: State of the art. Computational Statistics Quarterly, 1(2):101–113, 1984. Available at http://thames.cs.rhul.ac.\\nuk/~fionn/old-articles/complexities/.\\nFionn Murtagh. Multidimensional clustering algorithms, volume 4 of Compstat Lectures.\\nPhysica-Verlag, Würzburg/ Wien, 1985. ISBN 3-7051-0008-4. Available at http://www.\\nclassification-society.org/csna/mda-sw/.\\nDaniel Müllner. fastcluster: Fast hierarchical, agglomerative clustering routines for R and\\nPython. Preprint, 2011. Will be available at http://math.stanford.edu/~muellner.\\nR Development Core Team. R: A Language and Environment for Statistical Computing. R\\nFoundation for Statistical Computing, Vienna, Austria, 2011. http://www.R-project.org.\\nF. James Rohlf. Hierarchical clustering using the minimum spanning tree. Comput. Journal,\\n16:93–95, 1973. Available at http://life.bio.sunysb.edu/ee/rohlf/reprints.html.\\nF. James Rohlf. Single-link clustering algorithms. In P.R. Krishnaiah and L.N. Kanal, editors,\\nClassification Pattern Recognition and Reduction of Dimensionality, volume 2 of Handbook\\nof Statistics, pages 267–284. Elsevier, 1982. doi: 10.1016/S0169-7161(82)02015-X.\\nR. Sibson. SLINK: an optimally efficient algorithm for the single-link cluster method. Comput.\\nJournal, 16:30–34, 1973. doi: 10.1093/comjnl/16.1.30.\\n\\n28\\n\\n\\x0cPeter H. A. Sneath and Robert R. Sokal. Numerical taxonomy. W. H. Freeman, San Francisco,\\n1973.\\nThe MathWorks, Inc. MATLAB, 2011. http://www.mathworks.com.\\nGuido van Rossum et al. Python programming language. Available at http://www.python.\\norg.\\nWolfram Research, Inc. Mathematica, 2010. http://www.wolfram.com.\\n\\nDaniel Müllner\\nStanford University\\nDepartment of Mathematics\\n450 Serra Mall, Building 380\\nStanford, CA 94305\\nE-mail: muellner@math.stanford.edu\\nhttp://math.stanford.edu/~muellner\\n\\n29\\n\\n\\x0c',\n",
       " 'title': 'Modern hierarchical, agglomerative clustering algorithms'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def parse_html(filename):\n",
    "    \"\"\"Extract the Author, Title and Text from a HTML file\n",
    "    which was produced by pdftotext with the option -htmlmeta.\"\"\"\n",
    "    with open(filename) as infile:\n",
    "        html = BeautifulSoup(infile, \"html.parser\", from_encoding='utf-8')\n",
    "        d = {'text': html.pre.text}\n",
    "        if html.title is not None:\n",
    "            d['title'] = html.title.text\n",
    "        for meta in html.findAll('meta'):\n",
    "            try:\n",
    "                if meta['name'] in ('Author', 'Title'):\n",
    "                    d[meta['name'].lower()] = meta['content']\n",
    "            except KeyError:\n",
    "                continue\n",
    "        return d\n",
    "    \n",
    "parse_html('data/muellner2011.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quiz!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a)** The `parse_html` function returns a dictionary consisting of the contents of some of the fields in our index schema. We will need this dictionary when we add new documents to our index. Reimplement the `pdftotext` function. It should convert a PDF file into a HTML file, stored in the directory `pydf/data`. Plug the function `parse_html` into the `pdftotext` function. Write the contents of the `text` field to a `.txt` file in the `pydf/data` directory. Make sure that this text file has the same filename as the PDF file except for the extension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def pdftotext(pdf):\n",
    "    \"\"\"Convert a pdf to a text file. Extract the Author and Title \n",
    "    and return a dictionary consisting of the author, title and \n",
    "    text.\"\"\"\n",
    "    basename, _ = os.path.splitext(os.path.basename(pdf))\n",
    "    # insert your code here\n",
    "    subprocess.call(['pdftotext', '-enc', 'UTF-8', '-htmlmeta',\n",
    "                     pdf, os.path.join('data', basename + '.html')])\n",
    "    data = parse_html(os.path.join('data', basename + '.html'))\n",
    "    with open(os.path.join('data', basename + '.txt'), 'w') as outfile:\n",
    "        outfile.write(data['text'])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b**) For reasons that will become clear, it would be convenient to add the other fields of our index schema to the dictionary return by `parse_html` as well. Rewrite the function `pdftotext` and add the values for the `source`, `target` and `id` to the dictionary returned by `parse_html`. The values for the `source`, `target` and `id` should match the values we used in the examples above, where we manually added some documents to the index. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'author': 'Daniel Müllner',\n",
       " 'id': 'muellner2011',\n",
       " 'source': 'static/pdfs/muellner2011.pdf',\n",
       " 'target': 'data/muellner2011.txt',\n",
       " 'text': '\\nModern hierarchical, agglomerative\\nclustering algorithms\\n\\narXiv:1109.2378v1 [stat.ML] 12 Sep 2011\\n\\nDaniel Müllner\\n\\nThis paper presents algorithms for hierarchical, agglomerative clustering which\\nperform most efficiently in the general-purpose setup that is given in modern\\nstandard software. Requirements are: (1) the input data is given by pairwise dissimilarities between data points, but extensions to vector data are also discussed\\n(2) the output is a “stepwise dendrogram”, a data structure which is shared by\\nall implementations in current standard software. We present algorithms (old and\\nnew) which perform clustering in this setting efficiently, both in an asymptotic\\nworst-case analysis and from a practical point of view. The main contributions of\\nthis paper are: (1) We present a new algorithm which is suitable for any distance\\nupdate scheme and performs significantly better than the existing algorithms. (2)\\nWe prove the correctness of two algorithms by Rohlf and Murtagh, which is necessary in each case for different reasons. (3) We give well-founded recommendations\\nfor the best current algorithms for the various agglomerative clustering schemes.\\nKeywords: clustering, hierarchical, agglomerative, partition, linkage\\n\\n1 Introduction\\nHierarchical, agglomerative clustering is an important and well-established technique in unsupervised machine learning. Agglomerative clustering schemes start from the partition of\\nthe data set into singleton nodes and merge step by step the current pair of mutually closest\\nnodes into a new node until there is one final node left, which comprises the entire data set.\\nVarious clustering schemes share this procedure as a common definition, but differ in the way\\nin which the measure of inter-cluster dissimilarity is updated after each step. The seven most\\ncommon methods are termed single, complete, average (UPGMA), weighted (WPGMA, McQuitty), Ward, centroid (UPGMC) and median (WPGMC) linkage (see Everitt et al., 2011,\\nTable 4.1). They are implemented in standard numerical and statistical software such as R\\n(R Development Core Team, 2011), MATLAB (The MathWorks, Inc., 2011), Mathematica\\n(Wolfram Research, Inc., 2010), SciPy (Jones et al., 2001).\\nThe stepwise, procedural definition of these clustering methods directly gives a valid but\\ninefficient clustering algorithm. Starting with Gower’s and Ross’s observation (Gower and\\nRoss, 1969) that single linkage clustering is related to the minimum spanning tree of a graph\\nin 1969, several authors have contributed algorithms to reduce the computational complexity\\nof agglomerative clustering, in particular Sibson (1973), Rohlf (1973), Anderberg (1973, page\\n135), Murtagh (1984), Day and Edelsbrunner (1984, Table 5).\\n\\n1\\n\\n\\x0cEven when software packages do not use the inefficient primitive algorithm (as SciPy (Eads,\\n2007) and the R (R Development Core Team, 2011) methods hclust and agnes do), the\\nauthor found that implementations largely use suboptimal algorithms rather than improved\\nmethods suggested in theoretical work. This paper is to advance the theory further up to a\\npoint where the algorithms can be readily used in the standard setting, and in this way bridge\\nthe gap between the theoretical advances that have been made and the existing software\\nimplementations, which are widely used in science and industry.\\nThe main contributions of this paper are:\\n• We present a new algorithm which is suitable for any distance update scheme and performs significantly better than the existing algorithms for the “centroid” and “median”\\nclustering schemes.\\n• We prove the correctness of two algorithms, a single linkage algorithm by Rohlf (1973)\\nand Murtagh’s nearest-neighbor-chain algorithm (Murtagh, 1985, page 86). These\\nproofs were still missing, and we detail why the two proofs are necessary, each for\\ndifferent reasons.\\n• These three algorithms (together with an alternative by Sibson, 1973) are the best\\ncurrently available ones, each for its own subset of agglomerative clustering schemes.\\nWe justify this carefully, discussing potential alternatives.\\nThe specific class of clustering algorithms which is dealt with in this paper has been characterized by the acronym SAHN (sequential, agglomerative, hierarchic, nonoverlapping methods) by Sneath and Sokal (1973, § 5.4, 5.5). The procedural definition (which is given in\\nFigure 1 below) is not the only possibility for a SAHN method, but this method together with\\nthe seven common distance update schemes listed above is most widely used. The scope of\\nthis paper is contained further by practical considerations: We consider methods here which\\ncomply to the input and output requirements of the general-purpose clustering functions in\\nmodern standard software:\\n• The input to the algorithm is the list of N2 pairwise dissimilarities between N points.\\n(We mention extensions to vector data in Section 6.)\\n• The output is a so called stepwise dendrogram (see Section 2.2), in contrast to laxly\\nspecified output structure or weaker notions of (non-stepwise) dendrograms in earlier\\nliterature.\\nThe first item has always been a distinctive characteristic to previous authors since the input\\nformat broadly divides into the stored matrix approach (Anderberg, 1973, § 6.2) and the stored\\ndata approach (Anderberg, 1973, § 6.3). In contrast, the second condition has not been given\\nattention yet, but we will see that it affects the validity of algorithms.\\nWe do not aim to present and compare all available clustering algorithms but build upon\\nthe existing knowledge and present only the algorithms which we found the best for the given\\npurpose. For reviews and overviews we refer to Rohlf (1982), Murtagh (1983, 1985), Gordon\\n(1987, §3.1), Jain and Dubes (1988, § 3.2), Day (1996, § 4.2), Hansen and Jaumard (1997).\\nThose facts about alternative algorithms which are necessary to complete the discussion and\\nwhich are not covered in existing reviews are collected in Section 5.\\nThe paper is structured as follows:\\nSection 2 contains the definitions for input and output data structures as well as specifications of the distance update formulas and the “primitive” clustering algorithm.\\n\\n2\\n\\n\\x0cSection 3 is the main section of this paper. We present and discuss three algorithms: our\\nown “generic algorithm”, Murtagh’s nearest-neighbor-chain algorithm and Rohlf’s algorithm\\nbased on the minimum spanning tree of a graph. We prove the correctness of these algorithms.\\nSection 4 discusses the complexity of the algorithms, both as theoretical, asymptotic complexity in Section 4.1 and by use-case performance experiments in Section 4.2. We conclude\\nthis section by recommendations on which algorithm is the best one for each distance update\\nscheme, based on the preceding analysis.\\nSection 5 discusses alternative algorithms, and Section 6 gives a short outlook on extending\\nthe context of this paper to vector data instead of dissimilarity input. The paper ends with\\na brief conclusion in Section 7.\\nThe algorithms in this paper have been implemented in C++ by the author and are available\\nwith interfaces to the statistical software R and the programming language Python (van\\nRossum et al.). This implementation is presented elsewhere (Müllner, 2011).\\n\\n2 Data structures and the algorithmic definition of\\nSAHN clustering methods\\nIn this section, we recall the common algorithmic (procedural) definition of the SAHN clustering methods which demarcate the scope of this paper. Before we do so, we concretize the\\nsetting further by specifying the input and output data structures for the clustering methods.\\nEspecially the output data structure has not been specifically considered in earlier works, but\\nnowadays there is a de facto standard given by the shared conventions in the most widely\\nused software. Hence, we adopt the setting from practice and specialize our theoretical consideration to the modern standard of the stepwise dendrogram. Later, Section 5 contains an\\nexample of how the choice of the output data structure affects the result which algorithms\\nare suitable and/or most efficient.\\n\\n2.1 Input data structure\\nThe input to the hierarchical clustering algorithms in this paper is always a finite set together\\nwith a dissimilarity index (see Hansen and Jaumard, 1997, § 2.1).\\nDefinition. A dissimilarity index on a set S is a map d : S × S → [0, ∞) which is reflexive\\nand symmetric, i.e. we have d(x, x) = 0 and d(x, y) = d(y, x) for all x, y ∈ S.\\nA metric on S is certainly a dissimilarity index. In the scope of this paper, we call the\\nvalues of d distances in a synonymous manner to dissimilarities, even though they are not\\nrequired to fulfill the triangle inequalities, and dissimilarities between different elements may\\nbe zero.\\nIf the set S has N elements, a dissimilarity index is given by the N2 pairwise dissimilarities.\\nHence, the input size to the clustering algorithms is Θ(N 2 ). Once the primitive clustering\\nalgorithm is specified in Section 2.4, it is easy to see that the hierarchical clustering schemes\\nare sensitive to each input value. More precisely, for every input size N and for every index\\npair i = j, there are two dissimilarities which differ only at position (i, j) and which produce\\ndifferent output. Hence, all input values must be processed by a clustering algorithm, and\\ntherefore the run-time is bounded below by Ω(N 2 ).\\nThis bound applies to the general setting when the input is a dissimilarity index. In a\\ndifferent setting, the input could also be given as N points in a normed vector space of\\ndimension D (the “stored data approach”, Anderberg, 1973, §6.3). This results in an input\\nsize of Θ(N D), so that the lower bound does not apply for clustering of vector data. See\\n\\n3\\n\\n\\x0cSection 6 for a discussion to which extent the algorithms in this paper can be used in the\\n“stored data approach”.\\n\\n2.2 Output data structures\\nThe output of a hierarchical clustering procedure is traditionally a dendrogram. The term\\n“dendrogram” has been used with three different meanings: a mathematical object, a data\\nstructure and a graphical representation of the former two. In the course of this section, we\\ndefine a data structure and call it stepwise dendrogram. A graphical representation may be\\ndrawn from the data in one of several existing fashions. The graphical representation might\\nlose information (e.g. when two merges happen at the same dissimilarity value), and at the\\nsame time contain extra information which is not contained in the data itself (like a linear\\norder of the leaves).\\nIn the older literature, e.g. Sibson (1973), a dendrogram (this time, as a mathematical\\nobject) is rigorously defined as a piecewise constant, right-continuous map D : [0, ∞) → P(S),\\nwhere P(S) denotes the partitions of S, such that\\n• D(s) is always coarser than or equal to D(t) for s > t,\\n• D(s) eventually becomes the one-set partition {S} for large s.\\nA dendrogram in this sense with the additional condition that D(0) is the singleton partition\\nis in one-to-one correspondence to an ultrametric on S (Johnson, 1967, § I). The ultrametric\\ndistance between x and y is given by µ(x, y) = min{s ≥ 0 | x ∼ y in D(s)}. Conversely,\\nthe partition at level s ≥ 0 in the dendrogram is given by the equivalence relation x ∼ y ⇔\\nµ(x, y) ≤ s. Sibson’s “pointer representation” and “packed representation” (Sibson, 1973, § 4)\\nare examples of data structures which allow the compact representation of a dendrogram or\\nultrametric.\\nIn the current software, however, the output of a hierarchical clustering procedure is a\\ndifferent data structure which conveys potentially more information. We call this a stepwise\\ndendrogram.\\nDefinition. Given a finite set S0 with cardinality N = |S0 |, a stepwise dendrogram is a list\\nof N − 1 triples (ai , bi , δi ) (i = 0, . . . , N − 2) such that δi ∈ [0, ∞) and ai , bi ∈ Si , where Si+1\\nis recursively defined as (Si \\\\ {ai , bi }) ∪ ni and ni ∈\\n/ S \\\\ {ai , bi } is a label for a new node.\\nThis has the following interpretation: The set S0 are the initial data points. In each step,\\nni is the new node which is formed by joining the nodes ai and bi at the distance δi . The\\norder of the nodes within each pair (ai , bi ) does not matter. The procedure contains N − 1\\nsteps, so that the final state is a single node which contains all N initial nodes.\\n(The mathematical object behind this data structure is a sequence of N + 1 distinct, nested\\npartitions from the singleton partition to the one-set partition, together with a nonnegative\\nreal number for each partition. We do not need this abstract point of view here, though.)\\nThe identity of the new labels ni is not part of the data structure; instead it is assumed that\\nthey are generated according to some rule which is part of the specific data format convention.\\nIn view of this, it is customary to label the initial data points and the new nodes by integers.\\nFor example, the following schemes are used in software:\\n• R convention: S0 := (−1, . . . , −N ), new nodes: (1, . . . , N − 1)\\n• SciPy convention: S0 := (0, . . . , N − 1), new nodes: (N, . . . , 2N − 2)\\n• MATLAB convention: S0 := (1, . . . , N ), new nodes: (N + 1, . . . , 2N − 1)\\n\\n4\\n\\n\\x0cWe regard a stepwise dendrogram both as an ((N − 1) × 3)-matrix or as a list of triples,\\nwhichever is more convenient in a given situation.\\nIf the sequence (δi ) in Section 2.2 is non-decreasing, one says that the stepwise dendrogram\\ndoes not contain inversions, otherwise it does.\\nIn contrast to the first notion of a dendrogram above, a stepwise dendrogram can take\\ninversions into account, which an ordinary dendrogram cannot. Moreover, if more than two\\nnodes are joined at the same distance, the order of the merging steps does matter in a stepwise\\ndendrogram. Consider e.g. the following data sets with three points:\\n(A)\\n2.0\\n\\nx1\\n\\n•\\n\\n(B)\\n\\nx0\\n•\\n\\n2.0\\n\\n2.0\\n\\n•\\nx2\\n\\n3.0\\n\\nx2\\n\\n•\\n\\nx1\\n•\\n\\n(C)\\n2.0\\n\\n2.0\\n\\n•\\nx0\\n\\n3.0\\n\\nx0\\n\\n•\\n\\nx2\\n•\\n\\n2.0\\n\\n•\\nx1\\n\\n3.0\\n\\nThe array\\n0, 1, 2.0\\n2, 3, 2.0\\nis a valid output (SciPy conventions) for single linkage clustering on the data sets (A) and\\n(B) but not for (C). Even more, there is no stepwise dendrogram which is valid for all three\\ndata sets simultaneously. On the other hand, the non-stepwise single linkage dendrogram is\\nthe same in all cases:\\nD(s) =\\n\\n{x0 }, {x1 }, {x2 }\\n{x0 , x1 , x2 }\\n\\nif s < 2\\nif s ≥ 2\\n\\n2.0\\n\\npictorially:\\n\\nx0\\n\\nx1\\n\\nx2\\n\\nHence, a stepwise dendrogram conveys slightly more information than a non-stepwise dendrogram in the case of ties (i.e. when more than one merging step occurs at a certain distance).\\nThis must be taken into account when we check the correctness of the algorithms. Although\\nthis complicates the proofs in Sections 3.3 and 3.2 and takes away from the simplicity of the\\nunderlying ideas, it is not a matter of hairsplitting: E.g. Sibson’s SLINK algorithm (Sibson,\\n1973) for single linkage clustering works flawlessly if all distances are distinct but produces\\nthe same output on all data sets (A), (B) and (C). Hence, the output cannot be converted\\ninto a stepwise dendrogram. See Section 5 for further details.\\n\\n2.3 Node labels\\nThe node labels ni in a stepwise dendrogram may be chosen as unique integers according to\\none of the schemes described in the last section. In an implementation, when the dissimilarities\\nare stored in a large array in memory, it is preferable if each node label ni for the joined cluster\\nreuses one of the indices ai , bi of its constituents, so that the dissimilarities can be updated\\nin-place. Since the clusters after each row in the dendrogram form a partition of the initial set\\nS0 , we can identify each cluster not only by its label but also by one of its members. Hence, if\\nthe new node label ni is chosen among ai , bi , this is sufficient to reconstruct the partition at\\nevery stage of the clustering process, and labels can be converted to any other convention in\\na postprocessing step. Generating unique labels from cluster representatives takes only Θ(N )\\ntime and memory with a suitable union-find data structure. See Section 3.2 and Figure 5 for\\ndetails.\\n\\n5\\n\\n\\x0cFigure 1 Algorithmic definition of a hierarchical clustering scheme.\\nprocedure Primitive_clustering(S, d)\\nN ← |S|\\nL ← []\\nsize[x] ← 1 for all x ∈ S\\nfor i ← 0, . . . , N − 2 do\\n(a, b) ← argmin(S×S)\\\\∆ d\\nAppend (a, b, d[a, b]) to L.\\n8:\\nS ← S \\\\ {a, b}\\n9:\\nCreate a new node label n ∈\\n/ S.\\n10:\\nUpdate d with the information\\n1:\\n\\n2:\\n3:\\n4:\\n5:\\n6:\\n7:\\n\\nS: node labels, d: pairwise dissimilarities\\nNumber of input nodes\\nOutput list\\n\\nd[n, x] = d[x, n] = Formula(d[a, x], d[b, x], d[a, b], size[a], size[b], size[x])\\nfor all x ∈ S.\\nsize[n] ← size[a] + size[b]\\nS ← S ∪ {n}\\nend for\\nreturn L\\nthe stepwise dendrogram, an ((N − 1) × 3)-matrix\\n15: end procedure\\n(As usual, ∆ denotes the diagonal in the Cartesian product S × S.)\\n11:\\n12:\\n13:\\n14:\\n\\n2.4 The primitive clustering algorithm\\nThe solution that we expect from a hierarchical clustering algorithm is defined procedurally.\\nAll algorithms in this paper are measured against the primitive algorithm in Figure 1. We state\\nit in a detailed form to point out exactly which information about the clusters is maintained:\\nthe pairwise dissimilarities and the number of elements in each cluster.\\nThe function Formula in line 10 is the distance update formula, which returns the distance\\nfrom a node x to the newly formed node a ∪ b in terms of the dissimilarities between clusters\\na, b and x and their sizes. The table in Figure 2 lists the formulas for the common distance\\nupdate methods.\\nFor five of the seven formulas, the distance between clusters does not depend on the order\\nwhich the clusters were formed by merging. In this case, we also state closed, non-iterative\\nformulas for the cluster dissimilarities in the third row in Figure 2. The distances in the\\n“weighted” and the “median” update scheme depend on the order, so we cannot give noniterative formulas.\\nThe “centroid” and “median” formulas can produce inversions in the stepwise dendrograms;\\nthe other five methods cannot. This can be checked easily: The sequence of dissimilarities at\\nwhich clusters are merged in Figure 1 cannot decrease if the following condition is fulfilled for\\nall disjoint subsets I, J, K ⊂ S0 :\\nd(I, J) ≤ min{d(I, K), d(J, K)}\\n\\n⇒\\n\\nd(I, J) ≤ d(I ∪ J, K)\\n\\nOn the other hand, configurations with inversion in the “centroid” and “median” schemes\\ncan be easily produced, e.g. three points near the vertices of an equilateral triangle in R2 .\\nThe primitive algorithm takes Θ(N 3 ) time since in the i-th iteration of N − 1 in total, all\\nN −1−i\\n∈ Θ(i2 ) pairwise distances between the N − i nodes in S are searched.\\n2\\nNote that the stepwise dendrogram from a clustering problem (S, d) is not always uniquely\\ndefined, since the minimum in line 6 of the algorithm might be attained for several index\\n\\n6\\n\\n\\x0cFigure 2 Agglomerative clustering schemes.\\nName\\n\\nDistance update formula\\nFormula for d(I ∪ J, K)\\n\\nsingle\\n\\nmin(d(I, K), d(J, K))\\n\\ncomplete\\n\\nmax(d(I, K), d(J, K))\\n\\naverage\\n\\nnI d(I, K) + nJ d(J, K)\\nnI + nJ\\n\\nweighted\\nWard\\n\\nCluster dissimilarity\\nbetween clusters A and B\\nmin\\n\\na∈A,b∈B\\n\\nd[a, b]\\n\\nmax d[a, b]\\n\\na∈A,b∈B\\n\\n1\\n|A||B|\\n\\nd[a, b]\\na∈A b∈B\\n\\nd(I, K) + d(J, K)\\n2\\n(nI + nK )d(I, K) + (nJ + nK )d(J, K) − nK d(I, J)\\nnI + nJ + nK\\n\\n2|A||B|\\n· cA − cB\\n|A| + |B|\\n\\ncentroid\\n\\nnI d(I, K) + nJ d(J, K) nI nJ d(I, J)\\n−\\nnI + nJ\\n(nI + nJ )2\\n\\ncA − cB\\n\\nmedian\\n\\nd(I, K) d(J, K) d(I, J)\\n+\\n−\\n2\\n2\\n4\\n\\nwA − w B\\n\\n2\\n\\n2\\n\\n2\\n\\nLegend: Let I, J be two clusters joined into a new cluster, and let K be any other cluster.\\nDenote by nI , nJ and nK the sizes of (i.e. number of elements in) clusters I, J, K, respectively.\\nThe update formulas for the “Ward”, “centroid” and “median” methods assume that the input\\npoints are given as vectors in Euclidean space with the Euclidean distance as dissimilarity\\nmeasure. The expression cX denotes the centroid of a cluster X. The point wX is defined\\niteratively and depends on the clustering steps: If the cluster L is formed by joining I and J,\\nwe define wL as the midpoint 21 (wI + wJ ).\\nAll these formulas can be subsumed (for squared Euclidean distances in the three latter cases)\\nunder a single formula\\nd(I ∪ J, K) := αI d(I, K) + αJ d(J, K) + βd(I, J) + γ|d(I, K) − d(J, K)|,\\nwhere the coefficients αI , αJ , β may depend on the number of elements in the clusters I, J\\nand K. For example, αI = αJ = 21 , β = 0, γ = − 12 gives the single linkage formula. All\\nclustering methods which use this formula are combined under the name “flexible” in this\\npaper, as introduced by Lance and Williams (1967).\\nReferences: Lance and Williams (1967), Kaufman and Rousseeuw (1990, §5.5.1)\\n\\n7\\n\\n\\x0cpairs. We consider every possible output of Primitive_clustering under any choices of\\nminima as a valid output.\\n\\n3 Algorithms\\nIn the main section of this paper, we present three algorithms which are the most efficient ones\\nfor the task of SAHN clustering with the stored matrix approach. Two of the algorithms were\\ndescribed previously: The nearest-neighbor chain (“NN-chain”) algorithm by Murtagh (1985,\\npage 86), and an algorithm by Rohlf (1973), which we call “MST-algorithm” here since it is\\nbased on Prim’s algorithm for the minimum spanning tree of a graph. Both algorithms were\\npresented by the respective authors, but for different reasons each one still lacks a correctness\\nproof. Sections 3.2 and 3.3 state the algorithms in a way which is suitable for modern standard\\ninput and output structures and supply the proofs of correctness.\\nThe third algorithm in Section 3.1 is a new development based on Anderberg’s idea to\\nmaintain a list of nearest neighbors for each node Anderberg (1973, pages 135–136). While\\nwe do not show that the worst-case behavior of our algorithm is better than the O(N 3 ) worstcase complexity of Anderberg’s algorithm, the new algorithm is for all inputs at least equally\\nfast, and we show by experiments in Section 4.2 that the new algorithm is considerably faster\\nin practice since it cures Anderberg’s algorithm from its worst-case behavior at random input.\\nAs we saw in the last section, the solution to a hierarchical clustering task does not have a\\nsimple, self-contained specification but is defined as the outcome of the “primitive” clustering\\nalgorithm. The situation is complicated by the fact that the primitive clustering algorithm\\nitself is not completely specified: if a minimum inside the algorithm is attained at more than\\none place, a choice must be made. We do not require that ties are broken in a specific\\nway; instead we consider any output of the primitive algorithm under any choices as a valid\\nsolution. Each of the “advanced” algorithms is considered correct if it always returns one of\\nthe possible outputs of the primitive algorithm.\\n\\n3.1 The generic clustering algorithm\\nThe most generally usable algorithm is described in this section. We call it Generic_\\nlinkage since it can be used with any distance update formula. It is the only algorithm among\\nthe three in this paper which can deal with inversions in the dendrogram. Consequentially,\\nthe “centroid” and “median” methods must use this algorithm.\\nThe algorithm is presented in Figure 3. It is a sophistication of the primitive clustering\\nalgorithm and of Anderberg’s approach (Anderberg, 1973, pages 135–136). Briefly, candidates\\nfor nearest neighbors of clusters are cached in a priority queue to speed up the repeated\\nminimum searches in line 6 of Primitive_clustering.\\nFor the pseudocode in Figure 3, we assume that the set S are integer indices from 0 to\\nN − 1. This is the way in which it may be done in an implementation, and it makes the\\ndescription easier than for an abstract index set S. In particular, we rely on an order of the\\nindex set (see e.g. line 6: the index ranges over all y > x).\\nThere are two levels of sophistication from the primitive clustering algorithm to our generic\\nclustering algorithm. In a first step, one can maintain a list of nearest neighbors for each\\ncluster. For the sake of speed, it is enough to search for the nearest neighbors of a node x\\nonly among the nodes with higher index y > x. Since the dissimilarity index is symmetric, this\\nlist will still contain a pair of closest nodes. The list of nearest neighbors speeds up the global\\nminimum search in the i-th step from N −i−1\\ncomparisons to N − i − 1 comparisons at the\\n2\\nbeginning of each iteration. However, the list of nearest neighbors must be maintained: if the\\n\\n8\\n\\n\\x0cFigure 3 The generic clustering algorithm.\\n1:\\n2:\\n3:\\n4:\\n5:\\n6:\\n7:\\n8:\\n9:\\n10:\\n11:\\n12:\\n13:\\n14:\\n15:\\n16:\\n17:\\n18:\\n19:\\n20:\\n21:\\n22:\\n23:\\n24:\\n25:\\n26:\\n27:\\n28:\\n29:\\n30:\\n31:\\n32:\\n33:\\n34:\\n35:\\n36:\\n37:\\n38:\\n39:\\n40:\\n41:\\n42:\\n43:\\n\\nprocedure Generic_linkage(N, d)\\nN : input size, d: pairwise dissimilarities\\nS ← (0, . . . , N − 1)\\nL ← []\\nOutput list\\nsize[x] ← 1 for all x ∈ S\\nfor x in S \\\\ {N − 1} do\\nGenerate the list of nearest neighbors.\\nn_nghbr[x] ← argminy>x d[x, y]\\nmindist[x] ← d[x, n_nghbr[x]]\\nend for\\nQ ← (priority queue of indices in S \\\\ {N − 1}, keys are in mindist)\\nfor i ← 1, . . . , N − 1 do\\nMain loop.\\na ← (minimal element of Q)\\nb ← n_nghbr[a]\\nδ ← mindist[a]\\nwhile δ = d[a, b] do\\nRecalculation of nearest neighbors, if necessary.\\nn_nghbr[a] ← argminx>a d[a, x]\\nUpdate mindist and Q with (a, d[a, n_nghbr[a]])\\na ← (minimal element of Q)\\nb ← n_nghbr[a]\\nδ ← mindist[a]\\nend while\\nRemove the minimal element a from Q.\\nAppend (a, b, δ) to L.\\nMerge the pairs of nearest nodes.\\nsize[b] ← size[a] + size[b]\\nRe-use b as the index for the new node.\\nS ← S \\\\ {a}\\nfor x in S \\\\ {b} do\\nUpdate the distance matrix.\\nd[x, b] ← d[b, x] ← Formula(d[a, x], d[b, x], d[a, b], size[a], size[b], size[x])\\nend for\\nfor x in S such that x < a do\\nUpdate candidates for nearest neighbors.\\nif n_nghbr[x] = a then\\nDeferred search; no nearest\\nn_nghbr[x] ← b\\nneighbors are searched here.\\nend if\\nend for\\nfor x in S such that x < b do\\nif d[x, b] < mindist[x] then\\nn_nghbr[x] ← b\\nUpdate mindist and Q with (x, d[x, b])\\nPreserve a lower bound.\\nend if\\nend for\\nn_nghbr[b] ← argminx>b d[b, x]\\nUpdate mindist and Q with (b, d[b, n_nghbr[b]])\\nend for\\nreturn L\\nThe stepwise dendrogram, an ((N − 1) × 3)-matrix.\\nend procedure\\n\\n9\\n\\n\\x0cnearest neighbor of a node x is one of the clusters a, b which are joined, then it is sometimes\\nnecessary to search again for the nearest neighbor of x among all nodes y > x. Altogether,\\nthis reduces the best-case complexity of the clustering algorithm from Θ(N 3 ) to Θ(N 2 ), while\\nthe worst case complexity remains O(N 3 ). This is the method that Anderberg suggested in\\n(Anderberg, 1973, pages 135–136).\\nOn a second level, one can try to avoid or delay the nearest neighbor searches as long as\\npossible. Here is what the algorithm Generic_linkage does: It maintains a list n_nghbr\\nof candidates for nearest neighbors, together with a list mindist of lower bounds for the\\ndistance to the true nearest neighbor. If the distance d[x, n_nghbr[x]] is equal to mindist[x],\\nwe know that we have the true nearest neighbor, since we found a realization of the lower\\nbound; otherwise the algorithm must search for the nearest neighbor of x again.\\nTo further speed up the minimum searches, we also make the array mindist into a priority\\nqueue, so that the current minimum can be found quickly. We require a priority queue Q with\\na minimal set of methods as in the list below. This can be implemented conveniently by a\\nbinary heap (see Cormen et al., 2009, Chapter 6). We state the complexity of each operation\\nby the complexity for a binary heap.\\n• Queue(v): Generate a new queue from a vector v of length |v| = N . Return: an object\\nQ. Complexity: O(N ).\\n• Q.Argmin: Return the index to a minimal value of v. Complexity: O(1).\\n• Q.Remove_Min: Remove the minimal element from the queue. Complexity: O(log N ).\\n• Q.Update(i, x): Assign v[i] ← x and update the queue accordingly. Complexity:\\nO(log N ).\\nWe can now describe the Generic_linkage algorithm step by step: Lines 5 to 8 search\\nthe nearest neighbor and the closest distance for each point x among all points y > x. This\\ntakes O(N 2 ) time. In line 9, we generate a priority queue from the list of nearest neighbors\\nand minimal distances.\\nThe main loop is from line 10 to the end of the algorithm. In each step, the list L for\\na stepwise dendrogram is extended by one row, in the same way as the primitive clustering\\nalgorithm does.\\nLines 11 to 20 find a current pair of closest nodes. A candidate for this is the minimal index\\nin the queue (assigned to a), and its candidate for the nearest neighbor b := n_nghbr[a]. If\\nthe lower bound mindist[a] is equal to the actual dissimilarity d[a, b], then we are sure that\\nwe have a pair of closest nodes and their distance. Otherwise, the candidates for the nearest\\nneighbor and the minimal distance are not the true values, and we find the true values for\\nn_nghbr[a] and mindist[a] in line 15 in O(N ) time. We repeat this process and extract the\\nminimum among all lower bounds from the queue until we find a valid minimal entry in the\\nqueue and therefore the actual closest pair of points.\\nThis procedure is the performance bottleneck of the algorithm. The algorithm might be\\nforced to update the nearest neighbor O(N ) times with an effort of O(N ) for each of the\\nO(N ) iterations, so the worst-case performance is bounded by O(N 3 ). In practice, the inner\\nloop from lines 14 to 20 is executed less often, which results in faster performance.\\nLines 22 to 27 are nearly the same as in the primitive clustering algorithm. The only\\ndifference is that we specialize from an arbitrary label for the new node to re-using the index\\nb for the joined node. The index a becomes invalid, and we replace any nearest-neighbor\\nreference to a by a reference to the new cluster b in lines 28 to 32. Note that the array\\nn_nghbr contains only candidates for the nearest neighbors, so we could have written any\\n\\n10\\n\\n\\x0cvalid index here; however, for the single linkage method, it makes sense to choose b: if the\\nnearest neighbor of a node was at index a, it is now at b, which represents the join a ∪ b.\\nThe remaining code in the main loop ensures that the array n_nghbr still contains lower\\nbounds on the distances to the nearest neighbors. If the distance from the new cluster x to\\na cluster b < x is smaller than the old bound for b, we record the new smallest distance and\\nthe new nearest neighbor in lines 34 to 37.\\nLines 39 and 40 finally find the nearest neighbor of the new cluster and record it in the\\narrays n_nghbr and mindist and the queue Q\\nThe main idea behind this approach is that invalidated nearest neighbors are not recomputed immediately. Suppose that the nearest neighbor of a node x is far away from\\nx compared to the global closed pair of nodes. Then it does not matter that we do not know\\nthe nearest neighbor of x, as long as we have a lower bound on the distance to the nearest\\nneighbor. The candidate for the nearest neighbor might remain invalid, and the true distance\\nmight remain unknown for many iterations, until the lower bound for the nearest-neighbor\\ndistance has reached the top of the queue Q. By then, the set of nodes S might be much\\nsmaller since many of them were already merged, and the algorithm might have avoided many\\nunnecessary repeated nearest-neighbor searches for x in the meantime.\\nThis concludes the discussion of our generic clustering algorithm; for the performance see\\nSection 4. Our explanation of how the minimum search is improved also proves the correctness\\nof the algorithm: Indeed, in the same way as the primitive algorithm does, the Generic_\\nlinkage algorithm finds a pair of globally closest nodes in each iteration. Hence the output\\nis always the same as from the primitive algorithm (or more precisely: one of several valid\\npossibilities if the closest pair of nodes is not unique in some iteration).\\n\\n3.2 The nearest-neighbor-chain algorithm\\nIn this section, we present and prove correctness of the nearest-neighbor-chain algorithm\\n(shortly: NN-chain algorithm), which was described by Murtagh (1985, page 86). This algorithm can be used for the “single”, “complete”, “average”, “weighted” and “Ward” methods.\\nThe NN-chain algorithm is presented in Figure 4 as NN-chain-linkage. It consists of the\\ncore algorithm NN-chain-core and two postprocessing steps. Because of the postprocessing,\\nwe call the output of NN-chain-core an unsorted dendrogram. The unsorted dendrogram\\nmust first be sorted row-wise, with the dissimilarities in the third column as the sorting key.\\nIn order to correctly deal with merging steps which happen at the same dissimilarity, it is\\ncrucial that a stable sorting algorithm is employed, i.e. one which preserves the relative order\\nof elements with equal sorting keys. At this point, the first two columns of the output array\\nL contain the label of a member of the respective cluster, but not the unique label of the\\nnode itself. The second postprocessing step is to generate correct node labels from cluster\\nrepresentatives. This can be done in Θ(N ) time with a union-find data structure. Since this\\nis a standard technique, we do not discuss it here but state an algorithm in Figure 5 for the\\nsake of completeness. It generates integer node labels according to the convention in SciPy\\nbut can easily be adapted to follow any convention.\\nWe prove the correctness of the NN-chain algorithm in this paper for two reasons:\\n• We make sure that the algorithm resolves ties correctly, which was not in the scope of\\nearlier literature.\\n• Murtagh claims (Murtagh, 1984, page 111), (Murtagh, 1985, bottom of page 86) that\\nthe NN-chain algorithm works for any distance update scheme which fulfills a certain\\n“reducibility property”\\n\\n11\\n\\n\\x0cd(I, J) ≤ min{d(I, K), d(J, K)}\\n\\n⇒\\n\\nmin{d(I, K), d(J, K)} ≤ d(I ∪ J, K)\\n\\n(1)\\n\\nfor all disjoint nodes I, J, K at any stage of the clustering (Murtagh, 1984, § 3), (Murtagh, 1985, § 3.5). This is false.1 We give a correct proof which also shows the limitations\\nof the algorithm. In Murtagh’s papers (Murtagh, 1984, 1985), it is not taken into account\\nthat the dissimilarity between clusters may depend on the order of clustering steps; on\\nthe other hand, it is explicitly said that the algorithm works for the “weighted” scheme,\\nin which dissimilarities depend on the order of the steps.\\nSince there is no published proof for the NN-chain algorithm but claims which go beyond\\nwhat the algorithm can truly do, it is necessary to establish the correctness by a strict proof:\\nTheorem 1. Fix a distance update formula. For any sequence of merging steps and any four\\ndisjoint clusters I, J, K, L resulting from these steps, require two properties from the distance\\nupdate formula:\\n• It fulfills the reducibility property (1).\\n• The distance d(I ∪ J, K ∪ L) is independent of whether (I, J) are merged first and then\\n(K, L) or the other way round.\\nThen the algorithm NN-chain-linkage produces valid stepwise dendrograms for the given\\nmethod.\\nProposition 2. The “single”, “complete”, “average”, “weighted” and “Ward” distance update\\nformulas fulfill the requirements of Theorem 1.\\nProof of Theorem 1. We prove the theorem by induction in the size of the input set S. The\\ninduction start is trivial since a dendrogram for a one-point set is empty.\\nWe call two nodes a, b ∈ S reciprocal nearest neighbors (“pairwise nearest neighbors” in the\\nterminology of Murtagh, 1985) if the distance d[a, b] is minimal among all distances from a to\\npoints in S, and also minimal among all distances from b:\\nd[a, b] = min d[a, x] = min d[b, x].\\nx∈S\\nx=a\\n\\nx∈S\\nx=b\\n\\nEvery finite set S with at least two elements has at least one pair of reciprocal nearest\\nneighbors, namely a pair which realizes the global minimum distance.\\nThe list chain is in the algorithm constructed in a way such that every element is a nearest\\nneighbor of its predecessor. If chain ends in [. . . , b, a, b], we know that a and b are reciprocal\\nnearest neighbors. The main idea behind the algorithm is that reciprocal nearest neighbors\\na, b always contribute a row (a, b, d[a, b]) to the stepwise dendrogram, even if they are not\\ndiscovered in ascending order of dissimilarities.\\n1 For\\n\\nexample, consider the distance update formula d(I ∪ J, K) := d(I, K) + d(J, K) + d(I, J). This formula\\nfulfills the reducibility condition. Consider the following distance matrix between five points in the first\\ncolumn below. The Primitive_clustering algorithm produces the correct stepwise dendrogram in the\\nmiddle column. However, if the point A is chosen first in line 7 of NN-chain-core, the algorithm outputs\\nthe incorrect dendrogram in the right column.\\nA\\nB\\nC\\nD\\n\\nB\\n3\\n\\nC\\n4\\n5\\n\\nD\\n6\\n7\\n1\\n\\nE\\n15\\n12\\n13\\n14\\n\\n(C,\\n(A,\\n(AB,\\n(ABCD,\\n\\n12\\n\\nD,\\nB,\\nCD,\\nE,\\n\\n1)\\n3)\\n27)\\n85)\\n\\n(C,\\n(A,\\n(CD,\\n(AB,\\n\\nD,\\nB,\\nE,\\nCDE,\\n\\n1)\\n3)\\n28)\\n87)\\n\\n\\x0cFigure 4 The nearest-neighbor clustering algorithm.\\n1:\\n2:\\n3:\\n4:\\n5:\\n6:\\n\\nprocedure NN-chain-linkage(S, d)\\nS: node labels, d: pairwise dissimilarities\\nL ← NN-chain-core(N, d)\\nStably sort L with respect to the third column.\\nL ← Label(L)\\nFind node labels from cluster representatives.\\nreturn L\\nend procedure\\n\\n1:\\n2:\\n3:\\n\\nprocedure NN-chain-core(S, d)\\nS: node labels, d: pairwise dissimilarities\\nS ← (0, . . . , N − 1)\\nchain = [ ]\\nsize[x] ← 1 for all x ∈ S\\nwhile |S| > 1 do\\nif length(chain) ≤ 3 then\\na ← (any element of S)\\nE.g. S[0]\\nchain ← [a]\\nb ← (any element of S \\\\ {a})\\nE.g. S[1]\\nelse\\na ← chain[−4]\\nb ← chain[−3]\\nRemove chain[−1], chain[−2] and chain[−3]\\nCut the tail (x, y, x).\\nend if\\nrepeat\\nc ← argminx=a d[x, a] with preference for b\\na, b ← c, a\\nAppend a to chain\\nuntil length(chain) ≥ 3 and a = chain[−3]\\na, b are reciprocal\\nAppend (a, b, d[a, b]) to L\\nnearest neighbors.\\nRemove a, b from S\\nn ← (new node label)\\nsize[n] ← size[a] + size[b]\\nUpdate d with the information\\n\\n4:\\n5:\\n6:\\n7:\\n8:\\n9:\\n10:\\n11:\\n12:\\n13:\\n14:\\n15:\\n16:\\n17:\\n18:\\n19:\\n20:\\n21:\\n22:\\n23:\\n24:\\n\\nd[n, x] = d[x, n] = Formula(d[a, x], d[b, x], d[a, b], size[a], size[b], size[x])\\nfor all x ∈ S.\\nS ← S ∪ {n}\\nend while\\nreturn L\\nan unsorted dendrogram\\nend procedure\\n(We use the Python index notation: chain[−2] is the second-to-last element in the list chain.)\\n25:\\n26:\\n27:\\n28:\\n\\n13\\n\\n\\x0cFigure 5 A union-find data structure suited for the output conversion.\\n1:\\n2:\\n3:\\n4:\\n5:\\n6:\\n7:\\n8:\\n9:\\n10:\\n\\nprocedure Label(L)\\nL ← []\\nN ← (number of rows in L) + 1\\nNumber of initial nodes.\\nU ← new Union-Find(N )\\nfor (a, b, δ) in L do\\nAppend (U.Efficient-Find(a), U.Efficient-Find(b), δ) to L\\nU.Union(a, b)\\nend for\\nreturn L\\nend procedure\\n\\nclass Union-Find\\nmethod Constructor(N )\\nparent ← new int[2N − 1]\\nparent[0, . . . , 2N − 2] ← None\\n15:\\nnextlabel ← N\\n16:\\nend method\\n\\n11:\\n12:\\n13:\\n14:\\n\\n17:\\n18:\\n19:\\n20:\\n21:\\n22:\\n23:\\n24:\\n25:\\n26:\\n27:\\n28:\\n29:\\n30:\\n31:\\n32:\\n33:\\n34:\\n35:\\n36:\\n37:\\n38:\\n\\nmethod Union(m, n)\\nparent[m] = nextlabel\\nparent[n] = nextlabel\\nnextlabel ← nextlabel + 1\\nend method\\n\\nN is the number of data points.\\n\\nSciPy convention: new labels start at N\\n\\nSciPy convention: number new labels consecutively\\n\\nmethod Find(n)\\nThis works but the search process is not efficient.\\nwhile parent[n] is not None do\\nn ← parent[n]\\nend while\\nreturn n\\nend method\\nmethod Efficient-Find(n)\\np←n\\nwhile parent[n] is not None do\\nn ← parent[n]\\nend while\\nwhile parent[p] = n do\\np, parent[p] ← parent[p], n\\nend while\\nreturn n\\nend method\\nend class\\n\\nThis speeds up repeated calls.\\n\\n14\\n\\n\\x0cLines 15 to 19 in NN-chain-core clearly find reciprocal nearest neighbors (a, b) in S.\\nOne important detail is that the index b is preferred in the argmin search in line 16, if the\\nminimum is attained at several indices and b realizes the minimum. This can be respected in\\nan implementation with no effort, and it ensures that reciprocal nearest neighbors are indeed\\nfound. That is, the list chain never contains a cycle of length > 2, and a chain = [. . . , b, a]\\nwith reciprocal nearest neighbors at the end will always be extended by b, never with an\\nelement c = b which coincidentally has the same distance to a.\\nAfter line 19, the chain ends in (b, a, b). The nodes a and b are then joined, and the internal\\nvariables are updated as usual.\\nWe now show that the remaining iterations produce the same output as if the algorithm had\\nstarted with the set S := (S \\\\ {a, b}) ∪ {n}, where n is the new node label and the distance\\narray d and the size array are updated accordingly.\\nThe only data which could potentially be corrupted is that the list chain could not contain\\nsuccessive nearest neighbors any more, since the new node n could have become the nearest\\nneighbor of a node in the list.\\nAt the beginning of the next iteration, the last elements (b, a, b) are removed from chain.\\nThe list chain then clearly does not contain a or b at any place any more, since any occurrence\\nof a or b in the list would have led to an earlier pair of reciprocal nearest neighbors, before\\n(b, a, b) was appended to the list. Hence, chain contains only nodes which really are in S. Let\\ne, f be two successive entries in chain, i.e. f is a nearest neighbor of e. Then we know\\nd[e, f ] ≤ d[e, a]\\n\\nd[a, b] ≤ d[a, e]\\n\\nd[e, f ] ≤ d[e, b]\\n\\nd[a, b] ≤ d[b, e]\\n\\nTogether with the reducibility property (1) (for I = a, J = b, K = e), this implies d[e, f ] ≤\\nd[e, n]. Hence, f is still the nearest neighbor of e, which proves our assertion.\\nWe can therefore be sure that the remaining iterations of NN-chain-core produce the\\nsame output as if the algorithm would be run freshly on S . By the inductive assumption,\\nthis produces a valid stepwise dendrogram for the set S with N − 1 nodes. Proposition 3\\ncarries out the remainder of the proof, as it shows that the first line (a, b, d[a, b]) of the unsorted\\ndendrogram, when it is sorted into the right place in the dendrogram for the nodes in S , is\\na valid stepwise dendrogram for the original set S with N nodes.\\nProposition 3. Let (S, d) be a set with dissimilarities (|S| > 1). Fix a distance update\\nformula which fulfills the requirements in Theorem 1. Let a, b be two distinct nodes in S\\nwhich are reciprocal nearest neighbors.\\nDefine S as (S \\\\ {a, b}) ∪ {n}, where the label n represents the union a ∪ b. Let d be the updated dissimilarity matrix for S , according to the chosen formula. Let L = ((ai , bi , δi )i=0,...,m )\\nbe a stepwise dendrogram for S . Let j be the index such that all δi < d[a, b] for all i < j\\nand δi ≥ d[a, b] for all i ≥ j. That is, j is the index where the new row (a, b, d[a, b]) should\\nbe inserted to preserve the sorting order, giving d[a, b] priority over potentially equal sorting\\nkeys. Then the array L, which we define as\\n\\uf8ee\\n\\uf8f9\\na0\\nb0\\nδ0\\n\\uf8ef\\n\\uf8fa\\n\\uf8ef ...\\n\\uf8fa\\n\\uf8ef\\n\\uf8fa\\n\\uf8ef aj−1 bj−1 δj−1 \\uf8fa\\n\\uf8ef\\n\\uf8fa\\n\\uf8ef\\n\\uf8fa\\n→ \\uf8ef a\\nb\\nd[a, b] \\uf8fa ←\\n\\uf8ef\\n\\uf8fa\\n\\uf8ef aj\\n\\uf8fa\\nbj\\nδj\\n\\uf8ef\\n\\uf8fa\\n\\uf8ef\\n\\uf8fa\\n\\uf8f0 ...\\n\\uf8fb\\nam\\nbm\\nδm\\n\\n15\\n\\n\\x0cis a stepwise dendrogram for (S, d).\\nProof. Since a and b are reciprocal nearest neighbors at the beginning, the reducibility property (1) guarantees that they stay nearest neighbors after any number of merging steps between\\nother reciprocal nearest neighbors. Hence, the first j steps in a dendrogram for S cannot contain a or b, since these steps all happen at merging dissimilarities smaller than d[a, b]. This is\\nthe point where we must require that the sorting in line 3 of NN-chain-linkage is stable.\\nMoreover, the first j rows of L cannot contain a reference to n: Again by the reducibility\\nproperty, dissimilarities between n and any other node are at least as big as d[a, b]. Therefore,\\nthe first j rows of L are correct for a dendrogram for S.\\nAfter j steps, we know that no inter-cluster distances in S \\\\ {a, b} are smaller than d[a, b].\\nAlso, d[a, b] is minimal among all distances from a and b, so the row (a, b, d[a, b]) is a valid\\nnext row in L.\\nAfter this step, we claim that the situation is the same in both settings: The sets S after\\nj steps and the set S after j + 1 steps, including the last one merging a and b into a new\\ncluster n, are clearly equal as partitions of the original set. It is required to check that also\\nthe dissimilarities are the same in both settings. This is where we need the second condition\\nin Theorem 1:\\nThe row (a, b, d[a, b]) on top of the array L differs from the dendrogram L by j transpositions, where (a, b, d[a, b]) is moved one step downwards. Each transposition happens between\\ntwo pairs (a, b) and (ai , bi ), where all four nodes are distinct, as shown above. The dissimilarity from a distinct fifth node x to the join a ∪ b does not depend on the merging of ai and bi\\nsince there is no way in which dissimilarities to ai and bi enter the distance update formula\\nFormula(d[a, x], d[b, x], d[a, b], size[a], size[b], size[x]). The symmetric statement holds for the\\ndissimilarity d[x, ai ∪ bi ]. The nodes a, b, ai , bi are deleted after the two steps, so dissimilarities\\nlike d[a, ai ∪ bi ] can be neglected. The only dissimilarity between active nodes which could be\\naltered by the transposition is d[a ∪ b, ai ∪ bi ]. It is exactly the second condition in Theorem 1\\nthat this dissimilarity is independent of the order of merging steps. This finishes the proof of\\nTheorem 1.\\nWe still have to prove that the requirements of Theorem 1 are fulfilled by the “single”,\\n“complete”, “average”, “weighted” and “Ward” schemes:\\nProof of Proposition 2. It is easy and straightforward to check from the table in Figure 2\\nthat the distance update schemes in question fulfill the reducibility property. Moreover, the\\ntable also conveys that the dissimilarities between clusters in the “single”, “complete” and\\n“average” schemes do not depend on the order of the merging steps.\\nFor Ward’s scheme, the global dissimilarity expression in the third column in Figure 2\\napplies only if the dissimilarity matrix consists of Euclidean distances between vectors (which\\nis the prevalent setting for Ward’s method). For a general argument, note that the global\\ncluster dissimilarity for Ward’s method can also be expressed by a slightly more complicated\\nexpression:\\nd(A, B) =\\n\\n1\\n|A| + |B|\\n\\nd(a, b)2 −\\n\\n2\\na∈A b∈B\\n\\n|B|\\n|A|\\n\\nd(a, a )2 −\\na∈A a ∈A\\n\\n|A|\\n|B|\\n\\nd(b, b )2\\nb∈B b ∈B\\n\\nThis formula can be proved inductively from the recursive distance update formula for Ward’s\\nmethod, hence it holds independently of whether the data is Euclidean or not. This proves\\nthat the dissimilarities in Ward’s scheme are also independent of the order of merging steps.\\nDissimilarities in the “weighted” scheme, however, do in general depend on the order of\\nmerging steps. However, the dissimilarity between joined nodes I ∪ J and K ∪ L is always the\\n\\n16\\n\\n\\x0cFigure 6 The single linkage algorithm.\\n1:\\n2:\\n3:\\n4:\\n5:\\n6:\\n\\n1:\\n2:\\n3:\\n4:\\n5:\\n6:\\n7:\\n8:\\n9:\\n10:\\n11:\\n12:\\n13:\\n14:\\n15:\\n\\nprocedure MST-linkage(S, d)\\nS: node labels, d: pairwise dissimilarities\\nL ← MST-linkage-core(S, d)\\nStably sort L with respect to the third column.\\nL ← Label(L)\\nFind node labels from cluster representatives.\\nreturn L\\nend procedure\\nprocedure MST-linkage-core(S0 , d)\\nL ← []\\nc ← (any element of S0 )\\nD0 [s] ← ∞ for s ∈ S0 \\\\ {c}\\nfor i in (1, . . . , |S0 | − 1) do\\nSi ← Si−1 \\\\ {c}\\nfor s in Si do\\nDi [s] ← min{Di−1 [s], d[s, c]}\\nend for\\nn ← argmins∈Si Di [s]\\nAppend (c, n, Di [n]) to L\\nc←n\\nend for\\nreturn L\\nend procedure\\n\\nS0 : node labels, d: pairwise dissimilarities\\nc: current node\\n\\nn: new node\\n\\nan unsorted dendrogram\\n\\nmean dissimilarity 41 (d[I, K] + d[I, L] + d[J, K] + d[J, L]), independent of the order of steps,\\nand this is all that is required for Proposition 2.\\n\\n3.3 The single linkage algorithm\\nIn this section, we present and prove correctness of a fast algorithm for single linkage clustering. Gower and Ross (1969) observed that a single linkage dendrogram can be obtained\\nfrom a minimum spanning tree (MST) of the weighted graph which is given by the complete\\ngraph on the singleton set S with the dissimilarities as edge weights. The algorithm here\\nwas originally described by Rohlf (1973) and is based on Prim’s algorithm for the MST (see\\nCormen et al., 2009, § 23.2).\\nThe single linkage algorithm MST-linkage is given in Figure 6. In the same way as the NNchain algorithm, it consists of a core algorithm MST-linkage-core and two postprocessing\\nsteps. The output structure of the core algorithm is again an unsorted list of clustering\\nsteps with node representatives instead of unique labels. As will be proved, exactly the same\\npostprocessing steps can be used as for the NN-chain algorithm.\\nRohlf’s algorithm in its original version is a full Prim’s algorithm and maintains enough\\ndata to generate the MST. He also mentions a possible simplification which does not do\\nenough bookkeeping to generate an MST but enough for single linkage clustering. It is this\\nsimplification that is discussed in this paper. We prove the correctness of this algorithm for\\ntwo reasons:\\n• Since the algorithm MST-linkage-core does not generate enough information to reconstruct a minimum spanning tree, one cannot refer to the short proof of Prim’s algo-\\n\\n17\\n\\n\\x0crithm in any easy way to establish the correctness of MST-linkage.\\n• Like for the NN-chain algorithm in the last section, it is not clear a priori that the\\nalgorithm resolves ties correctly. A third algorithm can serve as a warning here (see\\nSection 5 for more details): There is an other fast algorithm for single linkage clustering, Sibson’s SLINK algorithm (Sibson, 1973). More or less by coincidence, all three\\nalgorithms NN-chain-core, MST-linkage-core and SLINK generate output which\\ncan be processed by exactly the same two steps: sorting followed by Label. In case\\nof the SLINK algorithm this works fine if all dissimilarities are distinct but produces\\nwrong stepwise dendrograms in situations when two merging dissimilarities are equal.\\nThere is nothing wrong with the SLINK algorithm, however. Sibson supplied a proof for\\nthe SLINK algorithm in his paper (Sibson, 1973), but it is written for a (non-stepwise)\\ndendrogram as the output structure, not for a stepwise dendrogram. Hence, the additional information which is contained in a stepwise dendrogram in the case of ties is not\\nprovided by all, otherwise correct algorithms.\\nThis should be taken as a warning that ties demand more from an algorithm and must be\\nexplicitly taken into account when we prove the correctness of the MST-linkage algorithm\\nbelow.\\nTheorem 4. The algorithm MST-linkage yields an output which can also be generated by\\nPrimitive_clustering.\\nWe do not explicitly refer to Prim’s algorithm in the following, and we make the proof selfcontained, since the algorithm does not collect enough information to construct a minimum\\nspanning tree. There are unmistakable similarities, of course, and the author got most of the\\nideas for this proof from Prim’s algorithm (see Cormen et al., 2009, § 23.2).\\nLet us first make two observations about the algorithm MST-linkage.\\n(a) Starting with the full initial set S0 , the algorithm MST-linkage-core chooses a “current node” c in each step and removes it from the current set Si in every iteration. Let\\nSic := S0 \\\\ Si be the complement of the current set Si . Then Di [s] (s ∈ Si ) is the\\ndistance from Sic to s, i.e.\\nDi [s] = minc d[s, t].\\nt∈Si\\n\\n(b) Let L be the output of MST-linkage-core(S, d). The 2i entries in the first two\\ncolumns and the first i rows contain only i + 1 distinct elements of S, since the second\\nentry in one row is the first entry in the next row.\\nWe prove Theorem 4 by the following stronger variant:\\nTheorem 5. Let L be the output of MST-linkage-core(S, d). For all n < |S|, the first n\\nrows of L are an unsorted single linkage dendrogram for the n + 1 points of S in this list (see\\nObservation (b)).\\nProof. We proceed by induction. After the first iteration, the list L contains one triple\\n(a0 , b0 , δ0 ). δ0 = D1 [b0 ] is clearly the dissimilarity d[a0 , b0 ], since the array D1 contains the\\ndissimilarities to a0 after the first iteration (Observation (a)).\\nLet (a0 , b0 , δ0 ), . . . , (an , bn , δn ) be the first n + 1 rows of L. We sort the rows with a stable\\nsorting algorithm as specified in MST-linkage. We leave the postprocessing step Label out\\nof our scope and work with the representatives ai , bi for the rest of the proof.\\n\\n18\\n\\n\\x0cLet s(0), . . . , s(n) be the stably sorted indices (i.e. δs(i) ≤ δs(i+1) for all i and s(i) < s(i + 1)\\nif δs(i) = δs(i+1) ). Let k be the sorted index of the last row n. Altogether, we have a sorted\\nmatrix\\n\\uf8f9\\n\\uf8ee\\nas(0)\\nbs(0)\\nδs(0)\\n\\uf8fa\\n\\uf8ef\\n\\uf8fa\\n\\uf8ef ...\\n\\uf8fa\\n\\uf8ef\\n\\uf8ef as(k−1) bs(k−1) δs(k−1) \\uf8fa\\n\\uf8fa\\n\\uf8ef\\n\\uf8fa\\n\\uf8ef\\n→ \\uf8ef as(k)\\nbs(k)\\nδs(k)\\n\\uf8fa ←\\n\\uf8fa\\n\\uf8ef\\n\\uf8ef as(k+1) bs(k+1) δs(k+1) \\uf8fa\\n\\uf8fa\\n\\uf8ef\\n\\uf8fa\\n\\uf8ef\\n\\uf8fb\\n\\uf8f0 ...\\nas(n)\\nbs(n)\\nδs(n)\\nThe new row is at the index k, i.e. (as(k) , bs(k) , δs(k) ) = (an , bn , δn ). The matrix without\\nthe k-th row is a valid stepwise, single linkage dendrogram for the points a0 , . . . , an , by the\\ninduction hypothesis. (Recall that bi = ai+1 .) Our goal is to show that the matrix with its\\nk-th row inserted yields a valid single linkage dendrogram on the points a0 , . . . , an , bn .\\nFirst step: rows 0 to k − 1. The distance δn is the minimal distance from bn to any\\nof the points a0 , . . . , an . Therefore, the dendrograms for the sets S − := {a0 , . . . , an } and\\nS + := S − ∪ {bn } have the same first k steps, when all the inter-cluster distances are smaller\\nthan or equal to δn . (If the distance δn occurs more than once, i.e. when δs(k−1) = δn , we\\nassume by stable sorting that the node pairs which do not contain bn are chosen first.)\\nTherefore, the first k rows are a possible output of Primitive_clustering in the first k\\nsteps. After this step, we have the same partial clusters in S + as in the smaller data set, plus\\na singleton {bn }.\\nSecond step: row k. The distance δn from bn to some point a0 , . . . , an is clearly the\\nsmallest inter-cluster distance at this point, since all other inter-cluster distances are at least\\nδs(k+1) , which is greater than δs(k) = δn . Since the output row is (an , bn , δn ), it remains to\\ncheck that the distance δn is realized as the distance from bn to a point in the cluster of an ,\\ni.e. that an is in a cluster with distance δn to bn .\\nThe clusters mentioned in the last sentence refer to the partition of S + which is generated\\nby the relations as(0) ∼ bs(0) , . . . , as(k−1) ∼ bs(k−1) . Since we have bi = ai+1 , the partition of\\nS + consists of contiguous chains in the original order of points a0 , a1 , . . . , an , bn .\\nThe diagram below visualizes a possible partition after k steps.\\na0 a1 a2 a3 a4 a5\\n\\n...\\n\\n. . . am\\n\\n. . . an bn\\n\\nIn this particular example, the distances δ0 , δ1 and δ4 are among the first k smallest, while\\nδ2 , δ3 and δ5 come later in the sorted order.\\nLet δn be realized as the distance between bn and am for some m ≤ n. Then the dissimilarities between consecutive points in the sequence am , bm = am+1 , bm+1 = am+2 , . . . ,\\nbn−1 = an must be less than or equal to δn ; otherwise bn and the dissimilarity δn = d[bn , am ]\\nwould have been chosen first over these other dissimilarities in one of the first k steps. Since\\nthe dissimilarities of all pairs (ai , bi ) in this chain are not more than δn , they are contained\\nin the first k sorted triples. Hence, am and an have been joined into a cluster in the first k\\nsteps, and an is a valid representative of the cluster that also contains am .\\nNote that the argument in the last paragraph is the point where we need that the sorting\\nin line 3 of MST-linkage is stable. Otherwise it could not be guaranteed that am and an\\nhave been joined into a cluster before bn is added.\\nThird step: rows k + 1 to n. Here is the situation after row k: We have the same clusters\\nin S + as after k steps in the smaller data set S, except that the last cluster (the one which\\n\\n19\\n\\n\\x0ccontains an ) additionally contains the point bn . In a diagram:\\na0 a1 a2 a3 a4 a5\\n\\n...\\n\\n. . . am\\n\\n. . . an bn\\n\\nThe inter-cluster distances in S + from the cluster with bn to the other clusters might be\\nsmaller than without the point bn in S − . We show, however, that this does not affect the\\nremaining clustering steps:\\nIn each step r > k, we have the following situation for some x ≤ y ≤ s(k). The point bn\\nmight or might not be in the same cluster as bs(r) .\\n. . . ax\\n\\n. . . ay\\n\\n. . . as(r) bs(r) = as(r)+1\\n\\n...\\n\\n. . . bn−1\\n\\nLet the distance δs(r) be realized as the distance from bs(r) to ay . From Observation (a) and\\nline 10 in MST-linkage-core, we know that this distance is minimal among the distances\\nfrom X := {a0 , . . . , as(r) } to all other points in S0 \\\\ X. In particular, the distance from X to\\nbn ∈ S0 \\\\ X is not smaller than δs(r) .\\nThis proves that the addition of bn in step k does not change the single linkage clustering\\nin any later step r > k. This completes the inductive proof of Theorem 5\\n\\n4 Performance\\nIn this section, we compare the performance of the algorithms and give recommendations on\\nwhich algorithm to choose for which clustering method. We compare both the theoretical,\\nasymptotic worst-case performance, and the use-case performance on a range of synthetic\\nrandom data sets.\\n\\n4.1 Asymptotic worst-case performance\\nLet N denote the problem size, which is in this case the number of input data points. The\\ninput size is N2 ∈ Θ(N 2 ).\\nThe asymptotic run-time complexity of MST-linkage-core is obviously Θ(N 2 ), since\\nthere are two nested levels of loops in the algorithm (line 8 and implicitly line 10). The\\nrun-time complexity of the NN-chain-core algorithm is also Θ(N 2 ) (Murtagh, 1985, page\\n86). Postprocessing is the same for both algorithms and is less complex, namely O(N log N )\\nfor sorting and Θ(N ) for Label, so the overall complexity is Θ(N 2 ). This is optimal (in the\\nasymptotic sense): the lower bound is also quadratic since all Θ(N 2 ) input values must be\\nprocessed.\\nThe NN-chain algorithm needs a writable working copy of the input array to store intermediate dissimilarities and otherwise only Θ(N ) additional memory.\\nThe generic algorithm has a best-case time complexity of Θ(N 2 ), but without deeper analysis, the worst-case complexity is O(N 3 ). The bottleneck is line 15 in Generic_linkage:\\nIn O(N ) iterations, this line might be executed up to O(N ) times and does a minimum search\\nover O(N ) elements, which gives a total upper bound of O(N 3 ). This applies for all clustering\\nschemes except single linkage, where the loop starting at line 14 is never executed and thus\\nthe worst-case performance is Θ(N 2 ). The memory requirements for the generic algorithm are\\nsimilar to the NN-chain algorithm: a working copy of the dissimilarity array and additionally\\nonly Θ(N ) temporary memory.\\nIn contrast, the MST algorithm does not write to the input array d. All other temporary\\nvariables are of size O(N ). Hence, MST-linkage requires no working copy of the input\\n\\n20\\n\\n\\x0carray and hence only half as much memory as Generic_linkage and NN-chain-linkage\\nasymptotically.\\nAnderberg’s algorithm (Anderberg, 1973, pages 135–136) has the same asymptotic bounds\\nas our generic algorithm. The performance bottleneck are again the repeated minimum\\nsearches among the updated dissimilarities. Since the generic algorithm defers minimum\\nsearches to a later point in the algorithm (if they need to be performed at all, by then),\\nthere are at least as many minimum searches among at least as many elements in Anderberg’s algorithm as in the generic algorithm. The only point where the generic algorithm\\ncould be slower is the maintenance of the priority queue with nearest neighbor candidates,\\nsince this does not exist in Anderberg’s algorithm. The bottleneck here are potentially up to\\nO(N 2 ) updates of a queue in line 36 of Generic_linkage. In the implementation in the\\nnext section, the queue is realized by a binary heap, so an update takes O(log N ) time. This\\ncould potentially amount to O(N 2 log N ) operations for maintenance of the priority queue.\\nHowever, a reasonable estimate is that the saved minimum searches in most cases save more\\ntime than the maintenance of the queue with O(N ) elements costs, and hence there is a good\\nreason to believe that the generic algorithm is at least as fast as Anderberg’s algorithm.\\nNote that the maintenance effort of the priority queue can be easily reduced to O(N 2 )\\ninstead of O(N 2 log N ) worst case:\\n• A different priority queue structure can be chosen, where the “decrease-key” operation\\ntakes only O(1) time. (Note that the bottleneck operation in line 36 of Generic_linkage never increases the nearest-neighbor distance, only decreases it.) The author did\\nnot test a different structure since a binary heap convinces by its simple implementation.\\n• Changed keys (minimal distances) need not be updated in the priority queue immediately. Instead, the entire queue might be resorted/regenerated at the beginning of\\nevery iteration. This takes N − 1 times O(N ) time with a binary heap. Although this\\nlowers the theoretical complexity for the maintenance of the binary queue, it effectively\\nslowed down the algorithms in practice by a small margin. The reason is, of course, that\\nthe number and complexity of updates of the priority queue did by far not reach their\\ntheoretical upper bound in our test data sets (see below). Altogether, the maintenance\\nof the priority queue, as proposed in Figure 3 seems quite optimal from the practical\\nperspective.\\n\\n4.2 Use-case performance\\nIn addition to the theoretical, asymptotic and worst-case considerations, we also measured\\nthe practical performance of the algorithms. Figure 7 shows the run-time of the algorithms\\nfor a number of synthetic test data sets (for details see below). The solid lines are the average\\nover the data sets. (The graphs labeled “Day-Edelsbrunner” are discussed in Section 5.) The\\nlightly colored bands show the range from minimum to maximum time over all data sets for\\na given number of points.\\nThe following observations can be made:\\n• For single linkage clustering, the MST-algorithm is clearly the fastest one. Together\\nwith the fact that it has only half the memory requirements of the other algorithms (if\\nthe input array is to be preserved), and thus allows the processing of larger data sets,\\nthe MST-algorithm is clearly the best choice for single linkage clustering.\\n• For the clustering schemes without inversions (all except “centroid” and “median”), the\\ngeneric algorithm, the NN-chain algorithm and Anderberg’s algorithm have very similar\\nperformance.\\n\\n21\\n\\n\\x0c103\\n102\\n\\nMethod: “single”\\n\\nCPU time in s\\n\\n101\\n100\\n10−1\\n10−2\\n10−3\\n10−4\\n10−5\\n\\n101\\n\\n102\\n\\n103\\nNumber of points\\n\\n104\\n\\n101\\n\\n102\\n\\n103\\nNumber of points\\n\\n104\\n\\n101\\n\\n102\\n\\n103\\nNumber of points\\n\\n104\\n\\n103\\n102\\n\\nMethod: “average”\\n“complete”, “weighted” and\\n“Ward” look very similar.\\n\\nCPU time in s\\n\\n101\\n100\\n10−1\\n10−2\\n10−3\\n10−4\\n10−5\\n\\n103\\n102\\n\\nMethod: “centroid”\\n“median” looks very similar.\\n\\nCPU time in s\\n\\n101\\n100\\n10−1\\n10−2\\n10−3\\n10−4\\n10−5\\n\\nFigure 7: Performance of several SAHN clustering algorithms. Legend:\\nGeneric algorithm\\n(Figure 3), Anderberg (1973, pages 135–136), NN-chain algorithm (Figure 4),\\nMST-algorithm (Figure 6), Day and Edelsbrunner (1984, Table 5).\\n\\n22\\n\\n\\x0cThe NN-chain algorithm is the only one with guaranteed O(N 2 ) performance here. We\\ncan conclude that the good worst-case performance can be had here without any cutbacks to the use-case performance.\\n• For the “centroid” and “median” methods, we see a very clear disadvantage to Anderberg’s algorithm. Here, the worst case cubic time complexity occurs already in the\\nrandom test data sets. This happens with great regularity, over the full range of input\\nsizes. Our Generic_linkage algorithm, on the other hand, does not suffer from this\\nweakness: Even though the theoretical worst-case bounds are the same, the complexity\\ndoes not raise above the quadratic behavior in our range of test data sets. Hence, we\\nhave grounds to assume that Generic_linkage is much faster in practice.\\n\\n4.3 Conclusions\\nBased on the theoretical considerations and use-case tests, we can therefore recommend algorithms for the various distance update schemes as follows:\\n• “single” linkage clustering: The MST-algorithm is the best, with respect to worst-case\\ncomplexity, use-case performance and memory requirements.\\n• “complete”, “average”, “weighted”, “ward”: The NN-chain algorithm is preferred, since\\nit guarantees O(N 2 ) worst case complexity without any disadvantage to practical performance and memory requirements.\\n• “centroid”, “median”: The generic clustering algorithm is the best choice, since it can\\nhandle inversions in the dendrogram and the performance exhibits quadratic complexity\\nin all observed cases.\\nOf course, the timings in the use-case tests depend on implementation, compiler optimizations, machine architecture and the choice of data sets. Nevertheless, the differences between\\nthe algorithms are very clear here, and the comparison was performed with careful implementations in the identical environment.\\nThe test setup was as follows: All algorithms were implemented in C++ with an interface\\nto Python (van Rossum et al.) and the scientific computing package NumPy (Num) to handle\\nthe input and output of arrays. The test data sets are samples from mixtures of multivariate\\nGaussian distributions with unity covariance\\nmatrix in various dimensions (2, 3, 10, 200) with\\n√\\nvarious numbers of modes (1, 5, [ N ]), ranging from N = 10 upwards until memory was\\nexhausted (N = 20000 except for single linkage). The centers of the Gaussian distributions\\nare also distributed by a Gaussian distribution. Moreover, for the methods for which it makes\\nsense (single, complete, average, weighted: the “combinatorial” methods), we also generated\\n10 test sets per number of input points with a uniform distribution of dissimilarities.\\nThe timings were obtained on a PC with an Intel dual-core CPU T7500 with 2.2 GHz clock\\nspeed and 4GB of RAM and no swap space. The operating system was Ubuntu 11.04 64-bit,\\nPython version: 2.7.1, NumPy version: 1.5.1, compiler: GNU C++ compiler, version 4.5.2.\\nOnly one core of the two available CPU cores was used in all computations.\\n\\n5 Alternative algorithms\\nThe MST algorithm has the key features that it (1) needs no working copy of the Θ(N 2 ) input\\narray and only Θ(N ) working memory, (2) is fast since it reads every input dissimilarity only\\nonce and otherwise deals only with Θ(N ) memory. There is a second algorithm with these\\n\\n23\\n\\n\\x0ccharacteristics, Sibson’s SLINK algorithm (Sibson, 1973). It is based on the insight that a\\nsingle linkage dendrogram for N + 1 points can be computed from the dendrogram of the first\\nN points plus a single row of distances (d[N, 0], . . . , d[N, N − 1]). In this fashion, the SLINK\\nalgorithm even reads the input dissimilarities in a fixed order, which can be an advantage\\nover the MST algorithm if the favorable input order can be realized in an application, or if\\ndissimilarities do not fit into random-access memory and are read from disk.\\nHowever, there is one important difference: even though the output data format looks\\ndeceptively similar to the MST algorithm (the output can be converted to a stepwise dendrogram by exactly the same process: sorting with respect to dissimilarities and a union-find\\nprocedure to generate node labels from cluster representatives), the SLINK algorithm cannot handle ties. This is definite, since e.g. the output in the example situation on page 5 is\\nthe same in all three cases, and hence no postprocessing can recover the different stepwise\\ndendrograms.\\nThere is an easy way out by specifying a secondary order\\nd(i, j) ≺ d(k, l)\\n\\n:⇐⇒\\n\\nd(i, j) < d(k, l)\\nNi + j < Nk + l\\n\\nif this holds,\\nif d(i, j) = d(k, l)\\n\\nto make all dissimilarities artificially distinct. In terms of performance, the extra comparisons\\nput a slight disadvantage on the SLINK algorithm, according to the author’s experiments.\\nHowever, the difference is not much, and the effect on timings may be compensated or even\\nreversed in a different software environment or when the input order of dissimilarities is in\\nfavor of SLINK. Hence, the SLINK algorithm is a perfectly fine tool, as long as care is taken\\nto make all dissimilarities unique.\\nThe same idea of generating a dendrogram inductively is the basis of an algorithm by\\nDefays (1977). This paper is mostly cited as a fast algorithm for complete linkage clustering.\\nHowever, it definitely is not an algorithm for complete linkage clustering, as the complete\\nlinkage method is commonly defined, in this paper and identically elsewhere.\\nAn algorithm which is interesting from the theoretical point of view is given by Day and\\nEdelsbrunner (1984, Table 5). It uses N priority queues for the nearest neighbor of each\\npoint. By doing so, the authors achieve a worst-case time complexity of O(N 2 log N ), which\\nis better than the existing bound O(N 3 ) for the schemes where the NN-chain algorithm\\ncannot be applied. The overhead for maintaining a priority queue for each point, however,\\nslows the algorithm down in practice. The performance measurements in Figure 7 include\\nthe Day-Edelsbrunner algorithm. Day and Edelsbrunner write their algorithm in general\\nterms, for any choice of priority queue structure. We implemented the algorithm for the\\nmeasurements in this paper with binary heaps, since these have a fixed structure and thus\\nrequire the least additional memory. But even so, the priority queues need additional memory\\nof order Θ(N 2 ) for their bookkeeping, which can also be seen in the graphs since they stop\\nat fewer points, within the given memory size of the test. The graphs show that even if the\\nDay-Edelsbrunner algorithm gives the currently best asymptotic worst-case bound for the\\n“centroid” and “median” methods, it is inefficient for practical purposes.\\nKřivánek (1990, § II) suggested to put all N2 dissimilarity values into an (a, b)-tree data\\nstructure. He claims that this enables hierarchical clustering to be implemented in O(N 2 )\\ntime. Křivánek’s conceptually very simple algorithm relies on the fact that m insertions into\\nan (a, b)-tree can be done in O(m) amortized time. This is only true when the positions,\\nwhere the elements should be inserted into the tree, are known. Searching for these positions\\ntakes O(log N ) time per element, however. (See Mehlhorn and Tsakalidis (1990, § 2.1.2) for\\nan accessible discussion of amortized complexity for (2, 4)-trees; Huddleston and Mehlhorn\\n(1982) introduce and discuss (a, b)-trees in general.) Křivánek did not give any details of his\\n\\n24\\n\\n\\x0canalysis, but based on his short remarks, the author cannot see how Křivánek’s algorithm\\nachieves O(N 2 ) worst-case performance for SAHN clustering.\\n\\n6 Extension to vector data\\nIf the input to a SAHN clustering algorithm is not the array of pairwise dissimilarities but\\nN points in a D-dimensional real vector space, the lower bound Ω(N 2 ) on time complexity\\ndoes not hold any more. Since much of the time in an SAHN clustering scheme is spent on\\nnearest-neighbor searches, algorithms and data structures for fast nearest-neighbor searches\\ncan potentially be useful. The situation is not trivial, however, since (1) in the “combinatorial”\\nmethods (e.g. single, complete, average, weighted linkage) the inter-cluster distances are not\\nsimply defined as distances between special points like cluster centers, and (2) even in the\\n“geometric” methods (the Ward, centroid and median schemes), points are removed and\\nnew centers added with the same frequency as pairs of closest points are searched, so a\\ndynamic nearest-neighbor algorithm is needed, which handles the removal and insertion of\\npoints efficiently.\\nMoreover, all known fast nearest-neighbor algorithms lose their advantage over exhaustive\\nsearch with increasing dimensionality. Additionally, algorithms will likely work for one metric\\nin RD but not universally. Since this paper is concerned with the general situation, we do\\nnot go further into the analysis of the “stored data approach” (Anderberg, 1973, § 6.3). We\\nonly list at this point what can be achieved with the algorithms from this paper. This will\\nlikely be the best solution for high-dimensional data or general-purpose algorithms, but there\\nare better solutions for low-dimensional data outside the scope of this paper. The suggestions\\nbelow are at least helpful to process large data sets since memory requirements are of class\\nΘ(N D), but they do not overcome their Ω(N 2 ) lower bound on time complexity.\\n• The MST algorithm for single linkage can compute distances on-the-fly. Since every\\npairwise dissimilarity is read in only once, there is no performance penalty compared to\\nfirst computing the whole dissimilarity matrix and then applying the MST algorithm.\\nQuite the contrary, computing pairwise distances in-process can result in faster execution since much less memory must be reserved and accessed. The MST algorithm is\\nsuitable for any dissimilarity measure which can be computed from vector representations (that is, all scale types are possible, e.g. R-valued measurements, binary sequences\\nand categorical data).\\n• The NN-chain algorithm is suitable for the “Ward” scheme, since inter-cluster distances\\ncan be defined by means of centroids as in Figure 2. The initial inter-point dissimilarities\\nmust be Euclidean distances (which is anyway the only setting in which Ward linkage\\ndescribes a meaningful procedure).\\n• The generic algorithm is suitable for the “Ward”, “centroid” and “median” scheme on\\nEuclidean data. There is a simpler variant of the Generic_linkage algorithm in Section 6, which works even faster in this setting. The principle of the algorithm Generic_\\nlinkage_variant is the same: each array entry mindist[x] maintains a lower bound on\\nall dissimilarities d[x, y] for nodes with label y > x. The Generic_linkage algorithm\\nis designed to work efficiently with a large array of pairwise dissimilarities. For this purpose, the join of two nodes a and b re-uses the label b, which facilitates in-place updating\\nof the dissimilarity array in an implementation. The Generic_linkage_variant algorithm, in contrast, generates a unique new label for each new node, which is smaller\\nthan all existing labels. Since the new label is at the beginning of the (ordered) list of\\n\\n25\\n\\n\\x0cFigure 8 The generic clustering algorithm (variant).\\n1:\\n\\n5:\\n\\nprocedure Generic_linkage_variant(N, d)\\nd is either an array or a function which computes dissimilarities from cluster centers.\\n..\\n.\\n(Lines 2 to 13 are the same as in Generic_linkage.)\\nfor x in S \\\\ {N − 1} do\\n..\\n.\\n\\n14:\\n\\nwhile b ∈\\n/ S do\\n..\\n.\\n\\n21:\\n22:\\n23:\\n24:\\n25:\\n26:\\n\\nend while\\nRemove a and b from Q.\\nAppend (a, b, δ) to L.\\nCreate a new label n ← −i\\nsize[n] ← size[a] + size[b]\\nS ← (S \\\\ {a, b}) ∪ {n}\\n\\n27:\\n28:\\n29:\\n\\nfor x in S \\\\ {n} do\\nExtend the distance information.\\nd[x, n] ← d[n, x] ← Formula(d[a, x], d[b, x], d[a, b], size[a], size[b], size[x])\\nend for\\n\\nRecalculation of nearest neighbors, if necessary.\\n\\nor\\n27:\\n30:\\n31:\\n32:\\n33:\\n34:\\n\\nCompute the cluster center for n as in Figure 2.\\nn_nghbr[n] ← argminx>n d[n, x]\\nInsert (n, d[n, n_nghbr[n]]) into mindist and Q\\nend for\\nreturn L\\nend procedure\\n\\nnodes and not somewhere in the middle, the bookkeeping of nearest neighbor candidates\\nand minimal distances is simpler in Generic_linkage_variant: in particular, the\\ntwo loops in lines 28–38 of Generic_linkage can be disposed of entirely. Moreover,\\nexperiments show that Generic_linkage_variant needs much less recalculations of\\nnearest neighbors in some data sets. However, both algorithms are similar, and which\\none is faster in an implementation seems to depend strongly on the actual data structures\\nand their memory layout.\\n\\nAnother issue which is not in the focus of this paper is that of parallel algorithms. For the\\n“stored matrix approach”, this has a good reason since the balance of memory requirements\\nversus computational complexity does not make it seem worthwhile to attempt parallelization\\nwith current hardware. This changes for vector data, when the available memory is not the\\nlimiting factor and the run-time is pushed up by bigger data sets. In high-dimensional vector\\nspaces, the advanced clustering algorithms in this paper require little time compared to the\\ncomputation of inter-cluster distances. Hence, parallelizing the nearest-neighbor searches with\\ntheir inherent distance computations appears a fruitful and easy way of sharing the workload.\\nThe situation becomes less clear for low-dimensional data, however.\\n\\n26\\n\\n\\x0c7 Conclusion\\nAmong the algorithms for sequential, agglomerative, hierarchic, nonoverlapping (SAHN)\\nclustering on data with a dissimilarity index, three current algorithms are most efficient:\\nRohlf’s algorithm MST-linkage for single linkage clustering, Murtagh’s algorithm NNchain-linkage for the “complete”, “average”, “weighted” and “Ward” schemes, and the\\nauthor’s Generic_linkage algorithm for the “centroid” and “median” schemes and the\\n“flexible” family. The last algorithm can also be used for an arbitrary distance update formula. There is even a simpler variant Generic_linkage_variant, which seems to require\\nless internal calculations, while the original algorithm is optimized for in-place updating of\\na dissimilarity array as input. The Generic_linkage algorithm and its variant are new;\\nthe other two algorithms were described before, but for the first time they are proved to be\\ncorrect.\\n\\nAcknowledgments\\nThis work was funded by the National Science Foundation grant DMS-0905823 and the Air\\nForce Office of Scientific Research grant FA9550-09-1-0643.\\n\\nReferences\\nNumPy: Scientific computing tools for Python. Available at http://numpy.scipy.org/.\\nMichael R. Anderberg. Cluster analysis for applications. Academic Press, New York, 1973.\\nISBN 0120576503.\\nThomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest, and Clifford Stein. Introduction\\nto Algorithms. MIT Press, 3rd edition, 2009.\\nWilliam H. E. Day. Complexity theory: an introduction for practitioners of classification. In\\nClustering and classification, pages 199–233. World Scientific Publishing, River Edge, NJ,\\n1996.\\nWilliam H. E. Day and Herbert Edelsbrunner.\\nEfficient algorithms for agglomerative hierarchical clustering methods. Journal of Classification, 1(1):7–24, 1984. doi:\\n10.1007/BF01890115.\\nDaniel Defays. An efficient algorithm for a complete link method. The Computer Journal, 20\\n(4):364–366, 1977. doi: 10.1093/comjnl/20.4.364.\\nDamian Eads. Hierarchical clustering (scipy.cluster.hierarchy), 2007. Package for SciPy\\nversion 0.9.0. Available at http://www.scipy.org.\\nBrian S. Everitt, Sabine Landau, Morven Leese, and Daniel Stahl. Cluster Analysis. John\\nWiley & Sons, 5th edition, 2011. doi: 10.1002/9780470977811.\\nAllan D. Gordon. A review of hierarchical classification. Journal of the Royal Statistical\\nSociety. Series A (General), 150(2):119–137, 1987. doi: 10.2307/2981629.\\nJohn C. Gower and G. J. S. Ross. Minimum spanning trees and single linkage cluster analysis.\\nJournal of the Royal Statistical Society. Series C (Applied Statistics), 18(1):54–64, 1969. doi:\\n10.2307/2346439.\\n\\n27\\n\\n\\x0cPierre Hansen and Brigitte Jaumard. Cluster analysis and mathematical programming. Mathematical Programming, 79(1–3):191–215, 1997. doi: 10.1007/BF02614317.\\nScott Huddleston and Kurt Mehlhorn. A new data structure for representing sorted lists.\\nActa Informatica, 17(2):157–184, 1982. doi: 10.1007/BF00288968.\\nAnil K. Jain and Richard C. Dubes. Algorithms for Clustering Data. Prentice Hall, Englewood\\nCliffs, NJ, 1988.\\nStephen C. Johnson. Hierarchical clustering schemes. Psychometrika, 32(3):241–254, 1967.\\ndoi: 10.1007/BF02289588.\\nEric Jones, Travis Oliphant, Pearu Peterson, et al. SciPy: Open source scientific tools for\\nPython, 2001. http://www.scipy.org.\\nLeonard Kaufman and Peter J. Rousseeuw. Finding groups in data: An introduction to cluster\\nanalysis. John Wiley & Sons, New York, 1990. doi: 10.1002/9780470316801.\\nMirko Křivánek. Connected admissible hierarchical clustering. KAM series, (90-189), 1990.\\nDepartment of Applied Mathematics, Charles University, Prague (CZ). Available at http:\\n//kam.mff.cuni.cz/~kamserie/serie/clanky/1990/s189.pdf.\\nG. N. Lance and W. T. Williams. A general theory of classificatory sorting strategies. Computer Journal, 9(4):373–380, 1967. doi: 10.1093/comjnl/9.4.373.\\nKurt Mehlhorn and Athanasios Tsakalidis. Data structures. In Handbook of theoretical computer science, Vol. A, pages 301–341. Elsevier, Amsterdam, 1990. Available at\\nhttp://www.mpi-sb.mpg.de/~mehlhorn/ftp/DataStructures.pdf.\\nFionn Murtagh. A survey of recent advances in hierarchical clustering algorithms. Computer\\nJournal, 26(4):354–359, 1983. doi: 10.1093/comjnl/26.4.354.\\nFionn Murtagh. Complexities of hierarchic clustering algorithms: State of the art. Computational Statistics Quarterly, 1(2):101–113, 1984. Available at http://thames.cs.rhul.ac.\\nuk/~fionn/old-articles/complexities/.\\nFionn Murtagh. Multidimensional clustering algorithms, volume 4 of Compstat Lectures.\\nPhysica-Verlag, Würzburg/ Wien, 1985. ISBN 3-7051-0008-4. Available at http://www.\\nclassification-society.org/csna/mda-sw/.\\nDaniel Müllner. fastcluster: Fast hierarchical, agglomerative clustering routines for R and\\nPython. Preprint, 2011. Will be available at http://math.stanford.edu/~muellner.\\nR Development Core Team. R: A Language and Environment for Statistical Computing. R\\nFoundation for Statistical Computing, Vienna, Austria, 2011. http://www.R-project.org.\\nF. James Rohlf. Hierarchical clustering using the minimum spanning tree. Comput. Journal,\\n16:93–95, 1973. Available at http://life.bio.sunysb.edu/ee/rohlf/reprints.html.\\nF. James Rohlf. Single-link clustering algorithms. In P.R. Krishnaiah and L.N. Kanal, editors,\\nClassification Pattern Recognition and Reduction of Dimensionality, volume 2 of Handbook\\nof Statistics, pages 267–284. Elsevier, 1982. doi: 10.1016/S0169-7161(82)02015-X.\\nR. Sibson. SLINK: an optimally efficient algorithm for the single-link cluster method. Comput.\\nJournal, 16:30–34, 1973. doi: 10.1093/comjnl/16.1.30.\\n\\n28\\n\\n\\x0cPeter H. A. Sneath and Robert R. Sokal. Numerical taxonomy. W. H. Freeman, San Francisco,\\n1973.\\nThe MathWorks, Inc. MATLAB, 2011. http://www.mathworks.com.\\nGuido van Rossum et al. Python programming language. Available at http://www.python.\\norg.\\nWolfram Research, Inc. Mathematica, 2010. http://www.wolfram.com.\\n\\nDaniel Müllner\\nStanford University\\nDepartment of Mathematics\\n450 Serra Mall, Building 380\\nStanford, CA 94305\\nE-mail: muellner@math.stanford.edu\\nhttp://math.stanford.edu/~muellner\\n\\n29\\n\\n\\x0c',\n",
       " 'title': 'Modern hierarchical, agglomerative clustering algorithms'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "def pdftotext(pdf):\n",
    "    \"\"\"Convert a pdf to a text file. Extract the Author and Title \n",
    "    and return a dictionary consisting of the author, title, text\n",
    "    the source path, the path of the converted text file and the \n",
    "    file ID.\"\"\"\n",
    "    basename, _ = os.path.splitext(os.path.basename(pdf))\n",
    "    subprocess.call(['pdftotext', '-enc', 'UTF-8', '-htmlmeta',\n",
    "                     pdf, os.path.join('data', basename + '.html')])\n",
    "    data = parse_html(os.path.join('data', basename + '.html'))\n",
    "    with open(os.path.join('data', basename + '.txt'), 'w') as outfile:\n",
    "        outfile.write(data['text'])\n",
    "    # insert your code here\n",
    "    os.remove(os.path.join('data', basename + '.html'))\n",
    "    if not os.path.exists('static/pdfs'):\n",
    "        os.mkdir('static/pdfs')\n",
    "    shutil.copy(pdf, os.path.join('static/pdfs', basename + '.pdf'))\n",
    "    data['source'] = os.path.join('static/pdfs', basename + '.pdf')\n",
    "    data['target'] = os.path.join('data', basename + '.txt')\n",
    "    data['id'] = basename\n",
    "    return data\n",
    "\n",
    "\n",
    "pdftotext(\"pdfs/muellner2011.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c)** After we have extracted the meta information from the HTML file, we no longer need it. The `remove` function in the model `os` can be used to remove files from your computer. Add a line of code to the `pdftotext` function that removes the HTML file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**d)** We need to store the PDF files in the directory `pydf/static/pdfs`. The function `copy` from the `shutil` module allows you to copy or move files from one directory to the other. Add some lines of code to the `pdftotext` function in which you copy the original PDF file to the directory `pydf/static/pdfs`. Make sure that this directory exists. Otherwise, first create it using the function `mkdir` from the `os` module. You can use the function `exists` from the `os.path` module to check whether a particular file or directory exists."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the two hardest points off our list, we can move on to the next two. We need a way to make Python aware of the directories we would like to index. There are many different ways to accomplish this. I choose to make a configuration file in which we store the paths of the directories we want to index (and possibly some other information). The Python module [configparser](https://docs.python.org/3.4/library/configparser.html) provides a class `ConfigParser` with which we can parse configuration files in a format similar to Microsoft Windows INI files:\n",
    "\n",
    "    [filepaths]\n",
    "    # pdf directory represents the directory or directories\n",
    "    # that you would like to index. Separate multiple directories\n",
    "    # by a semicolon.\n",
    "    pdf directory = pdfs\n",
    "    txt directory = data\n",
    "    index directory = pdf-index\n",
    "    source directory = static/pdfs\n",
    "\n",
    "    [programpaths]\n",
    "    pdftotext = /usr/local/bin/pdftotext\n",
    "\n",
    "    [indexer.options]\n",
    "    recompile = no\n",
    "    move pdfs = no\n",
    "    search limit = 20\n",
    "    \n",
    "Copy these lines to a file named `pydf.ini` in the `pydf` directory and adapt the paths to your own. We will read the contents the configuration file using the class `ConfigParser`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['filepaths', 'programpaths', 'indexer.options']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import configparser\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read('pydf.ini')\n",
    "config.sections()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It returns a strucure that functions much like a dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pdfs'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config['filepaths']['pdf directory']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a configuration file, let's adjust the function `pdftotext` to make it a little more general. In the current version we hard-coded the path to the output directory as well as the path to the `pdftotext` binary. We move those elements to the function declaration to make them variable arguments of the function. While we're at it, let's also remove some other code redundancies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'blei2003',\n",
       " 'path': 'data/blei2003.txt',\n",
       " 'source': 'static/pdfs/blei2003.pdf',\n",
       " 'text': '\\nJournal of Machine Learning Research 3 (2003) 993-1022\\n\\nSubmitted 2/02; Published 1/03\\n\\nLatent Dirichlet Allocation\\nDavid M. Blei\\n\\nBLEI @ CS . BERKELEY. EDU\\n\\nComputer Science Division\\nUniversity of California\\nBerkeley, CA 94720, USA\\n\\nAndrew Y. Ng\\n\\nANG @ CS . STANFORD . EDU\\n\\nComputer Science Department\\nStanford University\\nStanford, CA 94305, USA\\n\\nMichael I. Jordan\\n\\nJORDAN @ CS . BERKELEY. EDU\\n\\nComputer Science Division and Department of Statistics\\nUniversity of California\\nBerkeley, CA 94720, USA\\n\\nEditor: John Lafferty\\n\\nAbstract\\nWe describe latent Dirichlet allocation (LDA), a generative probabilistic model for collections of\\ndiscrete data such as text corpora. LDA is a three-level hierarchical Bayesian model, in which each\\nitem of a collection is modeled as a finite mixture over an underlying set of topics. Each topic is, in\\nturn, modeled as an infinite mixture over an underlying set of topic probabilities. In the context of\\ntext modeling, the topic probabilities provide an explicit representation of a document. We present\\nefficient approximate inference techniques based on variational methods and an EM algorithm for\\nempirical Bayes parameter estimation. We report results in document modeling, text classification,\\nand collaborative filtering, comparing to a mixture of unigrams model and the probabilistic LSI\\nmodel.\\n\\n1. Introduction\\nIn this paper we consider the problem of modeling text corpora and other collections of discrete\\ndata. The goal is to find short descriptions of the members of a collection that enable efficient\\nprocessing of large collections while preserving the essential statistical relationships that are useful\\nfor basic tasks such as classification, novelty detection, summarization, and similarity and relevance\\njudgments.\\nSignificant progress has been made on this problem by researchers in the field of information retrieval (IR) (Baeza-Yates and Ribeiro-Neto, 1999). The basic methodology proposed by\\nIR researchers for text corpora—a methodology successfully deployed in modern Internet search\\nengines—reduces each document in the corpus to a vector of real numbers, each of which represents ratios of counts. In the popular tf-idf scheme (Salton and McGill, 1983), a basic vocabulary\\nof “words” or “terms” is chosen, and, for each document in the corpus, a count is formed of the\\nnumber of occurrences of each word. After suitable normalization, this term frequency count is\\ncompared to an inverse document frequency count, which measures the number of occurrences of a\\nc 2003 David M. Blei, Andrew Y. Ng and Michael I. Jordan.\\n\\n\\x0cB LEI , N G , AND J ORDAN\\n\\nword in the entire corpus (generally on a log scale, and again suitably normalized). The end result\\nis a term-by-document matrix X whose columns contain the tf-idf values for each of the documents\\nin the corpus. Thus the tf-idf scheme reduces documents of arbitrary length to fixed-length lists of\\nnumbers.\\nWhile the tf-idf reduction has some appealing features—notably in its basic identification of sets\\nof words that are discriminative for documents in the collection—the approach also provides a relatively small amount of reduction in description length and reveals little in the way of inter- or intradocument statistical structure. To address these shortcomings, IR researchers have proposed several\\nother dimensionality reduction techniques, most notably latent semantic indexing (LSI) (Deerwester\\net al., 1990). LSI uses a singular value decomposition of the X matrix to identify a linear subspace\\nin the space of tf-idf features that captures most of the variance in the collection. This approach can\\nachieve significant compression in large collections. Furthermore, Deerwester et al. argue that the\\nderived features of LSI, which are linear combinations of the original tf-idf features, can capture\\nsome aspects of basic linguistic notions such as synonymy and polysemy.\\nTo substantiate the claims regarding LSI, and to study its relative strengths and weaknesses, it is\\nuseful to develop a generative probabilistic model of text corpora and to study the ability of LSI to\\nrecover aspects of the generative model from data (Papadimitriou et al., 1998). Given a generative\\nmodel of text, however, it is not clear why one should adopt the LSI methodology—one can attempt\\nto proceed more directly, fitting the model to data using maximum likelihood or Bayesian methods.\\nA significant step forward in this regard was made by Hofmann (1999), who presented the\\nprobabilistic LSI (pLSI) model, also known as the aspect model, as an alternative to LSI. The pLSI\\napproach, which we describe in detail in Section 4.3, models each word in a document as a sample\\nfrom a mixture model, where the mixture components are multinomial random variables that can be\\nviewed as representations of “topics.” Thus each word is generated from a single topic, and different\\nwords in a document may be generated from different topics. Each document is represented as\\na list of mixing proportions for these mixture components and thereby reduced to a probability\\ndistribution on a fixed set of topics. This distribution is the “reduced description” associated with\\nthe document.\\nWhile Hofmann’s work is a useful step toward probabilistic modeling of text, it is incomplete\\nin that it provides no probabilistic model at the level of documents. In pLSI, each document is\\nrepresented as a list of numbers (the mixing proportions for topics), and there is no generative\\nprobabilistic model for these numbers. This leads to several problems: (1) the number of parameters in the model grows linearly with the size of the corpus, which leads to serious problems with\\noverfitting, and (2) it is not clear how to assign probability to a document outside of the training set.\\nTo see how to proceed beyond pLSI, let us consider the fundamental probabilistic assumptions\\nunderlying the class of dimensionality reduction methods that includes LSI and pLSI. All of these\\nmethods are based on the “bag-of-words” assumption—that the order of words in a document can\\nbe neglected. In the language of probability theory, this is an assumption of exchangeability for the\\nwords in a document (Aldous, 1985). Moreover, although less often stated formally, these methods\\nalso assume that documents are exchangeable; the specific ordering of the documents in a corpus\\ncan also be neglected.\\nA classic representation theorem due to de Finetti (1990) establishes that any collection of exchangeable random variables has a representation as a mixture distribution—in general an infinite\\nmixture. Thus, if we wish to consider exchangeable representations for documents and words, we\\nneed to consider mixture models that capture the exchangeability of both words and documents.\\n994\\n\\n\\x0cL ATENT D IRICHLET A LLOCATION\\n\\nThis line of thinking leads to the latent Dirichlet allocation (LDA) model that we present in the\\ncurrent paper.\\nIt is important to emphasize that an assumption of exchangeability is not equivalent to an assumption that the random variables are independent and identically distributed. Rather, exchangeability essentially can be interpreted as meaning “conditionally independent and identically distributed,” where the conditioning is with respect to an underlying latent parameter of a probability\\ndistribution. Conditionally, the joint distribution of the random variables is simple and factored\\nwhile marginally over the latent parameter, the joint distribution can be quite complex. Thus, while\\nan assumption of exchangeability is clearly a major simplifying assumption in the domain of text\\nmodeling, and its principal justification is that it leads to methods that are computationally efficient,\\nthe exchangeability assumptions do not necessarily lead to methods that are restricted to simple\\nfrequency counts or linear operations. We aim to demonstrate in the current paper that, by taking\\nthe de Finetti theorem seriously, we can capture significant intra-document statistical structure via\\nthe mixing distribution.\\nIt is also worth noting that there are a large number of generalizations of the basic notion of\\nexchangeability, including various forms of partial exchangeability, and that representation theorems are available for these cases as well (Diaconis, 1988). Thus, while the work that we discuss in\\nthe current paper focuses on simple “bag-of-words” models, which lead to mixture distributions for\\nsingle words (unigrams), our methods are also applicable to richer models that involve mixtures for\\nlarger structural units such as n-grams or paragraphs.\\nThe paper is organized as follows. In Section 2 we introduce basic notation and terminology.\\nThe LDA model is presented in Section 3 and is compared to related latent variable models in\\nSection 4. We discuss inference and parameter estimation for LDA in Section 5. An illustrative\\nexample of fitting LDA to data is provided in Section 6. Empirical results in text modeling, text\\nclassification and collaborative filtering are presented in Section 7. Finally, Section 8 presents our\\nconclusions.\\n\\n2. Notation and terminology\\nWe use the language of text collections throughout the paper, referring to entities such as “words,”\\n“documents,” and “corpora.” This is useful in that it helps to guide intuition, particularly when\\nwe introduce latent variables which aim to capture abstract notions such as topics. It is important\\nto note, however, that the LDA model is not necessarily tied to text, and has applications to other\\nproblems involving collections of data, including data from domains such as collaborative filtering,\\ncontent-based image retrieval and bioinformatics. Indeed, in Section 7.3, we present experimental\\nresults in the collaborative filtering domain.\\nFormally, we define the following terms:\\n• A word is the basic unit of discrete data, defined to be an item from a vocabulary indexed by\\n{1, . . . ,V }. We represent words using unit-basis vectors that have a single component equal to\\none and all other components equal to zero. Thus, using superscripts to denote components,\\nthe vth word in the vocabulary is represented by a V -vector w such that wv = 1 and wu = 0 for\\nu = v.\\n• A document is a sequence of N words denoted by w = (w1 , w2 , . . . , wN ), where wn is the nth\\nword in the sequence.\\n• A corpus is a collection of M documents denoted by D = {w1 , w2 , . . . , wM }.\\n995\\n\\n\\x0cB LEI , N G , AND J ORDAN\\n\\nWe wish to find a probabilistic model of a corpus that not only assigns high probability to\\nmembers of the corpus, but also assigns high probability to other “similar” documents.\\n\\n3. Latent Dirichlet allocation\\nLatent Dirichlet allocation (LDA) is a generative probabilistic model of a corpus. The basic idea is\\nthat documents are represented as random mixtures over latent topics, where each topic is characterized by a distribution over words.1\\nLDA assumes the following generative process for each document w in a corpus D :\\n1. Choose N ∼ Poisson(ξ).\\n2. Choose θ ∼ Dir(α).\\n3. For each of the N words wn :\\n(a) Choose a topic zn ∼ Multinomial(θ).\\n(b) Choose a word wn from p(wn | zn , β), a multinomial probability conditioned on the topic\\nzn .\\nSeveral simplifying assumptions are made in this basic model, some of which we remove in subsequent sections. First, the dimensionality k of the Dirichlet distribution (and thus the dimensionality\\nof the topic variable z) is assumed known and fixed. Second, the word probabilities are parameterized by a k ×V matrix β where βi j = p(w j = 1 | zi = 1), which for now we treat as a fixed quantity\\nthat is to be estimated. Finally, the Poisson assumption is not critical to anything that follows and\\nmore realistic document length distributions can be used as needed. Furthermore, note that N is\\nindependent of all the other data generating variables (θ and z). It is thus an ancillary variable and\\nwe will generally ignore its randomness in the subsequent development.\\nA k-dimensional Dirichlet random variable θ can take values in the (k − 1)-simplex (a k-vector\\nθ lies in the (k − 1)-simplex if θi ≥ 0, ∑ki=1 θi = 1), and has the following probability density on this\\nsimplex:\\nΓ ∑ki=1 αi α1 −1\\n· · · θkαk −1 ,\\nθ1\\np(θ | α) = k\\n∏i=1 Γ(αi )\\n\\n(1)\\n\\nwhere the parameter α is a k-vector with components αi > 0, and where Γ(x) is the Gamma function.\\nThe Dirichlet is a convenient distribution on the simplex — it is in the exponential family, has finite\\ndimensional sufficient statistics, and is conjugate to the multinomial distribution. In Section 5, these\\nproperties will facilitate the development of inference and parameter estimation algorithms for LDA.\\nGiven the parameters α and β, the joint distribution of a topic mixture θ, a set of N topics z, and\\na set of N words w is given by:\\nN\\n\\np(θ, z, w | α, β) = p(θ | α) ∏ p(zn | θ)p(wn | zn , β),\\n\\n(2)\\n\\nn=1\\n\\n1. We refer to the latent multinomial variables in the LDA model as topics, so as to exploit text-oriented intuitions, but\\nwe make no epistemological claims regarding these latent variables beyond their utility in representing probability\\ndistributions on sets of words.\\n\\n996\\n\\n\\x0cL ATENT D IRICHLET A LLOCATION\\n\\nβ\\n\\nα\\n\\nθ\\n\\nz\\n\\nw\\n\\nN\\n\\nM\\n\\nFigure 1: Graphical model representation of LDA. The boxes are “plates” representing replicates.\\nThe outer plate represents documents, while the inner plate represents the repeated choice\\nof topics and words within a document.\\n\\nwhere p(zn | θ) is simply θi for the unique i such that zin = 1. Integrating over θ and summing over\\nz, we obtain the marginal distribution of a document:\\np(w | α, β) =\\n\\np(θ | α)\\n\\nN\\n\\n∏ ∑ p(zn | θ)p(wn | zn , β)\\n\\ndθ.\\n\\n(3)\\n\\nn=1 zn\\n\\nFinally, taking the product of the marginal probabilities of single documents, we obtain the probability of a corpus:\\nM\\n\\np(D | α, β) = ∏\\n\\nd=1\\n\\np(θd | α)\\n\\nNd\\n\\n∏ ∑ p(zdn | θd )p(wdn | zdn , β)\\n\\ndθd .\\n\\nn=1 zdn\\n\\nThe LDA model is represented as a probabilistic graphical model in Figure 1. As the figure\\nmakes clear, there are three levels to the LDA representation. The parameters α and β are corpuslevel parameters, assumed to be sampled once in the process of generating a corpus. The variables\\nθd are document-level variables, sampled once per document. Finally, the variables zdn and wdn are\\nword-level variables and are sampled once for each word in each document.\\nIt is important to distinguish LDA from a simple Dirichlet-multinomial clustering model. A\\nclassical clustering model would involve a two-level model in which a Dirichlet is sampled once\\nfor a corpus, a multinomial clustering variable is selected once for each document in the corpus,\\nand a set of words are selected for the document conditional on the cluster variable. As with many\\nclustering models, such a model restricts a document to being associated with a single topic. LDA,\\non the other hand, involves three levels, and notably the topic node is sampled repeatedly within the\\ndocument. Under this model, documents can be associated with multiple topics.\\nStructures similar to that shown in Figure 1 are often studied in Bayesian statistical modeling,\\nwhere they are referred to as hierarchical models (Gelman et al., 1995), or more precisely as conditionally independent hierarchical models (Kass and Steffey, 1989). Such models are also often\\nreferred to as parametric empirical Bayes models, a term that refers not only to a particular model\\nstructure, but also to the methods used for estimating parameters in the model (Morris, 1983). Indeed, as we discuss in Section 5, we adopt the empirical Bayes approach to estimating parameters\\nsuch as α and β in simple implementations of LDA, but we also consider fuller Bayesian approaches\\nas well.\\n997\\n\\n\\x0cB LEI , N G , AND J ORDAN\\n\\n3.1 LDA and exchangeability\\nA finite set of random variables {z1 , . . . , zN } is said to be exchangeable if the joint distribution is\\ninvariant to permutation. If π is a permutation of the integers from 1 to N:\\np(z1 , . . . , zN ) = p(zπ(1) , . . . , zπ(N) ).\\nAn infinite sequence of random variables is infinitely exchangeable if every finite subsequence is\\nexchangeable.\\nDe Finetti’s representation theorem states that the joint distribution of an infinitely exchangeable\\nsequence of random variables is as if a random parameter were drawn from some distribution and\\nthen the random variables in question were independent and identically distributed, conditioned on\\nthat parameter.\\nIn LDA, we assume that words are generated by topics (by fixed conditional distributions) and\\nthat those topics are infinitely exchangeable within a document. By de Finetti’s theorem, the probability of a sequence of words and topics must therefore have the form:\\np(w, z) =\\n\\nN\\n\\np(θ)\\n\\n∏ p(zn | θ)p(wn | zn )\\n\\ndθ,\\n\\nn=1\\n\\nwhere θ is the random parameter of a multinomial over topics. We obtain the LDA distribution\\non documents in Eq. (3) by marginalizing out the topic variables and endowing θ with a Dirichlet\\ndistribution.\\n3.2 A continuous mixture of unigrams\\nThe LDA model shown in Figure 1 is somewhat more elaborate than the two-level models often\\nstudied in the classical hierarchical Bayesian literature. By marginalizing over the hidden topic\\nvariable z, however, we can understand LDA as a two-level model.\\nIn particular, let us form the word distribution p(w | θ, β):\\np(w | θ, β) = ∑ p(w | z, β)p(z | θ).\\nz\\n\\nNote that this is a random quantity since it depends on θ.\\nWe now define the following generative process for a document w:\\n1. Choose θ ∼ Dir(α).\\n2. For each of the N words wn :\\n(a) Choose a word wn from p(wn | θ, β).\\nThis process defines the marginal distribution of a document as a continuous mixture distribution:\\np(w | α, β) =\\n\\np(θ | α)\\n\\nN\\n\\n∏ p(wn | θ, β)\\n\\ndθ,\\n\\nn=1\\n\\nwhere p(wn | θ, β) are the mixture components and p(θ | α) are the mixture weights.\\nFigure 2 illustrates this interpretation of LDA. It depicts the distribution on p(w | θ, β) which is\\ninduced from a particular instance of an LDA model. Note that this distribution on the (V − 1)simplex is attained with only k + kV parameters yet exhibits a very interesting multimodal structure.\\n998\\n\\n\\x0cL ATENT D IRICHLET A LLOCATION\\n\\nFigure 2: An example density on unigram distributions p(w | θ, β) under LDA for three words and\\nfour topics. The triangle embedded in the x-y plane is the 2-D simplex representing all\\npossible multinomial distributions over three words. Each of the vertices of the triangle corresponds to a deterministic distribution that assigns probability one to one of the\\nwords; the midpoint of an edge gives probability 0.5 to two of the words; and the centroid\\nof the triangle is the uniform distribution over all three words. The four points marked\\nwith an x are the locations of the multinomial distributions p(w | z) for each of the four\\ntopics, and the surface shown on top of the simplex is an example of a density over the\\n(V − 1)-simplex (multinomial distributions of words) given by LDA.\\n\\n4. Relationship with other latent variable models\\nIn this section we compare LDA to simpler latent variable models for text—the unigram model, a\\nmixture of unigrams, and the pLSI model. Furthermore, we present a unified geometric interpretation of these models which highlights their key differences and similarities.\\n4.1 Unigram model\\nUnder the unigram model, the words of every document are drawn independently from a single\\nmultinomial distribution:\\nN\\n\\np(w) = ∏ p(wn ).\\nn=1\\n\\nThis is illustrated in the graphical model in Figure 3a.\\n999\\n\\n\\x0cB LEI , N G , AND J ORDAN\\n\\nw\\n\\nN\\n\\nM\\n\\n(a) unigram\\n\\nz\\n\\nw\\n\\nN\\n\\nM\\n\\n(b) mixture of unigrams\\n\\nz\\n\\nd\\n\\nw\\n\\nN\\n\\nM\\n\\n(c) pLSI/aspect model\\nFigure 3: Graphical model representation of different models of discrete data.\\n\\n4.2 Mixture of unigrams\\nIf we augment the unigram model with a discrete random topic variable z (Figure 3b), we obtain a\\nmixture of unigrams model (Nigam et al., 2000). Under this mixture model, each document is generated by first choosing a topic z and then generating N words independently from the conditional\\nmultinomial p(w | z). The probability of a document is:\\nN\\n\\np(w) = ∑ p(z) ∏ p(wn | z).\\nz\\n\\nn=1\\n\\nWhen estimated from a corpus, the word distributions can be viewed as representations of topics\\nunder the assumption that each document exhibits exactly one topic. As the empirical results in\\nSection 7 illustrate, this assumption is often too limiting to effectively model a large collection of\\ndocuments.\\nIn contrast, the LDA model allows documents to exhibit multiple topics to different degrees.\\nThis is achieved at a cost of just one additional parameter: there are k − 1 parameters associated\\nwith p(z) in the mixture of unigrams, versus the k parameters associated with p(θ | α) in LDA.\\n4.3 Probabilistic latent semantic indexing\\nProbabilistic latent semantic indexing (pLSI) is another widely used document model (Hofmann,\\n1999). The pLSI model, illustrated in Figure 3c, posits that a document label d and a word wn are\\n1000\\n\\n\\x0cL ATENT D IRICHLET A LLOCATION\\n\\nconditionally independent given an unobserved topic z:\\np(d, wn ) = p(d) ∑ p(wn | z)p(z | d).\\nz\\n\\nThe pLSI model attempts to relax the simplifying assumption made in the mixture of unigrams\\nmodel that each document is generated from only one topic. In a sense, it does capture the possibility\\nthat a document may contain multiple topics since p(z | d) serves as the mixture weights of the topics\\nfor a particular document d. However, it is important to note that d is a dummy index into the list\\nof documents in the training set. Thus, d is a multinomial random variable with as many possible\\nvalues as there are training documents and the model learns the topic mixtures p(z | d) only for those\\ndocuments on which it is trained. For this reason, pLSI is not a well-defined generative model of\\ndocuments; there is no natural way to use it to assign probability to a previously unseen document.\\nA further difficulty with pLSI, which also stems from the use of a distribution indexed by training documents, is that the number of parameters which must be estimated grows linearly with the\\nnumber of training documents. The parameters for a k-topic pLSI model are k multinomial distributions of size V and M mixtures over the k hidden topics. This gives kV + kM parameters and\\ntherefore linear growth in M. The linear growth in parameters suggests that the model is prone\\nto overfitting and, empirically, overfitting is indeed a serious problem (see Section 7.1). In practice, a tempering heuristic is used to smooth the parameters of the model for acceptable predictive performance. It has been shown, however, that overfitting can occur even when tempering is\\nused (Popescul et al., 2001).\\nLDA overcomes both of these problems by treating the topic mixture weights as a k-parameter\\nhidden random variable rather than a large set of individual parameters which are explicitly linked to\\nthe training set. As described in Section 3, LDA is a well-defined generative model and generalizes\\neasily to new documents. Furthermore, the k + kV parameters in a k-topic LDA model do not grow\\nwith the size of the training corpus. We will see in Section 7.1 that LDA does not suffer from the\\nsame overfitting issues as pLSI.\\n4.4 A geometric interpretation\\nA good way of illustrating the differences between LDA and the other latent topic models is by\\nconsidering the geometry of the latent space, and seeing how a document is represented in that\\ngeometry under each model.\\nAll four of the models described above—unigram, mixture of unigrams, pLSI, and LDA—\\noperate in the space of distributions over words. Each such distribution can be viewed as a point on\\nthe (V − 1)-simplex, which we call the word simplex.\\nThe unigram model finds a single point on the word simplex and posits that all words in the\\ncorpus come from the corresponding distribution. The latent variable models consider k points on\\nthe word simplex and form a sub-simplex based on those points, which we call the topic simplex.\\nNote that any point on the topic simplex is also a point on the word simplex. The different latent\\nvariable models use the topic simplex in different ways to generate a document.\\n• The mixture of unigrams model posits that for each document, one of the k points on the word\\nsimplex (that is, one of the corners of the topic simplex) is chosen randomly and all the words\\nof the document are drawn from the distribution corresponding to that point.\\n1001\\n\\n\\x0cB LEI , N G , AND J ORDAN\\n\\n000000000000000000000\\n111111111111111111111\\n000000000000000000000\\n111111111111111111111\\n000000000000000000000\\n111111111111111111111\\n11\\n00\\ntopic 1\\n000000000000000000000\\n111111111111111111111\\n00\\n11\\n000000000000000000000\\n111111111111111111111\\n00\\n11\\n000000000000000000000\\n111111111111111111111\\n000000000000000000000\\n111111111111111111111\\n000000000000000000000\\n111111111111111111111\\n000000000000000000000\\n111111111111111111111\\n000000000000000000000\\n111111111111111111111\\n000000000000000000000\\n111111111111111111111\\ntopic simplex\\n000000000000000000000\\n111111111111111111111\\n000000000000000000000\\nx 111111111111111111111\\n000000000000000000000\\n111111111111111111111\\n000000000000000000000\\n111111111111111111111\\n000000000000000000000\\n111111111111111111111\\n000000000000000000000\\n111111111111111111111\\n000000000000000000000\\n111111111111111111111\\n000000000000000000000\\n111111111111111111111\\n000000000000000000000\\n111111111111111111111\\nx\\nx\\n000000000000000000000\\n111111111111111111111\\nx\\n000000000000000000000\\n111111111111111111111\\n000000000000000000000\\n111111111111111111111\\n000000000000000000000\\n111111111111111111111\\n000000000000000000000\\n111111111111111111111\\nx\\n000000000000000000000\\n111111111111111111111\\nword simplex\\nx\\n000000000000000000000\\n111111111111111111111\\nx\\n000000000000000000000\\n111111111111111111111\\nx\\n000000000000000000000\\n111111111111111111111\\n000000000000000000000\\n111111111111111111111\\nx\\n000000000000000000000\\n111111111111111111111\\nx 111111111111111111111\\n000000000000000000000\\n000000000000000000000\\n111111111111111111111\\nx\\nx\\n000000000000000000000\\n111111111111111111111\\nx\\n000000000000000000000\\n111111111111111111111\\n000000000000000000000\\n111111111111111111111\\nx\\n000000000000000000000\\n111111111111111111111\\nx\\n000000000000000000000\\n111111111111111111111\\n000000000000000000000\\n111111111111111111111\\nx\\n000000000000000000000\\nx 111111111111111111111\\nx\\n000000000000000000000\\n111111111111111111111\\nx\\n000000000000000000000\\n111111111111111111111\\nx\\n000000000000000000000\\n111111111111111111111\\n000000000000000000000\\n111111111111111111111\\nx\\n000000000000000000000\\n111111111111111111111\\n000000000000000000000\\n111111111111111111111\\nx\\nx\\n000000000000000000000\\n111111111111111111111\\n000000000000000000000\\n111111111111111111111\\nx\\nx111111111111111111111\\nx\\n000000000000000000000\\nx\\n000000000000000000000\\n111111111111111111111\\ntopic 2\\n000000000000000000000\\n111111111111111111111\\n000000000000000000000\\n111111111111111111111\\n000000000000000000000\\n111111111111111111111\\n00\\n11\\n0\\n1\\n00000000000000000000000000000\\n11111111111111111111111111111\\n00\\n11\\n0\\n1\\n00000000000000000000000000000\\n11111111111111111111111111111\\n00000000000000000000000000000\\n11111111111111111111111111111\\n00000000000000000000000000000\\n11111111111111111111111111111\\ntopic\\n00000000000000000000000000000 3\\n11111111111111111111111111111\\n00000000000000000000000000000\\n11111111111111111111111111111\\n00000000000000000000000000000\\n11111111111111111111111111111\\n00000000000000000000000000000\\n11111111111111111111111111111\\n00000000000000000000000000000\\n11111111111111111111111111111\\n00000000000000000000000000000\\n11111111111111111111111111111\\n00000000000000000000000000000\\n11111111111111111111111111111\\n\\nFigure 4: The topic simplex for three topics embedded in the word simplex for three words. The\\ncorners of the word simplex correspond to the three distributions where each word (respectively) has probability one. The three points of the topic simplex correspond to three\\ndifferent distributions over words. The mixture of unigrams places each document at one\\nof the corners of the topic simplex. The pLSI model induces an empirical distribution on\\nthe topic simplex denoted by x. LDA places a smooth distribution on the topic simplex\\ndenoted by the contour lines.\\n\\n• The pLSI model posits that each word of a training document comes from a randomly chosen\\ntopic. The topics are themselves drawn from a document-specific distribution over topics,\\ni.e., a point on the topic simplex. There is one such distribution for each document; the set of\\ntraining documents thus defines an empirical distribution on the topic simplex.\\n• LDA posits that each word of both the observed and unseen documents is generated by a\\nrandomly chosen topic which is drawn from a distribution with a randomly chosen parameter.\\nThis parameter is sampled once per document from a smooth distribution on the topic simplex.\\nThese differences are highlighted in Figure 4.\\n\\n5. Inference and Parameter Estimation\\nWe have described the motivation behind LDA and illustrated its conceptual advantages over other\\nlatent topic models. In this section, we turn our attention to procedures for inference and parameter\\nestimation under LDA.\\n1002\\n\\n\\x0cL ATENT D IRICHLET A LLOCATION\\n\\nβ\\n\\nα\\n\\nθ\\n\\nγ\\n\\nz\\n\\nw\\n\\nN\\n\\nφ\\n\\nθ\\n\\nM\\n\\nz\\n\\nN\\n\\nM\\n\\nFigure 5: (Left) Graphical model representation of LDA. (Right) Graphical model representation\\nof the variational distribution used to approximate the posterior in LDA.\\n\\n5.1 Inference\\nThe key inferential problem that we need to solve in order to use LDA is that of computing the\\nposterior distribution of the hidden variables given a document:\\np(θ, z | w, α, β) =\\n\\np(θ, z, w | α, β)\\n.\\np(w | α, β)\\n\\nUnfortunately, this distribution is intractable to compute in general. Indeed, to normalize the distribution we marginalize over the hidden variables and write Eq. (3) in terms of the model parameters:\\np(w | α, β) =\\n\\nΓ (∑i αi )\\n∏i Γ(αi )\\n\\nk\\n\\n∏ θαi i −1\\ni=1\\n\\nN\\n\\nk\\n\\nV\\n\\n∏ ∑ ∏(θi βi j )w\\n\\nj\\nn\\n\\ndθ,\\n\\nn=1 i=1 j=1\\n\\na function which is intractable due to the coupling between θ and β in the summation over latent\\ntopics (Dickey, 1983). Dickey shows that this function is an expectation under a particular extension\\nto the Dirichlet distribution which can be represented with special hypergeometric functions. It has\\nbeen used in a Bayesian context for censored discrete data to represent the posterior on θ which, in\\nthat setting, is a random parameter (Dickey et al., 1987).\\nAlthough the posterior distribution is intractable for exact inference, a wide variety of approximate inference algorithms can be considered for LDA, including Laplace approximation, variational\\napproximation, and Markov chain Monte Carlo (Jordan, 1999). In this section we describe a simple\\nconvexity-based variational algorithm for inference in LDA, and discuss some of the alternatives in\\nSection 8.\\n5.2 Variational inference\\nThe basic idea of convexity-based variational inference is to make use of Jensen’s inequality to obtain an adjustable lower bound on the log likelihood (Jordan et al., 1999). Essentially, one considers\\na family of lower bounds, indexed by a set of variational parameters. The variational parameters\\nare chosen by an optimization procedure that attempts to find the tightest possible lower bound.\\nA simple way to obtain a tractable family of lower bounds is to consider simple modifications\\nof the original graphical model in which some of the edges and nodes are removed. Consider in\\nparticular the LDA model shown in Figure 5 (left). The problematic coupling between θ and β\\n1003\\n\\n\\x0cB LEI , N G , AND J ORDAN\\n\\narises due to the edges between θ, z, and w. By dropping these edges and the w nodes, and endowing the resulting simplified graphical model with free variational parameters, we obtain a family\\nof distributions on the latent variables. This family is characterized by the following variational\\ndistribution:\\nN\\n\\nq(θ, z | γ, φ) = q(θ | γ) ∏ q(zn | φn ),\\n\\n(4)\\n\\nn=1\\n\\nwhere the Dirichlet parameter γ and the multinomial parameters (φ1 , . . . , φN ) are the free variational\\nparameters.\\nHaving specified a simplified family of probability distributions, the next step is to set up an\\noptimization problem that determines the values of the variational parameters γ and φ. As we show\\nin Appendix A, the desideratum of finding a tight lower bound on the log likelihood translates\\ndirectly into the following optimization problem:\\n(γ∗ , φ∗ ) = arg min D(q(θ, z | γ, φ)\\n(γ,φ)\\n\\np(θ, z | w, α, β)).\\n\\n(5)\\n\\nThus the optimizing values of the variational parameters are found by minimizing the KullbackLeibler (KL) divergence between the variational distribution and the true posterior p(θ, z | w, α, β).\\nThis minimization can be achieved via an iterative fixed-point method. In particular, we show in\\nAppendix A.3 that by computing the derivatives of the KL divergence and setting them equal to\\nzero, we obtain the following pair of update equations:\\nφni ∝ βiwn exp{Eq [log(θi ) | γ]}\\nγi =\\n\\nαi + ∑Nn=1 φni .\\n\\n(6)\\n(7)\\n\\nAs we show in Appendix A.1, the expectation in the multinomial update can be computed as follows:\\nEq [log(θi ) | γ] = Ψ(γi ) − Ψ ∑kj=1 γ j ,\\n\\n(8)\\n\\nwhere Ψ is the first derivative of the log Γ function which is computable via Taylor approximations (Abramowitz and Stegun, 1970).\\nEqs. (6) and (7) have an appealing intuitive interpretation. The Dirichlet update is a posterior Dirichlet given expected observations taken under the variational distribution, E[zn | φn ]. The\\nmultinomial update is akin to using Bayes’ theorem, p(zn | wn ) ∝ p(wn | zn )p(zn ), where p(zn ) is\\napproximated by the exponential of the expected value of its logarithm under the variational distribution.\\nIt is important to note that the variational distribution is actually a conditional distribution,\\nvarying as a function of w. This occurs because the optimization problem in Eq. (5) is conducted\\nfor fixed w, and thus yields optimizing parameters (γ∗ , φ∗ ) that are a function of w. We can write\\nthe resulting variational distribution as q(θ, z | γ∗ (w), φ∗ (w)), where we have made the dependence\\non w explicit. Thus the variational distribution can be viewed as an approximation to the posterior\\ndistribution p(θ, z | w, α, β).\\nIn the language of text, the optimizing parameters (γ∗ (w), φ∗ (w)) are document-specific. In\\nparticular, we view the Dirichlet parameters γ∗ (w) as providing a representation of a document in\\nthe topic simplex.\\n1004\\n\\n\\x0cL ATENT D IRICHLET A LLOCATION\\n\\n(1)\\n(2)\\n(3)\\n(4)\\n(5)\\n(6)\\n(7)\\n(8)\\n(9)\\n\\ninitialize φ0ni := 1/k for all i and n\\ninitialize γi := αi + N/k for all i\\nrepeat\\nfor n = 1 to N\\nfor i = 1 to k\\nt\\nφt+1\\nni := βiwn exp(Ψ(γi ))\\nt+1\\nnormalize φn to sum to 1.\\nt+1\\nγ := α + ∑Nn=1 φt+1\\nn\\nuntil convergence\\nFigure 6: A variational inference algorithm for LDA.\\n\\nWe summarize the variational inference procedure in Figure 6, with appropriate starting points\\nfor γ and φn . From the pseudocode it is clear that each iteration of variational inference for LDA\\nrequires O((N + 1)k) operations. Empirically, we find that the number of iterations required for a\\nsingle document is on the order of the number of words in the document. This yields a total number\\nof operations roughly on the order of N 2 k.\\n5.3 Parameter estimation\\nIn this section we present an empirical Bayes method for parameter estimation in the LDA model\\n(see Section 5.4 for a fuller Bayesian approach). In particular, given a corpus of documents D =\\n{w1 , w2 , . . . , wM }, we wish to find parameters α and β that maximize the (marginal) log likelihood\\nof the data:\\n(α, β) =\\n\\nM\\n\\n∑ log p(wd | α, β).\\n\\nd=1\\n\\nAs we have described above, the quantity p(w | α, β) cannot be computed tractably. However,\\nvariational inference provides us with a tractable lower bound on the log likelihood, a bound which\\nwe can maximize with respect to α and β. We can thus find approximate empirical Bayes estimates\\nfor the LDA model via an alternating variational EM procedure that maximizes a lower bound with\\nrespect to the variational parameters γ and φ, and then, for fixed values of the variational parameters,\\nmaximizes the lower bound with respect to the model parameters α and β.\\nWe provide a detailed derivation of the variational EM algorithm for LDA in Appendix A.4.\\nThe derivation yields the following iterative algorithm:\\n1. (E-step) For each document, find the optimizing values of the variational parameters {γ∗d , φ∗d :\\nd ∈ D }. This is done as described in the previous section.\\n2. (M-step) Maximize the resulting lower bound on the log likelihood with respect to the model\\nparameters α and β. This corresponds to finding maximum likelihood estimates with expected\\nsufficient statistics for each document under the approximate posterior which is computed in\\nthe E-step.\\n1005\\n\\n\\x0cB LEI , N G , AND J ORDAN\\n\\nη\\n\\nα\\n\\nβ\\n\\nθ\\n\\nk\\n\\nz\\n\\nw\\n\\nN\\n\\nM\\n\\nFigure 7: Graphical model representation of the smoothed LDA model.\\n\\nThese two steps are repeated until the lower bound on the log likelihood converges.\\nIn Appendix A.4, we show that the M-step update for the conditional multinomial parameter β\\ncan be written out analytically:\\nβi j ∝\\n\\nM Nd\\n\\n∑ ∑ φ∗dni wdnj .\\n\\n(9)\\n\\nd=1 n=1\\n\\nWe further show that the M-step update for Dirichlet parameter α can be implemented using an\\nefficient Newton-Raphson method in which the Hessian is inverted in linear time.\\n5.4 Smoothing\\nThe large vocabulary size that is characteristic of many document corpora creates serious problems\\nof sparsity. A new document is very likely to contain words that did not appear in any of the\\ndocuments in a training corpus. Maximum likelihood estimates of the multinomial parameters\\nassign zero probability to such words, and thus zero probability to new documents. The standard\\napproach to coping with this problem is to “smooth” the multinomial parameters, assigning positive\\nprobability to all vocabulary items whether or not they are observed in the training set (Jelinek,\\n1997). Laplace smoothing is commonly used; this essentially yields the mean of the posterior\\ndistribution under a uniform Dirichlet prior on the multinomial parameters.\\nUnfortunately, in the mixture model setting, simple Laplace smoothing is no longer justified as a\\nmaximum a posteriori method (although it is often implemented in practice; cf. Nigam et al., 1999).\\nIn fact, by placing a Dirichlet prior on the multinomial parameter we obtain an intractable posterior\\nin the mixture model setting, for much the same reason that one obtains an intractable posterior in\\nthe basic LDA model. Our proposed solution to this problem is to simply apply variational inference\\nmethods to the extended model that includes Dirichlet smoothing on the multinomial parameter.\\nIn the LDA setting, we obtain the extended graphical model shown in Figure 7. We treat β as\\na k × V random matrix (one row for each mixture component), where we assume that each row\\nis independently drawn from an exchangeable Dirichlet distribution.2 We now extend our inference procedures to treat the βi as random variables that are endowed with a posterior distribution,\\n2. An exchangeable Dirichlet is simply a Dirichlet distribution with a single scalar parameter η. The density is the same\\nas a Dirichlet (Eq. 1) where αi = η for each component.\\n\\n1006\\n\\n\\x0cL ATENT D IRICHLET A LLOCATION\\n\\nconditioned on the data. Thus we move beyond the empirical Bayes procedure of Section 5.3 and\\nconsider a fuller Bayesian approach to LDA.\\nWe consider a variational approach to Bayesian inference that places a separable distribution on\\nthe random variables β, θ, and z (Attias, 2000):\\nk\\n\\nM\\n\\ni=1\\n\\nd=1\\n\\nq(β1:k , z1:M , θ1:M | λ, φ, γ) = ∏ Dir(βi | λi ) ∏ qd (θd , zd | φd , γd ),\\nwhere qd (θ, z | φ, γ) is the variational distribution defined for LDA in Eq. (4). As is easily verified,\\nthe resulting variational inference procedure again yields Eqs. (6) and (7) as the update equations\\nfor the variational parameters φ and γ, respectively, as well as an additional update for the new\\nvariational parameter λ:\\nM Nd\\n\\nλi j = η + ∑\\n\\n∑ φ∗dni wdnj .\\n\\nd=1 n=1\\n\\nIterating these equations to convergence yields an approximate posterior distribution on β, θ, and z.\\nWe are now left with the hyperparameter η on the exchangeable Dirichlet, as well as the hyperparameter α from before. Our approach to setting these hyperparameters is again (approximate)\\nempirical Bayes—we use variational EM to find maximum likelihood estimates of these parameters\\nbased on the marginal likelihood. These procedures are described in Appendix A.4.\\n\\n6. Example\\nIn this section, we provide an illustrative example of the use of an LDA model on real data. Our\\ndata are 16,000 documents from a subset of the TREC AP corpus (Harman, 1992). After removing\\na standard list of stop words, we used the EM algorithm described in Section 5.3 to find the Dirichlet\\nand conditional multinomial parameters for a 100-topic LDA model. The top words from some of\\nthe resulting multinomial distributions p(w | z) are illustrated in Figure 8 (top). As we have hoped,\\nthese distributions seem to capture some of the underlying topics in the corpus (and we have named\\nthem according to these topics).\\nAs we emphasized in Section 4, one of the advantages of LDA over related latent variable models is that it provides well-defined inference procedures for previously unseen documents. Indeed,\\nwe can illustrate how LDA works by performing inference on a held-out document and examining\\nthe resulting variational posterior parameters.\\nFigure 8 (bottom) is a document from the TREC AP corpus which was not used for parameter\\nestimation. Using the algorithm in Section 5.1, we computed the variational posterior Dirichlet\\nparameters γ for the article and variational posterior multinomial parameters φn for each word in the\\narticle.\\nRecall that the ith posterior Dirichlet parameter γi is approximately the ith prior Dirichlet parameter αi plus the expected number of words which were generated by the ith topic (see Eq. 7).\\nTherefore, the prior Dirichlet parameters subtracted from the posterior Dirichlet parameters indicate\\nthe expected number of words which were allocated to each topic for a particular document. For\\nthe example article in Figure 8 (bottom), most of the γi are close to αi . Four topics, however, are\\nsignificantly larger (by this, we mean γi − αi ≥ 1). Looking at the corresponding distributions over\\nwords identifies the topics which mixed to form this document (Figure 8, top).\\n1007\\n\\n\\x0cB LEI , N G , AND J ORDAN\\n\\nFurther insight comes from examining the φn parameters. These distributions approximate\\np(zn | w) and tend to peak towards one of the k possible topic values. In the article text in Figure 8,\\nthe words are color coded according to these values (i.e., the ith color is used if qn (zin = 1) > 0.9).\\nWith this illustration, one can identify how the different topics mixed in the document text.\\nWhile demonstrating the power of LDA, the posterior analysis also highlights some of its limitations. In particular, the bag-of-words assumption allows words that should be generated by the\\nsame topic (e.g., “William Randolph Hearst Foundation”) to be allocated to several different topics. Overcoming this limitation would require some form of extension of the basic LDA model;\\nin particular, we might relax the bag-of-words assumption by assuming partial exchangeability or\\nMarkovianity of word sequences.\\n\\n7. Applications and Empirical Results\\nIn this section, we discuss our empirical evaluation of LDA in several problem domains—document\\nmodeling, document classification, and collaborative filtering.\\nIn all of the mixture models, the expected complete log likelihood of the data has local maxima at the points where all or some of the mixture components are equal to each other. To avoid\\nthese local maxima, it is important to initialize the EM algorithm appropriately. In our experiments,\\nwe initialize EM by seeding each conditional multinomial distribution with five documents, reducing their effective total length to two words, and smoothing across the whole vocabulary. This is\\nessentially an approximation to the scheme described in Heckerman and Meila (2001).\\n7.1 Document modeling\\nWe trained a number of latent variable models, including LDA, on two text corpora to compare the\\ngeneralization performance of these models. The documents in the corpora are treated as unlabeled;\\nthus, our goal is density estimation—we wish to achieve high likelihood on a held-out test set. In\\nparticular, we computed the perplexity of a held-out test set to evaluate the models. The perplexity,\\nused by convention in language modeling, is monotonically decreasing in the likelihood of the test\\ndata, and is algebraicly equivalent to the inverse of the geometric mean per-word likelihood. A\\nlower perplexity score indicates better generalization performance.3 More formally, for a test set of\\nM documents, the perplexity is:\\nperplexity(Dtest ) = exp −\\n\\n∑M\\nd=1 log p(wd )\\n.\\n∑M\\nd=1 Nd\\n\\nIn our experiments, we used a corpus of scientific abstracts from the C. Elegans community (Avery, 2002) containing 5,225 abstracts with 28,414 unique terms, and a subset of the TREC AP corpus\\ncontaining 16,333 newswire articles with 23,075 unique terms. In both cases, we held out 10% of\\nthe data for test purposes and trained the models on the remaining 90%. In preprocessing the data,\\n3. Note that we simply use perplexity as a figure of merit for comparing models. The models that we compare are all\\nunigram (“bag-of-words”) models, which—as we have discussed in the Introduction—are of interest in the information retrieval context. We are not attempting to do language modeling in this paper—an enterprise that would require\\nus to examine trigram or other higher-order models. We note in passing, however, that extensions of LDA could be\\nconsidered that involve Dirichlet-multinomial over trigrams instead of unigrams. We leave the exploration of such\\nextensions to language modeling to future work.\\n\\n1008\\n\\n\\x0cL ATENT D IRICHLET A LLOCATION\\n\\nÖØ×\\n\\nÆ Ï\\nÁÄÅ\\nËÀÇÏ\\nÅÍËÁ\\nÅÇÎÁ\\nÈÄ\\nÅÍËÁ Ä\\nËÌ\\nÌÇÊ\\nÁÊËÌ\\nÇÊÃ\\nÇÈ Ê\\nÌÀ Ì Ê\\nÌÊ ËË\\nÄÇÎ\\n\\nÙ\\n\\nØ×\\n\\nÅÁÄÄÁÇÆ\\nÌ\\nÈÊÇ Ê Å\\nÍ Ì\\nÁÄÄÁÇÆ\\nÊ Ä\\nÊ\\nËÈ Æ ÁÆ\\nÆ Ï\\nËÌ Ì\\nÈÄ Æ\\nÅÇÆ\\nÈÊÇ Ê ÅË\\nÇÎ ÊÆÅ ÆÌ\\nÇÆ Ê ËË\\n\\nÐ Ö Ò\\n\\nÀÁÄ Ê Æ\\nÏÇÅ Æ\\nÈ ÇÈÄ\\nÀÁÄ\\nÊË\\nÅÁÄÁ Ë\\nÏÇÊÃ\\nÈ Ê ÆÌË\\nË Ë\\nÅÁÄ\\nÏ Ä Ê\\nÅ Æ\\nÈ Ê ÆÌ\\nÊ\\nÄÁ\\n\\nÙ Ø ÓÒ\\n\\nË ÀÇÇÄ\\nËÌÍ ÆÌË\\nË ÀÇÇÄË\\nÍ ÌÁÇÆ\\nÌ À ÊË\\nÀÁ À\\nÈÍ ÄÁ\\nÌ À Ê\\nÆÆ ÌÌ\\nÅ ÆÁ Ì\\nÆ ÅÈÀ\\nËÌ Ì\\nÈÊ ËÁ ÆÌ\\nÄ Å ÆÌ Ê\\nÀ ÁÌÁ\\n\\nThe William Randolph Hearst Foundation will give $1.25 million to Lincoln Center, Metropolitan Opera Co., New York Philharmonic and Juilliard School. “Our board felt that we had a\\nreal opportunity to make a mark on the future of the performing arts with these grants an act\\nevery bit as important as our traditional areas of support in health, medical research, education\\nand the social services,” Hearst Foundation President Randolph A. Hearst said Monday in\\nannouncing the grants. Lincoln Center’s share will be $200,000 for its new building, which\\nwill house young artists and provide new public facilities. The Metropolitan Opera Co. and\\nNew York Philharmonic will receive $400,000 each. The Juilliard School, where music and\\nthe performing arts are taught, will get $250,000. The Hearst Foundation, a leading supporter\\nof the Lincoln Center Consolidated Corporate Fund, will make its usual annual $100,000\\ndonation, too.\\nFigure 8: An example article from the AP corpus. Each color codes a different factor from which\\nthe word is putatively generated.\\n\\n1009\\n\\n\\x0cB LEI , N G , AND J ORDAN\\n\\n3400\\nSmoothed Unigram\\nSmoothed Mixt. Unigrams\\nLDA\\nFold in pLSI\\n\\n3200\\n3000\\n\\nPerplexity\\n\\n2800\\n2600\\n2400\\n2200\\n2000\\n1800\\n1600\\n1400\\n0\\n\\n10\\n\\n20\\n\\n30\\n\\n40\\n\\n50\\n\\n60\\n\\n70\\n\\n80\\n\\n90\\n\\n100\\n\\nNumber of Topics\\n7000\\nSmoothed Unigram\\nSmoothed Mixt. Unigrams\\nLDA\\nFold in pLSI\\n\\n6500\\n\\nPerplexity\\n\\n6000\\n5500\\n5000\\n4500\\n4000\\n3500\\n3000\\n2500\\n0\\n\\n20\\n\\n40\\n\\n60\\n\\n80\\n\\n100\\n\\n120\\n\\n140\\n\\n160\\n\\n180\\n\\n200\\n\\nNumber of Topics\\nFigure 9: Perplexity results on the nematode (Top) and AP (Bottom) corpora for LDA, the unigram\\nmodel, mixture of unigrams, and pLSI.\\n\\n1010\\n\\n\\x0cL ATENT D IRICHLET A LLOCATION\\n\\nNum. topics (k)\\n2\\n5\\n10\\n20\\n50\\n100\\n200\\n\\nPerplexity (Mult. Mixt.)\\n22,266\\n2.20 × 108\\n1.93 × 1017\\n1.20 × 1022\\n4.19 × 10106\\n2.39 × 10150\\n3.51 × 10264\\n\\nPerplexity (pLSI)\\n7,052\\n17,588\\n63,800\\n2.52 × 105\\n5.04 × 106\\n1.72 × 107\\n1.31 × 107\\n\\nTable 1: Overfitting in the mixture of unigrams and pLSI models for the AP corpus. Similar behavior is observed in the nematode corpus (not reported).\\n\\nwe removed a standard list of 50 stop words from each corpus. From the AP data, we further\\nremoved words that occurred only once.\\nWe compared LDA with the unigram, mixture of unigrams, and pLSI models described in Section 4. We trained all the hidden variable models using EM with exactly the same stopping criteria,\\nthat the average change in expected log likelihood is less than 0.001%.\\nBoth the pLSI model and the mixture of unigrams suffer from serious overfitting issues, though\\nfor different reasons. This phenomenon is illustrated in Table 1. In the mixture of unigrams model,\\noverfitting is a result of peaked posteriors in the training set; a phenomenon familiar in the supervised setting, where this model is known as the naive Bayes model (Rennie, 2001). This leads to a\\nnearly deterministic clustering of the training documents (in the E-step) which is used to determine\\nthe word probabilities in each mixture component (in the M-step). A previously unseen document\\nmay best fit one of the resulting mixture components, but will probably contain at least one word\\nwhich did not occur in the training documents that were assigned to that component. Such words\\nwill have a very small probability, which causes the perplexity of the new document to explode.\\nAs k increases, the documents of the training corpus are partitioned into finer collections and thus\\ninduce more words with small probabilities.\\nIn the mixture of unigrams, we can alleviate overfitting through the variational Bayesian smoothing scheme presented in Section 5.4. This ensures that all words will have some probability under\\nevery mixture component.\\nIn the pLSI case, the hard clustering problem is alleviated by the fact that each document is\\nallowed to exhibit a different proportion of topics. However, pLSI only refers to the training documents and a different overfitting problem arises that is due to the dimensionality of the p(z|d)\\nparameter. One reasonable approach to assigning probability to a previously unseen document is by\\nmarginalizing over d:\\nN\\n\\np(w) = ∑ ∏ ∑ p(wn | z)p(z | d)p(d).\\nd n=1 z\\n\\nEssentially, we are integrating over the empirical distribution on the topic simplex (see Figure 4).\\nThis method of inference, though theoretically sound, causes the model to overfit. The documentspecific topic distribution has some components which are close to zero for those topics that do not\\nappear in the document. Thus, certain words will have very small probability in the estimates of\\n1011\\n\\n\\x0cB LEI , N G , AND J ORDAN\\n\\neach mixture component. When determining the probability of a new document through marginalization, only those training documents which exhibit a similar proportion of topics will contribute\\nto the likelihood. For a given training document’s topic proportions, any word which has small\\nprobability in all the constituent topics will cause the perplexity to explode. As k gets larger, the\\nchance that a training document will exhibit topics that cover all the words in the new document\\ndecreases and thus the perplexity grows. Note that pLSI does not overfit as quickly (with respect to\\nk) as the mixture of unigrams.\\nThis overfitting problem essentially stems from the restriction that each future document exhibit\\nthe same topic proportions as were seen in one or more of the training documents. Given this\\nconstraint, we are not free to choose the most likely proportions of topics for the new document. An\\nalternative approach is the “folding-in” heuristic suggested by Hofmann (1999), where one ignores\\nthe p(z|d) parameters and refits p(z|dnew ). Note that this gives the pLSI model an unfair advantage\\nby allowing it to refit k − 1 parameters to the test data.\\nLDA suffers from neither of these problems. As in pLSI, each document can exhibit a different\\nproportion of underlying topics. However, LDA can easily assign probability to a new document;\\nno heuristics are needed for a new document to be endowed with a different set of topic proportions\\nthan were associated with documents in the training corpus.\\nFigure 9 presents the perplexity for each model on both corpora for different values of k. The\\npLSI model and mixture of unigrams are suitably corrected for overfitting. The latent variable\\nmodels perform better than the simple unigram model. LDA consistently performs better than the\\nother models.\\n7.2 Document classification\\nIn the text classification problem, we wish to classify a document into two or more mutually exclusive classes. As in any classification problem, we may wish to consider generative approaches\\nor discriminative approaches. In particular, by using one LDA module for each class, we obtain a\\ngenerative model for classification. It is also of interest to use LDA in the discriminative framework,\\nand this is our focus in this section.\\nA challenging aspect of the document classification problem is the choice of features. Treating\\nindividual words as features yields a rich but very large feature set (Joachims, 1999). One way to\\nreduce this feature set is to use an LDA model for dimensionality reduction. In particular, LDA\\nreduces any document to a fixed set of real-valued features—the posterior Dirichlet parameters\\nγ∗ (w) associated with the document. It is of interest to see how much discriminatory information\\nwe lose in reducing the document description to these parameters.\\nWe conducted two binary classification experiments using the Reuters-21578 dataset. The\\ndataset contains 8000 documents and 15,818 words.\\nIn these experiments, we estimated the parameters of an LDA model on all the documents,\\nwithout reference to their true class label. We then trained a support vector machine (SVM) on the\\nlow-dimensional representations provided by LDA and compared this SVM to an SVM trained on\\nall the word features.\\nUsing the SVMLight software package (Joachims, 1999), we compared an SVM trained on all\\nthe word features with those trained on features induced by a 50-topic LDA model. Note that we\\nreduce the feature space by 99.6 percent in this case.\\n1012\\n\\n\\x0cL ATENT D IRICHLET A LLOCATION\\n\\n95\\n\\n98\\n\\nAccuracy\\n\\nAccuracy\\n\\n97\\n\\n90\\n\\nLDA Features\\n\\n96\\n\\n95\\n\\n94\\n\\nLDA Features\\n\\nWord Features\\n\\n85\\n0\\n\\n0.05\\n0.1\\n0.15\\n0.2\\nProportion of data used for training\\n\\nWord Features\\n\\n0.25\\n\\n93\\n0\\n\\n0.05\\n0.1\\n0.15\\n0.2\\nProportion of data used for training\\n\\n(a)\\n\\n(b)\\n\\nFigure 10: Classification results on two binary classification problems from the Reuters-21578\\ndataset for different proportions of training data. Graph (a) is EARN vs. NOT EARN.\\nGraph (b) is GRAIN vs. NOT GRAIN.\\n\\n600\\n\\nLDA\\nFold in pLSI\\nSmoothed Mixt. Unigrams\\n\\nPredictive Perplexity\\n\\n550\\n500\\n450\\n400\\n350\\n300\\n250\\n200\\n0\\n\\n10\\n\\n20\\n30\\nNumber of Topics\\n\\n40\\n\\n50\\n\\nFigure 11: Results for collaborative filtering on the EachMovie data.\\n\\nFigure 10 shows our results. We see that there is little reduction in classification performance\\nin using the LDA-based features; indeed, in almost all cases the performance is improved with the\\nLDA features. Although these results need further substantiation, they suggest that the topic-based\\nrepresentation provided by LDA may be useful as a fast filtering algorithm for feature selection in\\ntext classification.\\n1013\\n\\n0.25\\n\\n\\x0cB LEI , N G , AND J ORDAN\\n\\n7.3 Collaborative filtering\\nOur final experiment uses the EachMovie collaborative filtering data. In this data set, a collection\\nof users indicates their preferred movie choices. A user and the movies chosen are analogous to a\\ndocument and the words in the document (respectively).\\nThe collaborative filtering task is as follows. We train a model on a fully observed set of users.\\nThen, for each unobserved user, we are shown all but one of the movies preferred by that user and\\nare asked to predict what the held-out movie is. The different algorithms are evaluated according to\\nthe likelihood they assign to the held-out movie. More precisely, define the predictive perplexity on\\nM test users to be:\\npredictive-perplexity(Dtest ) = exp −\\n\\n∑M\\nd=1 log p(wd,Nd | wd,1:Nd −1 )\\n) .\\nM\\n\\nWe restricted the EachMovie dataset to users that positively rated at least 100 movies (a positive\\nrating is at least four out of five stars). We divided this set of users into 3300 training users and 390\\ntesting users.\\nUnder the mixture of unigrams model, the probability of a movie given a set of observed movies\\nis obtained from the posterior distribution over topics:\\np(w|wobs ) = ∑ p(w|z)p(z|wobs ).\\nz\\n\\nIn the pLSI model, the probability of a held-out movie is given by the same equation except that\\np(z|wobs ) is computed by folding in the previously seen movies. Finally, in the LDA model, the\\nprobability of a held-out movie is given by integrating over the posterior Dirichlet:\\np(w|wobs ) =\\n\\n∑ p(w|z)p(z|θ)p(θ|wobs )dθ,\\nz\\n\\nwhere p(θ|wobs ) is given by the variational inference method described in Section 5.2. Note that\\nthis quantity is efficient to compute. We can interchange the sum and integral sign, and compute a\\nlinear combination of k Dirichlet expectations.\\nWith a vocabulary of 1600 movies, we find the predictive perplexities illustrated in Figure 11.\\nAgain, the mixture of unigrams model and pLSI are corrected for overfitting, but the best predictive\\nperplexities are obtained by the LDA model.\\n\\n8. Discussion\\nWe have described latent Dirichlet allocation, a flexible generative probabilistic model for collections of discrete data. LDA is based on a simple exchangeability assumption for the words and\\ntopics in a document; it is therefore realized by a straightforward application of de Finetti’s representation theorem. We can view LDA as a dimensionality reduction technique, in the spirit of LSI,\\nbut with proper underlying generative probabilistic semantics that make sense for the type of data\\nthat it models.\\nExact inference is intractable for LDA, but any of a large suite of approximate inference algorithms can be used for inference and parameter estimation within the LDA framework. We have\\npresented a simple convexity-based variational approach for inference, showing that it yields a fast\\n1014\\n\\n\\x0cL ATENT D IRICHLET A LLOCATION\\n\\nalgorithm resulting in reasonable comparative performance in terms of test set likelihood. Other\\napproaches that might be considered include Laplace approximation, higher-order variational techniques, and Monte Carlo methods. In particular, Leisink and Kappen (2002) have presented a\\ngeneral methodology for converting low-order variational lower bounds into higher-order variational bounds. It is also possible to achieve higher accuracy by dispensing with the requirement of\\nmaintaining a bound, and indeed Minka and Lafferty (2002) have shown that improved inferential\\naccuracy can be obtained for the LDA model via a higher-order variational technique known as expectation propagation. Finally, Griffiths and Steyvers (2002) have presented a Markov chain Monte\\nCarlo algorithm for LDA.\\nLDA is a simple model, and although we view it as a competitor to methods such as LSI and\\npLSI in the setting of dimensionality reduction for document collections and other discrete corpora, it is also intended to be illustrative of the way in which probabilistic models can be scaled\\nup to provide useful inferential machinery in domains involving multiple levels of structure. Indeed, the principal advantages of generative models such as LDA include their modularity and their\\nextensibility. As a probabilistic module, LDA can be readily embedded in a more complex model—\\na property that is not possessed by LSI. In recent work we have used pairs of LDA modules to\\nmodel relationships between images and their corresponding descriptive captions (Blei and Jordan,\\n2002). Moreover, there are numerous possible extensions of LDA. For example, LDA is readily\\nextended to continuous data or other non-multinomial data. As is the case for other mixture models,\\nincluding finite mixture models and hidden Markov models, the “emission” probability p(wn | zn )\\ncontributes only a likelihood value to the inference procedures for LDA, and other likelihoods are\\nreadily substituted in its place. In particular, it is straightforward to develop a continuous variant of\\nLDA in which Gaussian observables are used in place of multinomials. Another simple extension\\nof LDA comes from allowing mixtures of Dirichlet distributions in the place of the single Dirichlet\\nof LDA. This allows a richer structure in the latent topic space and in particular allows a form of\\ndocument clustering that is different from the clustering that is achieved via shared topics. Finally,\\na variety of extensions of LDA can be considered in which the distributions on the topic variables\\nare elaborated. For example, we could arrange the topics in a time series, essentially relaxing the\\nfull exchangeability assumption to one of partial exchangeability. We could also consider partially\\nexchangeable models in which we condition on exogenous variables; thus, for example, the topic\\ndistribution could be conditioned on features such as “paragraph” or “sentence,” providing a more\\npowerful text model that makes use of information obtained from a parser.\\n\\nAcknowledgements\\nThis work was supported by the National Science Foundation (NSF grant IIS-9988642) and the\\nMultidisciplinary Research Program of the Department of Defense (MURI N00014-00-1-0637).\\nAndrew Y. Ng and David M. Blei were additionally supported by fellowships from the Microsoft\\nCorporation.\\n\\nReferences\\nM. Abramowitz and I. Stegun, editors. Handbook of Mathematical Functions. Dover, New York,\\n1970.\\n1015\\n\\n\\x0cB LEI , N G , AND J ORDAN\\n\\n´\\nD. Aldous. Exchangeability and related topics. In Ecole\\nd’´et´e de probabilit´es de Saint-Flour, XIII—\\n1983, pages 1–198. Springer, Berlin, 1985.\\nH. Attias. A variational Bayesian framework for graphical models. In Advances in Neural Information Processing Systems 12, 2000.\\nL.\\n\\nAvery.\\nCaenorrhabditis genetic center\\nhttp://elegans.swmed.edu/wli/cgcbib.\\n\\nbibliography.\\n\\n2002.\\n\\nURL\\n\\nR. Baeza-Yates and B. Ribeiro-Neto. Modern Information Retrieval. ACM Press, New York, 1999.\\nD. Blei and M. Jordan. Modeling annotated data. Technical Report UCB//CSD-02-1202, U.C.\\nBerkeley Computer Science Division, 2002.\\nB. de Finetti. Theory of probability. Vol. 1-2. John Wiley & Sons Ltd., Chichester, 1990. Reprint\\nof the 1975 translation.\\nS. Deerwester, S. Dumais, T. Landauer, G. Furnas, and R. Harshman. Indexing by latent semantic\\nanalysis. Journal of the American Society of Information Science, 41(6):391–407, 1990.\\nP. Diaconis. Recent progress on de Finetti’s notions of exchangeability. In Bayesian statistics, 3\\n(Valencia, 1987), pages 111–125. Oxford Univ. Press, New York, 1988.\\nJ. Dickey. Multiple hypergeometric functions: Probabilistic interpretations and statistical uses.\\nJournal of the American Statistical Association, 78:628–637, 1983.\\nJ. Dickey, J. Jiang, and J. Kadane. Bayesian methods for censored categorical data. Journal of the\\nAmerican Statistical Association, 82:773–781, 1987.\\nA. Gelman, J. Carlin, H. Stern, and D. Rubin. Bayesian data analysis. Chapman & Hall, London,\\n1995.\\nT. Griffiths and M. Steyvers. A probabilistic approach to semantic representation. In Proceedings\\nof the 24th Annual Conference of the Cognitive Science Society, 2002.\\nD. Harman. Overview of the first text retrieval conference (TREC-1). In Proceedings of the First\\nText Retrieval Conference (TREC-1), pages 1–20, 1992.\\nD. Heckerman and M. Meila. An experimental comparison of several clustering and initialization\\nmethods. Machine Learning, 42:9–29, 2001.\\nT. Hofmann. Probabilistic latent semantic indexing. Proceedings of the Twenty-Second Annual\\nInternational SIGIR Conference, 1999.\\nF. Jelinek. Statistical Methods for Speech Recognition. MIT Press, Cambridge, MA, 1997.\\nT. Joachims. Making large-scale SVM learning practical. In Advances in Kernel Methods - Support\\nVector Learning. M.I.T. Press, 1999.\\nM. Jordan, editor. Learning in Graphical Models. MIT Press, Cambridge, MA, 1999.\\n1016\\n\\n\\x0cL ATENT D IRICHLET A LLOCATION\\n\\nM. Jordan, Z. Ghahramani, T. Jaakkola, and L. Saul. Introduction to variational methods for graphical models. Machine Learning, 37:183–233, 1999.\\nR. Kass and D. Steffey. Approximate Bayesian inference in conditionally independent hierarchical\\nmodels (parametric empirical Bayes models). Journal of the American Statistical Association, 84\\n(407):717–726, 1989.\\nM. Leisink and H. Kappen. General lower bounds based on computer generated higher order expansions. In Uncertainty in Artificial Intelligence, Proceedings of the Eighteenth Conference,\\n2002.\\nT. Minka. Estimating a Dirichlet distribution. Technical report, M.I.T., 2000.\\nT. P. Minka and J. Lafferty. Expectation-propagation for the generative aspect model. In Uncertainty\\nin Artificial Intelligence (UAI), 2002.\\nC. Morris. Parametric empirical Bayes inference: Theory and applications. Journal of the American\\nStatistical Association, 78(381):47–65, 1983. With discussion.\\nK. Nigam, J. Lafferty, and A. McCallum. Using maximum entropy for text classification. IJCAI-99\\nWorkshop on Machine Learning for Information Filtering, pages 61–67, 1999.\\nK. Nigam, A. McCallum, S. Thrun, and T. Mitchell. Text classification from labeled and unlabeled\\ndocuments using EM. Machine Learning, 39(2/3):103–134, 2000.\\nC. Papadimitriou, H. Tamaki, P. Raghavan, and S. Vempala. Latent semantic indexing: A probabilistic analysis. pages 159–168, 1998.\\nA. Popescul, L. Ungar, D. Pennock, and S. Lawrence. Probabilistic models for unified collaborative\\nand content-based recommendation in sparse-data environments. In Uncertainty in Artificial\\nIntelligence, Proceedings of the Seventeenth Conference, 2001.\\nJ. Rennie. Improving multi-class text classification with naive Bayes. Technical Report AITR-2001004, M.I.T., 2001.\\nG. Ronning. Maximum likelihood estimation of Dirichlet distributions. Journal of Statistcal Computation and Simulation, 34(4):215–221, 1989.\\nG. Salton and M. McGill, editors. Introduction to Modern Information Retrieval. McGraw-Hill,\\n1983.\\n\\nAppendix A. Inference and parameter estimation\\nIn this appendix, we derive the variational inference procedure (Eqs. 6 and 7) and the parameter\\nmaximization procedure for the conditional multinomial (Eq. 9) and for the Dirichlet. We begin by\\nderiving a useful property of the Dirichlet distribution.\\n1017\\n\\n\\x0cB LEI , N G , AND J ORDAN\\n\\nA.1 Computing E[log(θi | α)]\\nThe need to compute the expected value of the log of a single probability component under the\\nDirichlet arises repeatedly in deriving the inference and parameter estimation procedures for LDA.\\nThis value can be easily computed from the natural parameterization of the exponential family\\nrepresentation of the Dirichlet distribution.\\nRecall that a distribution is in the exponential family if it can be written in the form:\\np(x | η) = h(x) exp ηT T (x) − A(η) ,\\nwhere η is the natural parameter, T (x) is the sufficient statistic, and A(η) is the log of the normalization factor.\\nWe can write the Dirichlet in this form by exponentiating the log of Eq. (1):\\np(θ | α) = exp\\n\\n∑ki=1 (αi − 1) log θi + log Γ ∑ki=1 αi − ∑ki=1 log Γ(αi ) .\\n\\nFrom this form, we immediately see that the natural parameter of the Dirichlet is ηi = αi − 1 and\\nthe sufficient statistic is T (θi ) = log θi . Furthermore, using the general fact that the derivative of\\nthe log normalization factor with respect to the natural parameter is equal to the expectation of the\\nsufficient statistic, we obtain:\\nE[log θi | α] = Ψ(αi ) − Ψ ∑kj=1 α j\\nwhere Ψ is the digamma function, the first derivative of the log Gamma function.\\nA.2 Newton-Raphson methods for a Hessian with special structure\\nIn this section we describe a linear algorithm for the usually cubic Newton-Raphson optimization\\nmethod. This method is used for maximum likelihood estimation of the Dirichlet distribution (Ronning, 1989, Minka, 2000).\\nThe Newton-Raphson optimization technique finds a stationary point of a function by iterating:\\nαnew = αold − H(αold )−1 g(αold )\\nwhere H(α) and g(α) are the Hessian matrix and gradient respectively at the point α. In general,\\nthis algorithm scales as O(N 3 ) due to the matrix inversion.\\nIf the Hessian matrix is of the form:\\nH = diag(h) + 1z1T ,\\n\\n(10)\\n\\nwhere diag(h) is defined to be a diagonal matrix with the elements of the vector h along the diagonal,\\nthen we can apply the matrix inversion lemma and obtain:\\nH −1 = diag(h)−1 −\\n\\ndiag(h)−1 11T diag(h)−1\\nz−1 + ∑kj=1 h−1\\nj\\n\\nMultiplying by the gradient, we obtain the ith component:\\n(H −1 g)i =\\n\\n1018\\n\\ngi − c\\nhi\\n\\n\\x0cL ATENT D IRICHLET A LLOCATION\\n\\nwhere\\nc=\\n\\n∑kj=1 g j /h j\\nz−1 + ∑kj=1 h−1\\nj\\n\\n.\\n\\nObserve that this expression depends only on the 2k values hi and gi and thus yields a NewtonRaphson algorithm that has linear time complexity.\\nA.3 Variational inference\\nIn this section we derive the variational inference algorithm described in Section 5.1. Recall that\\nthis involves using the following variational distribution:\\nN\\n\\nq(θ, z | γ, φ) = q(θ | γ) ∏ q(zn | φn )\\n\\n(11)\\n\\nn=1\\n\\nas a surrogate for the posterior distribution p(θ, z, w | α, β), where the variational parameters γ and\\nφ are set via an optimization procedure that we now describe.\\nFollowing Jordan et al. (1999), we begin by bounding the log likelihood of a document using\\nJensen’s inequality. Omitting the parameters γ and φ for simplicity, we have:\\n\\n∑ p(θ, z, w | α, β)dθ\\n\\nlog p(w | α, β) = log\\n\\nz\\n\\n∑\\n\\n= log\\n\\nz\\n\\n≥\\n\\np(θ, z, w | α, β)q(θ, z)\\ndθ\\nq(θ, z)\\n\\n∑ q(θ, z) log p(θ, z, w | α, β)dθ − ∑ q(θ, z) log q(θ, z)dθ\\nz\\n\\nz\\n\\n= Eq [log p(θ, z, w | α, β)] − Eq [log q(θ, z)].\\n\\n(12)\\n\\nThus we see that Jensen’s inequality provides us with a lower bound on the log likelihood for an\\narbitrary variational distribution q(θ, z | γ, φ).\\nIt can be easily verified that the difference between the left-hand side and the right-hand side\\nof the Eq. (12) is the KL divergence between the variational posterior probability and the true\\nposterior probability. That is, letting L (γ, φ; α, β) denote the right-hand side of Eq. (12) (where we\\nhave restored the dependence on the variational parameters γ and φ in our notation), we have:\\nlog p(w | α, β) = L (γ, φ; α, β) + D(q(θ, z | γ, φ)\\n\\np(θ, z | w, α, β)).\\n\\n(13)\\n\\nThis shows that maximizing the lower bound L (γ, φ; α, β) with respect to γ and φ is equivalent to\\nminimizing the KL divergence between the variational posterior probability and the true posterior\\nprobability, the optimization problem presented earlier in Eq. (5).\\nWe now expand the lower bound by using the factorizations of p and q:\\n\\nL (γ, φ; α, β) = Eq [log p(θ | α)] + Eq [log p(z | θ)] + Eq [log p(w | z, β)]\\n− Eq [log q(θ)] − Eq [log q(z)].\\n1019\\n\\n(14)\\n\\n\\x0cB LEI , N G , AND J ORDAN\\n\\nFinally, we expand Eq. (14) in terms of the model parameters (α, β) and the variational parameters\\n(γ, φ). Each of the five lines below expands one of the five terms in the bound:\\nk\\n\\nk\\n\\ni=1\\n\\ni=1\\n\\nL (γ, φ; α, β) = log Γ ∑kj=1 α j − ∑ log Γ(αi ) + ∑ (αi − 1) Ψ(γi ) − Ψ ∑kj=1 γ j\\nN\\n\\nk\\n\\n+∑\\n\\n∑ φni\\n\\n+∑\\n\\n∑ ∑ φni wnj log βi j\\n\\nΨ(γi ) − Ψ ∑kj=1 γ j\\n\\nn=1 i=1\\nN k V\\n\\n(15)\\n\\nn=1 i=1 j=1\\n\\nk\\n\\nk\\n\\ni=1\\n\\ni=1\\n\\n− log Γ ∑kj=1 γ j + ∑ log Γ(γi ) − ∑ (γi − 1) Ψ(γi ) − Ψ ∑kj=1 γ j\\nN\\n\\n−∑\\n\\nk\\n\\n∑ φni log φni ,\\n\\nn=1 i=1\\n\\nwhere we have made use of Eq. (8).\\nIn the following two sections, we show how to maximize this lower bound with respect to the\\nvariational parameters φ and γ.\\nA.3.1 VARIATIONAL\\n\\nMULTINOMIAL\\n\\nWe first maximize Eq. (15) with respect to φni , the probability that the nth word is generated by\\nlatent topic i. Observe that this is a constrained maximization since ∑ki=1 φni = 1.\\nWe form the Lagrangian by isolating the terms which contain φni and adding the appropriate\\nLagrange multipliers. Let βiv be p(wvn = 1 | zi = 1) for the appropriate v. (Recall that each wn is\\na vector of size V with exactly one component equal to one; we can select the unique v such that\\nwvn = 1):\\n\\nL[φni ] = φni Ψ(γi ) − Ψ ∑kj=1 γ j + φni log βiv − φni log φni + λn ∑kj=1 φni − 1 ,\\nwhere we have dropped the arguments of L for simplicity, and where the subscript φni denotes that\\nwe have retained only those terms in L that are a function of φni . Taking derivatives with respect to\\nφni , we obtain:\\n∂L\\n= Ψ(γi ) − Ψ ∑kj=1 γ j + log βiv − log φni − 1 + λ.\\n∂φni\\nSetting this derivative to zero yields the maximizing value of the variational parameter φni (cf. Eq. 6):\\nφni ∝ βiv exp Ψ(γi ) − Ψ ∑kj=1 γ j\\n1020\\n\\n.\\n\\n(16)\\n\\n\\x0cL ATENT D IRICHLET A LLOCATION\\n\\nA.3.2 VARIATIONAL D IRICHLET\\nNext, we maximize Eq. (15) with respect to γi , the ith component of the posterior Dirichlet parameter. The terms containing γi are:\\nk\\n\\nN\\n\\nL[γ] = ∑ (αi − 1) Ψ(γi ) − Ψ ∑kj=1 γ j + ∑ φni Ψ(γi ) − Ψ ∑kj=1 γ j\\ni=1\\n\\nn=1\\n\\nk\\n\\n− log Γ ∑kj=1 γ j + log Γ(γi ) − ∑ (γi − 1) Ψ(γi ) − Ψ ∑kj=1 γ j\\n\\n.\\n\\ni=1\\n\\nThis simplifies to:\\nk\\n\\nL[γ] = ∑ Ψ(γi ) − Ψ ∑kj=1 γ j\\n\\nαi + ∑Nn=1 φni − γi − log Γ ∑kj=1 γ j + log Γ(γi ).\\n\\ni=1\\n\\nWe take the derivative with respect to γi :\\n∂L\\n= Ψ (γi ) αi + ∑Nn=1 φni − γi − Ψ ∑kj=1 γ j\\n∂γi\\n\\nk\\n\\n∑\\n\\nα j + ∑Nn=1 φn j − γ j .\\n\\nj=1\\n\\nSetting this equation to zero yields a maximum at:\\nγi = αi + ∑Nn=1 φni .\\n\\n(17)\\n\\nSince Eq. (17) depends on the variational multinomial φ, full variational inference requires\\nalternating between Eqs. (16) and (17) until the bound converges.\\nA.4 Parameter estimation\\nIn this final section, we consider the problem of obtaining empirical Bayes estimates of the model\\nparameters α and β. We solve this problem by using the variational lower bound as a surrogate\\nfor the (intractable) marginal log likelihood, with the variational parameters φ and γ fixed to the\\nvalues found by variational inference. We then obtain (approximate) empirical Bayes estimates by\\nmaximizing this lower bound with respect to the model parameters.\\nWe have thus far considered the log likelihood for a single document. Given our assumption\\nof exchangeability for the documents, the overall log likelihood of a corpus D = {w1 , w2 , . . . , wM }\\nis the sum of the log likelihoods for individual documents; moreover, the overall variational lower\\nbound is the sum of the individual variational bounds. In the remainder of this section, we abuse\\nnotation by using L for the total variational bound, indexing the document-specific terms in the\\nindividual bounds by d, and summing over all the documents.\\nRecall from Section 5.3 that our overall approach to finding empirical Bayes estimates is based\\non a variational EM procedure. In the variational E-step, discussed in Appendix A.3, we maximize\\nthe bound L (γ, φ; α, β) with respect to the variational parameters γ and φ. In the M-step, which we\\ndescribe in this section, we maximize the bound with respect to the model parameters α and β. The\\noverall procedure can thus be viewed as coordinate ascent in L .\\n1021\\n\\n\\x0cB LEI , N G , AND J ORDAN\\n\\nA.4.1 C ONDITIONAL\\n\\nMULTINOMIALS\\n\\nTo maximize with respect to β, we isolate terms and add Lagrange multipliers:\\n\\nL[β] =\\n\\nM Nd\\n\\n∑∑\\n\\nk\\n\\nV\\n\\nk\\n\\n∑ ∑ φdni wdn log βi j + ∑ λi ∑Vj=1 βi j − 1 .\\nj\\n\\nd=1 n=1 i=1 j=1\\n\\ni=1\\n\\nWe take the derivative with respect to βi j , set it to zero, and find:\\nβi j ∝\\n\\nM Nd\\n\\n∑ ∑ φdni wdnj .\\n\\nd=1 n=1\\n\\nA.4.2 D IRICHLET\\nThe terms which contain α are:\\n\\nL[α] =\\n\\nM\\n\\n∑\\n\\nd=1\\n\\nk\\n\\nk\\n\\ni=1\\n\\ni=1\\n\\nlog Γ ∑kj=1 α j − ∑ log Γ(αi ) + ∑ (αi − 1) Ψ(γdi ) − Ψ ∑kj=1 γd j\\n\\nTaking the derivative with respect to αi gives:\\nM\\n∂L\\n= M Ψ ∑kj=1 α j − Ψ(αi ) + ∑ Ψ(γdi ) − Ψ ∑kj=1 γd j\\n∂αi\\nd=1\\n\\nThis derivative depends on α j , where j = i, and we therefore must use an iterative method to find\\nthe maximal α. In particular, the Hessian is in the form found in Eq. (10):\\n∂L\\n= δ(i, j)MΨ (αi ) − Ψ ∑kj=1 α j ,\\n∂αi α j\\nand thus we can invoke the linear-time Newton-Raphson algorithm described in Appendix A.2.\\nFinally, note that we can use the same algorithm to find an empirical Bayes point estimate of η,\\nthe scalar parameter for the exchangeable Dirichlet in the smoothed LDA model in Section 5.4.\\n\\n1022\\n\\n\\x0c',\n",
       " 'title': 'blei03a.dvi'}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from os.path import basename, splitext\n",
    "\n",
    "\n",
    "def fileid(filepath):\n",
    "    \"\"\"\n",
    "    Return the basename of a file without its extension.\n",
    "    >>> fileid('/some/path/to/a/file.pdf')\n",
    "    file\n",
    "    \"\"\"\n",
    "    base, _ = splitext(basename(filepath))\n",
    "    return base\n",
    "\n",
    "\n",
    "def pdftotext(pdf, outdir='.', sourcedir='source', p2t='pdftotext', move=False):\n",
    "    \"\"\"Convert a pdf to a text file. Extract the Author and Title \n",
    "    and return a dictionary consisting of the author, title, text\n",
    "    the source path, the path of the converted text file and the \n",
    "    file ID.\"\"\"    \n",
    "    filename = fileid(pdf)\n",
    "    htmlpath = os.path.join(outdir, filename + '.html')\n",
    "    txtpath = os.path.join(outdir, filename + '.txt')\n",
    "    if not os.path.exists(sourcedir):\n",
    "        os.mkdir(sourcedir)\n",
    "    sourcepath = os.path.join(sourcedir, filename + '.pdf')\n",
    "    subprocess.call([p2t, '-enc', 'UTF-8', '-htmlmeta', pdf, htmlpath])\n",
    "    data = parse_html(htmlpath)\n",
    "    os.remove(htmlpath)\n",
    "    file_action = shutil.move if move else shutil.copy\n",
    "    file_action(pdf, sourcepath)\n",
    "    with open(txtpath, 'w') as outfile:\n",
    "        outfile.write(data['text'])\n",
    "    data['source'] = sourcepath\n",
    "    data['path'] = txtpath\n",
    "    data['id'] = fileid(pdf)\n",
    "    return data\n",
    "\n",
    "pdftotext(\"pdfs/blei2003.pdf\", \n",
    "          outdir=config.get('filepaths', 'txt directory'),\n",
    "          sourcedir=config.get('filepaths', 'source directory'),\n",
    "          move=config.getboolean('indexer.options', 'move pdfs'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quiz!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With that set, we are ready to write the main routine of our indexing procedure. I give you the skeleton of the main routine. The first sight might seem quite daunting, but it is actually just a prcocedure that puts together all statements we have used before. Fill in the gaps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "def index_collection(configpath):\n",
    "    \"Main routine to index a collection of PDFs using Whoosh.\"\n",
    "    config = configparser.ConfigParser()\n",
    "    # read the configuration file\n",
    "    # insert your code here\n",
    "    config.read(configpath)\n",
    "    \n",
    "    recompile = config.getboolean(\"indexer.options\", \"recompile\")\n",
    "    # check whether the supplied index directory already exists\n",
    "    if not os.path.exists(config.get(\"filepaths\", \"index directory\")):\n",
    "        # if not, create a new directory and initialize the index\n",
    "        os.mkdir(config.get(\"filepaths\", \"index directory\"))\n",
    "        index = create_in(config.get(\"filepaths\", \"index directory\"), schema=pdf_schema)\n",
    "        recompile = True\n",
    "    # open a connection to the index\n",
    "    index = open_dir(config.get(\"filepaths\", \"index directory\")) # insert your code here\n",
    "    \n",
    "    # retrieve a set of all file IDs we already indexed\n",
    "    indexed = set(map(fileid, os.listdir(config.get(\"filepaths\", \"txt directory\"))))\n",
    "    # initialize a IndexWriter object\n",
    "    writer = index.writer() # insert your code here\n",
    "    \n",
    "    # iterate over all directories \n",
    "    for directory in config.get(\"filepaths\", \"pdf directory\").split(';'):\n",
    "        # iterate over all PDF files in this directory\n",
    "        for filepath in glob.glob(directory + \"/*.pdf\"):\n",
    "            # poor man's solution to check whether we already indexed this pdf\n",
    "            if fileid(filepath) not in indexed or recompile:\n",
    "                try:\n",
    "                    # call the function pdftotext with the correct arguments\n",
    "                    data = pdftotext(\n",
    "                        filepath, \n",
    "                        outdir=config.get(\"filepaths\", \"txt directory\"),\n",
    "                        sourcedir=config.get(\"filepaths\", \"source directory\"),\n",
    "                        p2t=config.get('programpaths', 'pdftotext'),\n",
    "                        move=config.getboolean(\"indexer.options\", \"move pdfs\")) # insert your code here\n",
    "                    \n",
    "                    # add the new document to the index\n",
    "                    writer.add_document(**data)\n",
    "                except (IOError, UnicodeDecodeError) as error:\n",
    "                    print(error)\n",
    "    # commit our changes\n",
    "    # insert your code here\n",
    "    writer.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now, add some of your own PDF files to the pdf folder `pydf/pdfs` and execute the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "index_collection('pydf.ini')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Before we continue, move the functions `index_collection`, `pdftotext`, `parse_html` and `fileid` to the file `indexed.py` together with their corresponding imports.** Add the cell above to the file within the main environment at the end of the file:\n",
    "\n",
    "    if __name__ == '__main__':\n",
    "        index_collection('pydf.ini')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Web Interface with Flask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![flask](http://flask.pocoo.org/static/logo/flask.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, the dry and hard part of our PDF archiving and search app is over. Now it is time to focus on creating a web application with which we can query the index in user-friendly way. I choose to use the microframework [Flask](http://flask.pocoo.org/) which is an elegant web framework that enables you to get a web app up and running in no time. In no time? Really, in no time! Open a new file called `hello.py` in your favorite text editor and add the following lines of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from flask import Flask\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route(\"/\")\n",
    "def hello():\n",
    "    return \"Hello World!\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(port=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, open a terminal and run the script using:\n",
    "\n",
    "    python3 hello.py\n",
    "    \n",
    "Direct your browser to http://127.0.0.1:5000/ to see the result. That is what I call a simple web framework, yet a very powerful one too.\n",
    "\n",
    "In the directory `pydf/templates/index.html` I created a simple web page that will serve as the landing page of our web application. We can render such pages using Flask's `render_template` function. Open a file called `pydf.py` in the directory `pydf` and add the following lines of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from flask import Flask, render_template\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/')\n",
    "def index():\n",
    "    return render_template('index.html')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=True, host='localhost', port=8000, use_reloader=True, threaded=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the application with \n",
    "    \n",
    "    python3 pydf.py\n",
    "    \n",
    "and check out the result at http://127.0.0.1:8000/. The search box is not working yet. We need two things: (1) a function to search our collection on the basis of a query and (2) a function to show these results to Flask allowing it to render them properly. We'll start with the search function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quiz!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a)** Write a function called `search` that takes as argument a query represented by a string. Open the PDF Index, parse the query, search for the results and return a list of dictionaries in which each dictionary represents a separate search result with the field names as keys and their corresponding values as values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'author': 'David Blei, Andrew Ng, Michael Jordan', 'path': 'data/blei2003.txt', 'source': 'static/pdfs/blei2003.pdf', 'title': 'Latent Dirichlet Allocation', 'id': 'blei2003'}, {'author': 'Edgar Meij, Dolf Trieschnigg, Maarten de Rijke, Wessel Kraaij', 'path': 'data/meij2009.txt', 'source': 'static/pdfs/meij2009.pdf', 'title': 'Conceptual language models for domain-speciﬁc retrieval', 'id': 'meij2009'}, {'author': 'Byron Wallace', 'path': 'data/wallace2012.txt', 'source': 'static/pdfs/wallace2012.pdf', 'title': 'Multiple Narrative Disentanglement: Unraveling Infinite Jest', 'id': 'wallace2012'}, {'author': 'Konstantin Markov, Satoshi Nakamura, ATR, Japan', 'path': 'data/ICASSP_03.txt', 'source': 'static/pdfs/ICASSP_03.pdf', 'title': 'Hybrid HMM/BN LVCSR system integrating multiple acoustic features', 'id': 'ICASSP_03'}, {'author': 'Konstantin Markov, Satoshi Nakamura, ATR Spoken Language Translation Research Labs, Japan', 'path': 'data/ICASSP_05.txt', 'source': 'static/pdfs/ICASSP_05.pdf', 'title': 'Modeling Successive Frame Dependencies with Hybrid HMM~slash~BN Acoustic Model', 'id': 'ICASSP_05'}, {'path': 'data/ICASSP_08.txt', 'source': 'static/pdfs/ICASSP_08.pdf', 'title': 'LANGUAGE IDENTIFICATION WITH DYNAMIC HIDDEN MARKOV NETWORK', 'id': 'ICASSP_08'}]\n"
     ]
    }
   ],
   "source": [
    "from whoosh.index import open_dir\n",
    "from whoosh.qparser import QueryParser\n",
    "\n",
    "def search(query):\n",
    "    # insert your code here\n",
    "    index = open_dir(\"pdf-index\")\n",
    "    query = QueryParser(\"text\", index.schema).parse(query)\n",
    "    with index.searcher() as searcher:\n",
    "        results = searcher.search(query)\n",
    "        for hit in results:\n",
    "            yield dict(hit)\n",
    "    \n",
    "print(list(search(\"(topic model) OR (index probability\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b**) Whoosh' `Result` object contains a method to create highlighted search result excerpts. Field values that are stored in our index can be directly highlighted by Whoosh, using:\n",
    "\n",
    "    result.highlights(FIELDNAME)\n",
    "    \n",
    "Since we did not store the actual text of our pdfs in the collection, we must first open and read the text file corresponding to our search result. That is the reason why we stored the path to our text files in the field `path`. Once we have contents of the text file, we can call the highlight method as follows:\n",
    "\n",
    "    result.highlights(\"text\", text=contents)\n",
    "\n",
    "Adapt the function `search` in such as way that it includes the highlighted search result excerpts for each search result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'snippet': 'documents and the <b class=\"match term0\">model</b> learns the <b class=\"match term1\">topic</b> mixtures p(z | d) only...kV parameters in a k-<b class=\"match term1\">topic</b> LDA <b class=\"match term0\">model</b> do not grow\\nwith the...the corners of the <b class=\"match term1\">topic</b> simplex. The pLSI <b class=\"match term0\">model</b> induces an empirical', 'path': 'data/blei2003.txt', 'title': 'Latent Dirichlet Allocation', 'id': 'blei2003', 'source': 'static/pdfs/blei2003.pdf', 'author': 'David Blei, Andrew Ng, Michael Jordan'}, {'snippet': 'who use so-called <b class=\"match term1\">topic</b> signatures to <b class=\"match term2\">index</b> and retrieve documents...a document <b class=\"match term0\">model</b>, a collection <b class=\"match term0\">model</b>, and the cluster each...samples from this <b class=\"match term0\">model</b>. The query <b class=\"match term0\">model</b>, parametrized by hQ', 'path': 'data/meij2009.txt', 'title': 'Conceptual language models for domain-speciﬁc retrieval', 'id': 'meij2009', 'source': 'static/pdfs/meij2009.pdf', 'author': 'Edgar Meij, Dolf Trieschnigg, Maarten de Rijke, Wessel Kraaij'}, {'snippet': 'graphical <b class=\"match term0\">model</b> in Figure 1. As the figure...document. Under this <b class=\"match term0\">model</b>, documents can be associated...I set a\\nthreshold <b class=\"match term3\">probability</b> α such that', 'path': 'data/wallace2012.txt', 'title': 'Multiple Narrative Disentanglement: Unraveling Infinite Jest', 'id': 'wallace2012', 'source': 'static/pdfs/wallace2012.pdf', 'author': 'Byron Wallace'}, {'snippet': 'upon\\nthis <b class=\"match term0\">model</b>. In the HMM/BN <b class=\"match term0\">model</b>, in addition to speech...since this <b class=\"match term0\">model</b> behaves as a conventional...is the HMM <b class=\"match term2\">index</b> and ✩ is the state <b class=\"match term2\">index</b> of the ☞ HMM.\\nFor simple', 'path': 'data/ICASSP_03.txt', 'title': 'Hybrid HMM/BN LVCSR system integrating multiple acoustic features', 'id': 'ICASSP_03', 'source': 'static/pdfs/ICASSP_03.pdf', 'author': 'Konstantin Markov, Satoshi Nakamura, ATR, Japan'}, {'snippet': 'the Hidden Markov <b class=\"match term4\">Model</b> (HMM) for modeling\\nacoustical...the Buried Markov <b class=\"match term0\">model</b> was proposed [10], where...dependencies. In our <b class=\"match term0\">model</b>, the HMM state <b class=\"match term3\">probability</b> distribution', 'path': 'data/ICASSP_05.txt', 'title': 'Modeling Successive Frame Dependencies with Hybrid HMM~slash~BN Acoustic Model', 'id': 'ICASSP_05', 'source': 'static/pdfs/ICASSP_05.pdf', 'author': 'Konstantin Markov, Satoshi Nakamura, ATR Spoken Language Translation Research Labs, Japan'}, {'snippet': 'respectively.\\n<b class=\"match term0\">Index</b> Terms— Language identi...recognizers and the <b class=\"match term3\">probability</b> of occurrence...ending learning <b class=\"match term0\">model</b> which\\nwhen presented', 'path': 'data/ICASSP_08.txt', 'source': 'static/pdfs/ICASSP_08.pdf', 'title': 'LANGUAGE IDENTIFICATION WITH DYNAMIC HIDDEN MARKOV NETWORK', 'id': 'ICASSP_08'}]\n"
     ]
    }
   ],
   "source": [
    "def search(query):\n",
    "    # insert your code here\n",
    "    index = open_dir(\"pdf-index\")\n",
    "    query = QueryParser(\"text\", index.schema).parse(query)\n",
    "    with index.searcher() as searcher:\n",
    "        results = searcher.search(query)\n",
    "        for hit in results:\n",
    "            result = dict(hit)\n",
    "            with open(result['path']) as infile:\n",
    "                result['snippet'] = hit.highlights(\"text\", infile.read(), top=3)\n",
    "            yield result\n",
    "    \n",
    "print(list(search(\"(topic model) OR (index probability\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a search function we need a way to represent the results in a format that a browser can read. I choose for the simple solution to directly convert the results into HTML. The following function takes as argument a single search result yielded by `search` and returns a representation in HTML of the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        <div id='match'>\n",
      "          <span id='id'>\n",
      "            <a href='static/pdfs/blei2003.pdf' target='_blank'>Latent Dirichlet Allocation</a>\n",
      "          </span>\n",
      "          <span id='author'>David Blei, Andrew Ng, Michael Jordan</span>\n",
      "          </br>\n",
      "          <span id='text'>parameters for a k-<b class=\"match term0\">topic</b> pLSI <b class=\"match term1\">model</b> are k multinomial...kV parameters in a k-<b class=\"match term0\">topic</b> LDA <b class=\"match term1\">model</b> do not grow\n",
      "with the...the corners of the <b class=\"match term0\">topic</b> simplex. The pLSI <b class=\"match term1\">model</b> induces an empirical</span>\n",
      "        </div>\n",
      "           \n"
     ]
    }
   ],
   "source": [
    "def to_html(result):\n",
    "    \"Return a representation of a search result in HTML.\"\n",
    "    title = result['title'] if 'title' in result else result['id']\n",
    "    author = result['author'] if 'author' in result else ''\n",
    "    html = \"\"\"\n",
    "        <div id='match'>\n",
    "          <span id='id'>\n",
    "            <a href='%s' target='_blank'>%s</a>\n",
    "          </span>\n",
    "          <span id='author'>%s</span>\n",
    "          </br>\n",
    "          <span id='text'>%s</span>\n",
    "        </div>\n",
    "           \"\"\" % (result['source'], title, author, result['snippet'])\n",
    "    return html\n",
    "\n",
    "print(to_html(next(search(\"topic model\"))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With that in place, all that is left is to write a function that is connected to the search box in the web interface. This function will be called after a user presses enter in the search box and returns the results of the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from flask import request, jsonify\n",
    "\n",
    "@app.route('/searchbox', methods=['POST'])\n",
    "def searchbox():\n",
    "    query = request.form['q'].strip()\n",
    "    html_results = '\\n'.join(map(to_html, search(query)))\n",
    "    return jsonify({'html': html_results})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `searchbox` function is called by a piece of javascript that resides in `static/script.js`. It extracts the query, converts the results to html and returns that as JSON to the same javascript which is responsible for putting it in the right place at our web page.\n",
    "\n",
    "An exciting moment: our PDF search application is ready. Take it for a spin:\n",
    "\n",
    "    python3 pydf.py\n",
    "    \n",
    "and direct your browser to http://127.0.0.1:8000/. Have fun!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
